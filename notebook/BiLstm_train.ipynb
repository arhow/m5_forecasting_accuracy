{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "   div#notebook-container    { width: 95%; }\n",
       "   div#menubar-container     { width: 65%; }\n",
       "   div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "   div#notebook-container    { width: 95%; }\n",
    "   div#menubar-container     { width: 65%; }\n",
    "   div#maintoolbar-container { width: 99%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from attention import attention_3d_block\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "from module.prepare_data import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "VER = 3                          # Our model version\n",
    "SEED = 42\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "#STORES ids\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "#PATHS for Features\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "BASE     = '../cache/ori_grid_part_1.pkl'\n",
    "PRICE    = '../cache/ori_grid_part_2.pkl'\n",
    "CALENDAR = '../cache/ori_grid_part_3.pkl'\n",
    "LAGS     = '../cache/ori_lags_df_28.pkl'\n",
    "MEAN_ENC = '../cache/ori_mean_encoding_df.pkl'\n",
    "BASE_PATH = '../cache'\n",
    "FINAL_TARGETS = 'sales'\n",
    "SAV_BASE_PATH = '../cache/ver3'\n",
    "PKL_BASE_PATH = BASE_PATH\n",
    "\n",
    "########################### caculate mean and std\n",
    "#################################################################################\n",
    "# diff_series = []\n",
    "# for id_, group in tqdm(BASE_GRID_DF[['id','d','sales']].groupby('id')):\n",
    "#     diff_series += group['sales'].diff().dropna().tolist()\n",
    "# diff_mean = np.mean(diff_series)\n",
    "# diff_std = np.std(diff_series)\n",
    "diff_mean, diff_std = 0.00022844736211235283, 2.9674834203072016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### dataset sample\n",
    "#################################################################################\n",
    "\"\"\"\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "dataset = dataset.batch(2).prefetch(1)\n",
    "for x, y in dataset:\n",
    "    print('x = ', x.numpy())\n",
    "    print('y = ', y.numpy())\n",
    "    \n",
    "a  = gen_dataset(np.arange(100), window_size)\n",
    "b = a.concatenate(gen_dataset(np.arange(2000,2100), window_size))\n",
    "b = b.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "validaton_span = 28\n",
    "window_size = 28\n",
    "batch_size = 5\n",
    "shuffle_buffer = 2000\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = train_dataset.batch(5).prefetch(1)\n",
    "valid_series = train_dataset.batch(1).prefetch(1)\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "window_size = 28\n",
    "def gen_dataset(series, window_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46881677 entries, 0 to 46881676\n",
      "Data columns (total 44 columns):\n",
      " #   Column                           Dtype   \n",
      "---  ------                           -----   \n",
      " 0   id                               category\n",
      " 1   item_id                          category\n",
      " 2   dept_id                          category\n",
      " 3   cat_id                           category\n",
      " 4   store_id                         category\n",
      " 5   state_id                         category\n",
      " 6   d                                int16   \n",
      " 7   sales                            float64 \n",
      " 8   release                          int16   \n",
      " 9   sell_price                       float16 \n",
      " 10  price_max                        float16 \n",
      " 11  price_min                        float16 \n",
      " 12  price_std                        float16 \n",
      " 13  price_mean                       float16 \n",
      " 14  price_norm                       float16 \n",
      " 15  price_nunique                    float16 \n",
      " 16  item_nunique                     int16   \n",
      " 17  price_momentum                   float16 \n",
      " 18  price_momentum_m                 float16 \n",
      " 19  price_momentum_y                 float16 \n",
      " 20  event_name_1                     category\n",
      " 21  event_type_1                     category\n",
      " 22  event_name_2                     category\n",
      " 23  event_type_2                     category\n",
      " 24  snap_CA                          category\n",
      " 25  snap_TX                          category\n",
      " 26  snap_WI                          category\n",
      " 27  tm_d                             int8    \n",
      " 28  tm_w                             int8    \n",
      " 29  tm_m                             int8    \n",
      " 30  tm_y                             int8    \n",
      " 31  tm_wm                            int8    \n",
      " 32  tm_dw                            int8    \n",
      " 33  tm_w_end                         int8    \n",
      " 34  enc_store_id_cat_id_mean         float16 \n",
      " 35  enc_store_id_cat_id_std          float16 \n",
      " 36  enc_store_id_dept_id_mean        float16 \n",
      " 37  enc_store_id_dept_id_std         float16 \n",
      " 38  enc_store_id_item_id_mean        float16 \n",
      " 39  enc_store_id_item_id_std         float16 \n",
      " 40  enc_store_id_tm_dw_item_id_mean  float16 \n",
      " 41  enc_store_id_tm_dw_item_id_std   float16 \n",
      " 42  enc_store_id_tm_dw_mean          float16 \n",
      " 43  enc_store_id_tm_dw_std           float16 \n",
      "dtypes: category(13), float16(20), float64(1), int16(3), int8(7)\n",
      "memory usage: 3.3 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "BASE_GRID_DF = load_base_features(PKL_BASE_PATH, SAV_BASE_PATH, FINAL_TARGETS)\n",
    "print(BASE_GRID_DF.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid(base_grid, store_id):\n",
    "    \n",
    "    train_dataset, valid_dataset = None, None\n",
    "#     X_train, y_train, X_valid, y_valid = np.zeros((0,28)),np.zeros((0,1)),np.zeros((0,28)),np.zeros((0,1))\n",
    "    X_train, y_train, X_valid, y_valid = [],[],[],[]\n",
    "    grid = base_grid[base_grid['store_id']==store_id].reset_index(drop=True)\n",
    "    for id_, group in tqdm(grid[['id','d','sales']].groupby('id')):\n",
    "        if group.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        validaton_span = 28\n",
    "        window_size = 28\n",
    "\n",
    "        series = group['sales'].diff().dropna().values\n",
    "        series = (series-diff_mean)/diff_std\n",
    "\n",
    "        train_series = series[:-validaton_span]\n",
    "        valid_series = series[-(validaton_span+window_size):]\n",
    "        \n",
    "        train_dataset = gen_dataset(train_series, window_size)\n",
    "        valid_dataset = gen_dataset(valid_series, window_size)\n",
    "        for x, y in train_dataset:\n",
    "            X_train.append(x.numpy())# = np.vstack([X_train, x.numpy()])\n",
    "            y_train.append(y.numpy())# = np.vstack([y_train, y.numpy()])\n",
    "        for x, y in valid_dataset:\n",
    "            X_valid.append(x.numpy())# = np.vstack([X_valid, x.numpy()])\n",
    "            y_valid.append(y.numpy())# = np.vstack([y_valid, y.numpy()])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(-1,1)\n",
    "    X_valid = np.array(X_valid)\n",
    "    y_valid = np.array(y_valid).reshape(-1,1)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_valid, y_valid, store_id, base_path):\n",
    "    # model\n",
    "    input = tf.keras.layers.Input(shape=[None])\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None])(input)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = attention_3d_block(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    output = tf.keras.layers.Lambda(lambda x: x*100)(x)\n",
    "\n",
    "    model = Model(inputs=[input], outputs=[output])\n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: 1e-8 * 10 **(epoch/20)\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum =0.9)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    his = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=10, epochs=1, callbacks=[lr_schedule], verbose=1)\n",
    "    \n",
    "    model.save_weights('weightsfile.h5')\n",
    "#     existingModel.load_weights('weightsfile.h5')   \n",
    "#     model.save(f'{base_path}/bilstm_{store_id}.bin')\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model2():\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n",
    "            tf.keras.layers.Conv1D(filters=32,kernel_size=5,strides=1,padding=\"causal\",activation=\"relu\",input_shape=[None,1]),\n",
    "            tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "            tf.keras.layers.LSTM(64),\n",
    "            tf.keras.layers.Dense(1),\n",
    "    #         tf.keras.layers.Lambda(lambda x: x*100),\n",
    "        ]\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum =0.9)\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    model.fit(train_dataset, validation_data=valid_dataset, epochs=20, callbacks=[lr_schedule], verbose=1)\n",
    "    \"\"\"\n",
    "    raise Exception('no implement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX_2\n",
      "(4548716, 28) (4548716, 1) (85372, 28) (85372, 1)\n",
      "Train on 4548716 samples, validate on 85372 samples\n",
      "4548716/4548716 [==============================] - 3185s 700us/sample - loss: 0.9660 - root_mean_squared_error: 0.9829 - val_loss: 0.3974 - val_root_mean_squared_error: 0.6304\n",
      "TX_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30490/30490 [21:06<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4478002, 28) (4478002, 1) (85372, 28) (85372, 1)\n",
      "Train on 4478002 samples, validate on 85372 samples\n",
      "1172890/4478002 [======>.......................] - ETA: 38:25 - loss: 0.6979 - root_mean_squared_error: 0.8354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2504170/4478002 [===============>..............] - ETA: 22:54 - loss: 0.6353 - root_mean_squared_error: 0.7971"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3453810/4478002 [======================>.......] - ETA: 11:54 - loss: 0.6190 - root_mean_squared_error: 0.7868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864930/4478002 [========================>.....] - ETA: 7:07 - loss: 0.6203 - root_mean_squared_error: 0.7876"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4398560/4478002 [============================>.] - ETA: 55s - loss: 0.6122 - root_mean_squared_error: 0.7824"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464370/4478002 [============================>.] - ETA: 9s - loss: 0.6115 - root_mean_squared_error: 0.7820"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 30327/30490 [13:42<00:04, 36.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c2fddf583f9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{SAV_BASE_PATH}/y_valid_{store_id}_diff.npy'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASE_GRID_DF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstore_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{SAV_BASE_PATH}/X_train_{store_id}_diff.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# save\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{SAV_BASE_PATH}/y_train_{store_id}_diff.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# save\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-320532cbad70>\u001b[0m in \u001b[0;36mget_train_valid\u001b[1;34m(base_grid, store_id)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_series\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mvalid_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_series\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# = np.vstack([X_train, x.numpy()])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# = np.vstack([y_train, y.numpy()])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNextSync\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2468\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2469\u001b[1;33m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   2470\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for store_id in STORES_IDS[5:]:\n",
    "    print(store_id)\n",
    "    if os.path.exists(f'{SAV_BASE_PATH}/X_train_{store_id}_diff.npy'):\n",
    "        X_train = np.load(f'{SAV_BASE_PATH}/X_train_{store_id}_diff.npy') # load\n",
    "        y_train = np.load(f'{SAV_BASE_PATH}/y_train_{store_id}_diff.npy') # load\n",
    "        X_valid = np.load(f'{SAV_BASE_PATH}/X_valid_{store_id}_diff.npy') # load\n",
    "        y_valid = np.load(f'{SAV_BASE_PATH}/y_valid_{store_id}_diff.npy') # load\n",
    "    else:\n",
    "        X_train, y_train, X_valid, y_valid = get_train_valid(BASE_GRID_DF, store_id)\n",
    "        np.save(f'{SAV_BASE_PATH}/X_train_{store_id}_diff.npy', X_train) # save\n",
    "        np.save(f'{SAV_BASE_PATH}/y_train_{store_id}_diff.npy', y_train) # save\n",
    "        np.save(f'{SAV_BASE_PATH}/X_valid_{store_id}_diff.npy', X_valid) # save\n",
    "        np.save(f'{SAV_BASE_PATH}/y_valid_{store_id}_diff.npy', y_valid) # save\n",
    "                      \n",
    "    print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n",
    "    model = train_model(X_train, y_train, X_valid, y_valid, store_id, SAV_BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existingModel.save_weights('weightsfile.h5')\n",
    "existingModel.load_weights('weightsfile.h5')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f'{SAV_BASE_PATH}/bilstm_{store_id}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
