{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "   div#notebook-container    { width: 95%; }\n",
       "   div#menubar-container     { width: 65%; }\n",
       "   div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "   div#notebook-container    { width: 95%; }\n",
    "   div#menubar-container     { width: 65%; }\n",
    "   div#maintoolbar-container { width: 99%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from attention import attention_3d_block\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "from module.prepare_data import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "VER = 3                          # Our model version\n",
    "SEED = 42\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "#STORES ids\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "#PATHS for Features\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "BASE     = '../cache/ori_grid_part_1.pkl'\n",
    "PRICE    = '../cache/ori_grid_part_2.pkl'\n",
    "CALENDAR = '../cache/ori_grid_part_3.pkl'\n",
    "LAGS     = '../cache/ori_lags_df_28.pkl'\n",
    "MEAN_ENC = '../cache/ori_mean_encoding_df.pkl'\n",
    "BASE_PATH = '../cache'\n",
    "FINAL_TARGETS = 'sales'\n",
    "SAV_BASE_PATH = '../cache/ver3'\n",
    "PKL_BASE_PATH = BASE_PATH\n",
    "\n",
    "########################### caculate mean and std\n",
    "#################################################################################\n",
    "# diff_series = []\n",
    "# for id_, group in tqdm(BASE_GRID_DF[['id','d','sales']].groupby('id')):\n",
    "#     diff_series += group['sales'].diff().dropna().tolist()\n",
    "# diff_mean = np.mean(diff_series)\n",
    "# diff_std = np.std(diff_series)\n",
    "diff_mean, diff_std = 0.00022844736211235283, 2.9674834203072016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### dataset sample\n",
    "#################################################################################\n",
    "\"\"\"\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "dataset = dataset.batch(2).prefetch(1)\n",
    "for x, y in dataset:\n",
    "    print('x = ', x.numpy())\n",
    "    print('y = ', y.numpy())\n",
    "    \n",
    "a  = gen_dataset(np.arange(100), window_size)\n",
    "b = a.concatenate(gen_dataset(np.arange(2000,2100), window_size))\n",
    "b = b.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "validaton_span = 28\n",
    "window_size = 28\n",
    "batch_size = 5\n",
    "shuffle_buffer = 2000\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = train_dataset.batch(5).prefetch(1)\n",
    "valid_series = train_dataset.batch(1).prefetch(1)\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "window_size = 28\n",
    "def gen_dataset(series, window_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46881677 entries, 0 to 46881676\n",
      "Data columns (total 44 columns):\n",
      " #   Column                           Dtype   \n",
      "---  ------                           -----   \n",
      " 0   id                               category\n",
      " 1   item_id                          category\n",
      " 2   dept_id                          category\n",
      " 3   cat_id                           category\n",
      " 4   store_id                         category\n",
      " 5   state_id                         category\n",
      " 6   d                                int16   \n",
      " 7   sales                            float64 \n",
      " 8   release                          int16   \n",
      " 9   sell_price                       float16 \n",
      " 10  price_max                        float16 \n",
      " 11  price_min                        float16 \n",
      " 12  price_std                        float16 \n",
      " 13  price_mean                       float16 \n",
      " 14  price_norm                       float16 \n",
      " 15  price_nunique                    float16 \n",
      " 16  item_nunique                     int16   \n",
      " 17  price_momentum                   float16 \n",
      " 18  price_momentum_m                 float16 \n",
      " 19  price_momentum_y                 float16 \n",
      " 20  event_name_1                     category\n",
      " 21  event_type_1                     category\n",
      " 22  event_name_2                     category\n",
      " 23  event_type_2                     category\n",
      " 24  snap_CA                          category\n",
      " 25  snap_TX                          category\n",
      " 26  snap_WI                          category\n",
      " 27  tm_d                             int8    \n",
      " 28  tm_w                             int8    \n",
      " 29  tm_m                             int8    \n",
      " 30  tm_y                             int8    \n",
      " 31  tm_wm                            int8    \n",
      " 32  tm_dw                            int8    \n",
      " 33  tm_w_end                         int8    \n",
      " 34  enc_store_id_cat_id_mean         float16 \n",
      " 35  enc_store_id_cat_id_std          float16 \n",
      " 36  enc_store_id_dept_id_mean        float16 \n",
      " 37  enc_store_id_dept_id_std         float16 \n",
      " 38  enc_store_id_item_id_mean        float16 \n",
      " 39  enc_store_id_item_id_std         float16 \n",
      " 40  enc_store_id_tm_dw_item_id_mean  float16 \n",
      " 41  enc_store_id_tm_dw_item_id_std   float16 \n",
      " 42  enc_store_id_tm_dw_mean          float16 \n",
      " 43  enc_store_id_tm_dw_std           float16 \n",
      "dtypes: category(13), float16(20), float64(1), int16(3), int8(7)\n",
      "memory usage: 3.3 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "BASE_GRID_DF = load_base_features(PKL_BASE_PATH, SAV_BASE_PATH, FINAL_TARGETS)\n",
    "print(BASE_GRID_DF.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid(base_grid, store_id):\n",
    "    \n",
    "    train_dataset, valid_dataset = None, None\n",
    "#     X_train, y_train, X_valid, y_valid = np.zeros((0,28)),np.zeros((0,1)),np.zeros((0,28)),np.zeros((0,1))\n",
    "    X_train, y_train, X_valid, y_valid = [],[],[],[]\n",
    "    grid = base_grid[base_grid['store_id']==store_id].reset_index(drop=True)\n",
    "    for id_, group in tqdm(grid[['id','d','sales']].groupby('id')):\n",
    "        if group.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        validaton_span = 28\n",
    "        window_size = 28\n",
    "\n",
    "        series = group['sales'].diff().dropna().values\n",
    "        series = (series-diff_mean)/diff_std\n",
    "\n",
    "        train_series = series[:-validaton_span]\n",
    "        valid_series = series[-(validaton_span+window_size):]\n",
    "        \n",
    "        train_dataset = gen_dataset(train_series, window_size)\n",
    "        valid_dataset = gen_dataset(valid_series, window_size)\n",
    "        for x, y in train_dataset:\n",
    "            X_train.append(x.numpy())# = np.vstack([X_train, x.numpy()])\n",
    "            y_train.append(y.numpy())# = np.vstack([y_train, y.numpy()])\n",
    "        for x, y in valid_dataset:\n",
    "            X_valid.append(x.numpy())# = np.vstack([X_valid, x.numpy()])\n",
    "            y_valid.append(y.numpy())# = np.vstack([y_valid, y.numpy()])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(-1,1)\n",
    "    X_valid = np.array(X_valid)\n",
    "    y_valid = np.array(y_valid).reshape(-1,1)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_valid, y_valid, store_id, base_path):\n",
    "    # model\n",
    "    input = tf.keras.layers.Input(shape=[None])\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None])(input)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = attention_3d_block(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    output = tf.keras.layers.Lambda(lambda x: x*100)(x)\n",
    "\n",
    "    model = Model(inputs=[input], outputs=[output])\n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: 1e-8 * 10 **(epoch/20)\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum =0.9)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    his = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=10, epochs=20, callbacks=[lr_schedule], verbose=1)\n",
    "    model.save(f'{base_path}/bilstm_{store_id}.bin')\n",
    "    return\n",
    "\n",
    "\n",
    "def train_model2():\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n",
    "            tf.keras.layers.Conv1D(filters=32,kernel_size=5,strides=1,padding=\"causal\",activation=\"relu\",input_shape=[None,1]),\n",
    "            tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "            tf.keras.layers.LSTM(64),\n",
    "            tf.keras.layers.Dense(1),\n",
    "    #         tf.keras.layers.Lambda(lambda x: x*100),\n",
    "        ]\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum =0.9)\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    model.fit(train_dataset, validation_data=valid_dataset, epochs=20, callbacks=[lr_schedule], verbose=1)\n",
    "    \"\"\"\n",
    "    raise Exception('no implement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 16436/30490 [00:12<00:13, 1057.50it/s]"
     ]
    }
   ],
   "source": [
    "for store_id in STORES_IDS[:4]:\n",
    "    print(store_id)\n",
    "    X_train, y_train, X_valid, y_valid = get_train_valid(BASE_GRID_DF.sample(10000).reset_index(drop=True), store_id)\n",
    "    print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n",
    "    np.save(f'{SAV_BASE_PATH}/X_train_{store_id}_diff.npy', X_train) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/y_train_{store_id}_diff.npy', y_train) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/X_valid_{store_id}_diff.npy', X_valid) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/y_valid_{store_id}_diff.npy', y_valid) # save\n",
    "    X_train = np.load(f'{SAV_BASE_PATH}/X_train_{store_id}_diff.npy') # load\n",
    "    y_train = np.load(f'{SAV_BASE_PATH}/y_train_{store_id}_diff.npy') # load\n",
    "    X_valid = np.load(f'{SAV_BASE_PATH}/X_valid_{store_id}_diff.npy') # load\n",
    "    y_valid = np.load(f'{SAV_BASE_PATH}/y_valid_{store_id}_diff.npy') # load\n",
    "    \n",
    "    train_model(X_train, y_train, X_valid, y_valid, store_id, SAV_BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
