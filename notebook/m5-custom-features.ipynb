{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this kernel I would like to show: \n",
    "## 1. FE creation approaches\n",
    "## 2. Sequential fe validation\n",
    "## 3. Dimension reduction\n",
    "## 4. FE validation by Permutation importance\n",
    "## 5. Mean encodings\n",
    "## 6. Parallelization for FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, sys, gc, warnings, psutil, random\n",
    "# from sklearn.metrics import rm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMSE\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
    "\n",
    "def permutation_importance(model, validation_df, features_columns, target, metric=rmse, verbose=0):\n",
    "\n",
    "    list_ = []\n",
    "    # Make normal prediction with our model and save score\n",
    "    validation_df['preds'] = model.predict(validation_df[features_columns])\n",
    "    base_score = metric(validation_df[target], validation_df['preds'])\n",
    "    if verbose>0:\n",
    "        print('Standart RMSE', base_score)\n",
    "\n",
    "    # Now we are looping over all our numerical features\n",
    "    for col in features_columns:\n",
    "\n",
    "        # We will make validation set copy to restore\n",
    "        # features states on each run\n",
    "        temp_df = validation_df.copy()\n",
    "\n",
    "        # Error here appears if we have \"categorical\" features and can't \n",
    "        # do np.random.permutation without disrupt categories\n",
    "        # so we need to check if feature is numerical\n",
    "        if temp_df[col].dtypes.name != 'category':\n",
    "            temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "            temp_df['preds'] = model.predict(temp_df[features_columns])\n",
    "            cur_score = metric(temp_df[target], temp_df['preds'])\n",
    "            \n",
    "            list_.append({'feature':col, 'permutation_importance':np.round(cur_score - base_score, 4)})\n",
    "            # If our current rmse score is less than base score\n",
    "            # it means that feature most probably is a bad one\n",
    "            # and our model is learning on noise\n",
    "            if verbose>0:\n",
    "                print(col, np.round(cur_score - base_score, 4))\n",
    "            \n",
    "    return pd.DataFrame(list_).sort_values(by=['permutation_importance'], ascending=False)\n",
    "\n",
    "\n",
    "# permutation_importance_df = permutation_importance(estimator, valid_df, features_columns, TARGET, metric=rmse, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2960025 entries, 0 to 2960024\n",
      "Data columns (total 39 columns):\n",
      "id                  category\n",
      "item_id             category\n",
      "dept_id             category\n",
      "cat_id              category\n",
      "store_id            category\n",
      "state_id            category\n",
      "d                   int16\n",
      "sales               float64\n",
      "release             int16\n",
      "sell_price          float16\n",
      "price_max           float16\n",
      "price_min           float16\n",
      "price_std           float16\n",
      "price_mean          float16\n",
      "price_norm          float16\n",
      "price_nunique       float16\n",
      "item_nunique        int16\n",
      "price_momentum      float16\n",
      "price_momentum_m    float16\n",
      "price_momentum_y    float16\n",
      "event_name_1        category\n",
      "event_type_1        category\n",
      "event_name_2        category\n",
      "event_type_2        category\n",
      "snap_CA             category\n",
      "snap_TX             category\n",
      "snap_WI             category\n",
      "is_Halloween        category\n",
      "is_ValentinesDay    category\n",
      "is_Thanksgiving     category\n",
      "is_Christmas        category\n",
      "is_NewYear          category\n",
      "tm_d                int8\n",
      "tm_w                int8\n",
      "tm_m                int8\n",
      "tm_y                int8\n",
      "tm_wm               int8\n",
      "tm_dw               int8\n",
      "tm_w_end            int8\n",
      "dtypes: category(18), float16(10), float64(1), int16(3), int8(7)\n",
      "memory usage: 173.8 MB\n"
     ]
    }
   ],
   "source": [
    "########################### Load data\n",
    "########################### Basic features were created here:\n",
    "########################### https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "#################################################################################\n",
    "\n",
    "# Read data\n",
    "grid_df = pd.concat([pd.read_pickle('./grid_part_1.pkl'),\n",
    "                     pd.read_pickle('./grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('./grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "\n",
    "# Subsampling\n",
    "# to make all calculations faster.\n",
    "# Keep only 5% of original ids.\n",
    "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
    "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
    "\n",
    "# Let's \"inspect\" our grid DataFrame\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[302]\ttraining's rmse: 2.83927\tvalid_1's rmse: 2.39352\n"
     ]
    }
   ],
   "source": [
    "########################### Baseline model\n",
    "#################################################################################\n",
    "\n",
    "# We will need some global VARS for future\n",
    "\n",
    "SEED = 42             # Our random seed for everything\n",
    "random.seed(SEED)     # to make all tests \"deterministic\"\n",
    "np.random.seed(SEED)\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "TARGET = 'sales'      # Our Target\n",
    "END_TRAIN = 1913      # And we will use last 28 days as validation\n",
    "\n",
    "# Drop some items from \"TEST\" set part (1914...)\n",
    "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
    "\n",
    "# Features that we want to exclude from training\n",
    "remove_features = ['id','d',TARGET]\n",
    "\n",
    "# Our baseline model serves\n",
    "# to do fast checks of\n",
    "# new features performance \n",
    "\n",
    "# We will use LightGBM for our tests\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
    "                    'objective': 'regression',       # Standart loss for RMSE\n",
    "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
    "                    'subsample': 0.8,                \n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
    "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
    "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
    "                    'feature_fraction': 0.8,\n",
    "                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
    "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
    "                    'seed': SEED,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "## RMSE\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
    "\n",
    "# Small function to make fast features tests\n",
    "# estimator = make_fast_test(grid_df)\n",
    "# it will return lgb booster for future analisys\n",
    "def make_fast_test(df, permutate=False):\n",
    "\n",
    "    features_columns = [col for col in list(df) if col not in remove_features]\n",
    "\n",
    "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n",
    "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
    "    \n",
    "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "                            lgb_params,\n",
    "                            train_data,\n",
    "                            valid_sets = [train_data,valid_data],\n",
    "                            verbose_eval = 500,\n",
    "                        )\n",
    "    if permutate:\n",
    "        permutation_importance_df = permutation_importance(estimator, df[df['d']>(END_TRAIN-28)], features_columns, TARGET, metric=rmse, verbose=0)\n",
    "    else:\n",
    "        permutation_importance_df = None\n",
    "    \n",
    "    return estimator, permutation_importance_df\n",
    "\n",
    "# Make baseline model\n",
    "baseline_model,permutation_importance_df = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Lets test our normal Lags (7 days)\n",
    "########################### Some more info about lags here:\n",
    "########################### https://www.kaggle.com/kyakovlev/m5-lags-features\n",
    "#################################################################################\n",
    "\n",
    "# Small helper to make lags creation faster\n",
    "from multiprocessing import Pool                # Multiprocess Runs\n",
    "\n",
    "## Multiprocessing Run.\n",
    "# :t_split - int of lags days                   # type: int\n",
    "# :func - Function to apply on each split       # type: python function\n",
    "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def make_normal_lag(lag_day):\n",
    "    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n",
    "    col_name = 'sales_lag_'+str(lag_day)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "# Launch parallel lag creation\n",
    "# and \"append\" to our grid\n",
    "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model,permutation_importance_df  = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart RMSE 2.2671115304002565\n",
      "release 0.0\n",
      "sell_price 0.003\n",
      "price_max -0.0005\n",
      "price_min 0.0002\n",
      "price_std 0.0063\n",
      "price_mean 0.003\n",
      "price_norm 0.0083\n",
      "price_nunique -0.0022\n",
      "item_nunique 0.0012\n",
      "price_momentum 0.0002\n",
      "price_momentum_m 0.0085\n",
      "price_momentum_y 0.001\n",
      "tm_d 0.0034\n",
      "tm_w -0.0\n",
      "tm_m 0.0\n",
      "tm_y 0.0\n",
      "tm_wm 0.0001\n",
      "tm_dw 0.0951\n",
      "tm_w_end 0.0139\n",
      "sales_lag_1 0.4595\n",
      "sales_lag_2 0.021\n",
      "sales_lag_3 0.0037\n",
      "sales_lag_4 0.0121\n",
      "sales_lag_5 0.0119\n",
      "sales_lag_6 0.0122\n",
      "sales_lag_7 0.0278\n"
     ]
    }
   ],
   "source": [
    "########################### Permutation importance Test\n",
    "########################### https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
    "#################################################################################\n",
    "\n",
    "# Let's creat validation dataset and features\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "\n",
    "# Make normal prediction with our model and save score\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "\n",
    "# Now we are looping over all our numerical features\n",
    "for col in features_columns:\n",
    "    \n",
    "    # We will make validation set copy to restore\n",
    "    # features states on each run\n",
    "    temp_df = validation_df.copy()\n",
    "    \n",
    "    # Error here appears if we have \"categorical\" features and can't \n",
    "    # do np.random.permutation without disrupt categories\n",
    "    # so we need to check if feature is numerical\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        \n",
    "        # If our current rmse score is less than base score\n",
    "        # it means that feature most probably is a bad one\n",
    "        # and our model is learning on noise\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "# Remove Temp data\n",
    "del temp_df, validation_df\n",
    "\n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "\n",
    "# Results:\n",
    "## Lags with 1 days shift (nearest past) are important\n",
    "## Some other features are not important and probably just noise\n",
    "## Better make several Permutation runs to confirm useless of the feature\n",
    "## link again https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
    "\n",
    "## price_nunique -0.002 : strong negative values are most probably noise\n",
    "## price_max -0.0002 : values close to 0 need deeper investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's rmse: 2.74855\tvalid_1's rmse: 2.37852\n",
      "Early stopping, best iteration is:\n",
      "[517]\ttraining's rmse: 2.74172\tvalid_1's rmse: 2.37664\n",
      "Standart RMSE 2.376639096408089\n",
      "release 0.0\n",
      "sell_price 0.0064\n",
      "price_max 0.0075\n",
      "price_min 0.0043\n",
      "price_std 0.0021\n",
      "price_mean 0.0018\n",
      "price_norm 0.0111\n",
      "price_nunique 0.0188\n",
      "item_nunique 0.0076\n",
      "price_momentum 0.0006\n",
      "price_momentum_m 0.0364\n",
      "price_momentum_y 0.0088\n",
      "tm_d 0.0042\n",
      "tm_w 0.0042\n",
      "tm_m 0.0015\n",
      "tm_y 0.0\n",
      "tm_wm -0.0007\n",
      "tm_dw 0.1074\n",
      "tm_w_end 0.0153\n",
      "sales_lag_56 0.0148\n",
      "sales_lag_57 -0.0022\n",
      "sales_lag_58 0.0182\n",
      "sales_lag_59 0.0033\n",
      "sales_lag_60 0.0036\n",
      "sales_lag_61 -0.0031\n",
      "sales_lag_62 0.0019\n"
     ]
    }
   ],
   "source": [
    "########################### Lets test far away Lags (7 days with 56 days shift)\n",
    "########################### and check permutation importance\n",
    "#################################################################################\n",
    "\n",
    "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "for col in features_columns:\n",
    "    temp_df = validation_df.copy()\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "del temp_df, validation_df\n",
    "        \n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "\n",
    "# Results:\n",
    "## Lags with 56 days shift (far away past) are not as important\n",
    "## as nearest past lags\n",
    "## and at some point will be just noise for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: id 7\n",
      "[0.72243389 0.06622603 0.05933126 0.04200092 0.0388851  0.03610057\n",
      " 0.03502223]\n",
      "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[424]\ttraining's rmse: 2.60036\tvalid_1's rmse: 2.27278\n",
      "Standart RMSE 2.2727815880775624\n",
      "release 0.0\n",
      "sell_price 0.0097\n",
      "price_max 0.0008\n",
      "price_min -0.0005\n",
      "price_std 0.002\n",
      "price_mean 0.0021\n",
      "price_norm 0.0049\n",
      "price_nunique -0.0063\n",
      "item_nunique -0.0011\n",
      "price_momentum 0.0\n",
      "price_momentum_m 0.0165\n",
      "price_momentum_y 0.0009\n",
      "tm_d 0.0103\n",
      "tm_w -0.0001\n",
      "tm_m 0.0016\n",
      "tm_y 0.0\n",
      "tm_wm 0.0005\n",
      "tm_dw 0.208\n",
      "tm_w_end 0.0073\n",
      "sales_pca_id7_1 1.4509\n",
      "sales_pca_id7_2 0.016\n",
      "sales_pca_id7_3 0.007\n"
     ]
    }
   ],
   "source": [
    "########################### PCA\n",
    "#################################################################################\n",
    "\n",
    "# The main question here - can we have \n",
    "# almost same rmse boost with less features\n",
    "# less dimensionality?\n",
    "\n",
    "# Lets try PCA and make 7->3 dimensionality reduction\n",
    "\n",
    "# PCA is \"unsupervised\" learning\n",
    "# and with shifted target we can be sure\n",
    "# that we have no Target leakage\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def make_pca(df, pca_col, n_days):\n",
    "    print('PCA:', pca_col, n_days)\n",
    "    \n",
    "    # We don't need any other columns to make pca\n",
    "    pca_df = df[[pca_col,'d',TARGET]]\n",
    "    \n",
    "    # If we are doing pca for other series \"levels\" \n",
    "    # we need to agg first\n",
    "    if pca_col != 'id':\n",
    "        merge_base = pca_df[[pca_col,'d']]\n",
    "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
    "        pca_df[TARGET] = pca_df['sum']\n",
    "        del pca_df['sum']\n",
    "    \n",
    "    # Min/Max scaling\n",
    "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
    "    \n",
    "    # Making \"lag\" in old way (not parallel)\n",
    "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
    "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
    "    pca_df = pca_df.assign(**{\n",
    "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
    "            for l in LAG_DAYS\n",
    "            for col in [TARGET]\n",
    "        })\n",
    "    \n",
    "    pca_columns = list(pca_df)[3:]\n",
    "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
    "    pca = PCA(random_state=SEED)\n",
    "    \n",
    "    # You can use fit_transform here\n",
    "    pca.fit(pca_df[pca_columns])\n",
    "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
    "    \n",
    "    print(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # we will keep only 3 most \"valuable\" columns/dimensions \n",
    "    keep_cols = pca_columns[:3]\n",
    "    print('Columns to keep:', keep_cols)\n",
    "    \n",
    "    # If we are doing pca for other series \"levels\"\n",
    "    # we need merge back our results to merge_base df\n",
    "    # and only than return resulted df\n",
    "    # I'll skip that step here\n",
    "    \n",
    "    return pca_df[keep_cols]\n",
    "\n",
    "\n",
    "# Make PCA\n",
    "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "for col in features_columns:\n",
    "    temp_df = validation_df.copy()\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding item_id\n",
      "Encoding cat_id\n",
      "Encoding dept_id\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's rmse: 2.74303\tvalid_1's rmse: 2.37972\n",
      "Early stopping, best iteration is:\n",
      "[490]\ttraining's rmse: 2.74583\tvalid_1's rmse: 2.37905\n",
      "Standart RMSE 2.3790545391991853\n",
      "release 0.0\n",
      "sell_price 0.0254\n",
      "price_max 0.0095\n",
      "price_min 0.0001\n",
      "price_std 0.0081\n",
      "price_mean 0.0004\n",
      "price_norm 0.0117\n",
      "price_nunique -0.0045\n",
      "item_nunique -0.0026\n",
      "price_momentum -0.0\n",
      "price_momentum_m 0.0345\n",
      "price_momentum_y 0.0109\n",
      "tm_d 0.0068\n",
      "tm_w 0.007\n",
      "tm_m 0.0011\n",
      "tm_y 0.0\n",
      "tm_wm 0.0011\n",
      "tm_dw 0.1773\n",
      "tm_w_end 0.0124\n",
      "sales_item_id_encoding_std 0.0116\n",
      "sales_item_id_encoding_mean 1.9479\n",
      "sales_cat_id_encoding_std 0.0008\n",
      "sales_cat_id_encoding_mean 0.0012\n",
      "sales_dept_id_encoding_std 0.0029\n",
      "sales_dept_id_encoding_mean -0.0\n"
     ]
    }
   ],
   "source": [
    "########################### Mean/std target encoding\n",
    "#################################################################################\n",
    "\n",
    "# We will use these three columns for test\n",
    "# (in combination with store_id)\n",
    "icols = ['item_id','cat_id','dept_id']\n",
    "\n",
    "# But we can use any other column or even multiple groups\n",
    "# like these ones\n",
    "#            'state_id',\n",
    "#            'store_id',\n",
    "#            'cat_id',\n",
    "#            'dept_id',\n",
    "#            ['state_id', 'cat_id'],\n",
    "#            ['state_id', 'dept_id'],\n",
    "#            ['store_id', 'cat_id'],\n",
    "#            ['store_id', 'dept_id'],\n",
    "#            'item_id',\n",
    "#            ['item_id', 'state_id'],\n",
    "#            ['item_id', 'store_id']\n",
    "\n",
    "# There are several ways to do \"mean\" encoding\n",
    "## K-fold scheme\n",
    "## LOO (leave one out)\n",
    "## Smoothed/regularized \n",
    "## Expanding mean\n",
    "## etc \n",
    "\n",
    "# You can test as many options as you want\n",
    "# and decide what to use\n",
    "# Because of memory issues you can't \n",
    "# use many features.\n",
    "\n",
    "# We will use simple target encoding\n",
    "# by std and mean agg\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n",
    "    \n",
    "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
    "    joiner = '_'+col+'_encoding_'\n",
    "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
    "    temp_df = temp_df.reset_index()\n",
    "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
    "    del temp_df\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "for col in features_columns:\n",
    "    temp_df = validation_df.copy()\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "        \n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "# Bad thing that for some items  \n",
    "# we are using past and future values.\n",
    "# But we are looking for \"categorical\" similiarity\n",
    "# on a \"long run\". So future here is not a big problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's rmse: 2.63222\tvalid_1's rmse: 2.28391\n",
      "Early stopping, best iteration is:\n",
      "[840]\ttraining's rmse: 2.56826\tvalid_1's rmse: 2.27466\n",
      "Standart RMSE 2.2746555036589906\n",
      "release 0.0\n",
      "sell_price 0.0314\n",
      "price_max 0.0207\n",
      "price_min 0.0127\n",
      "price_std 0.0384\n",
      "price_mean 0.0073\n",
      "price_norm 0.0223\n",
      "price_nunique 0.0169\n",
      "item_nunique 0.006\n",
      "price_momentum 0.0001\n",
      "price_momentum_m 0.0174\n",
      "price_momentum_y -0.0001\n",
      "tm_d 0.0072\n",
      "tm_w 0.0016\n",
      "tm_m 0.0009\n",
      "tm_y 0.0\n",
      "tm_wm 0.0005\n",
      "tm_dw 0.2029\n",
      "tm_w_end 0.0061\n",
      "last_sale 0.6512\n"
     ]
    }
   ],
   "source": [
    "########################### Last non O sale\n",
    "#################################################################################\n",
    "\n",
    "def find_last_sale(df,n_day):\n",
    "    \n",
    "    # Limit initial df\n",
    "    ls_df = df[['id','d',TARGET]]\n",
    "    \n",
    "    # Convert target to binary\n",
    "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
    "    \n",
    "    # Make lags to prevent any leakage\n",
    "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
    "\n",
    "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
    "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
    "\n",
    "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
    "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
    "\n",
    "    return ls_df[['last_sale']]\n",
    "\n",
    "\n",
    "# Find last non zero\n",
    "# Need some \"dances\" to fit in memory limit with groupers\n",
    "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "for col in features_columns:\n",
    "    temp_df = validation_df.copy()\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "grid_df = pd.concat([pd.read_pickle('../cache/grid_part_1.pkl'),\n",
    "                     pd.read_pickle('../cache/grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('../cache/grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 30, 31,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "       15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28], dtype=int8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df.tm_d.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>release</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>price_max</th>\n",
       "      <th>price_min</th>\n",
       "      <th>price_std</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>price_norm</th>\n",
       "      <th>price_nunique</th>\n",
       "      <th>item_nunique</th>\n",
       "      <th>price_momentum</th>\n",
       "      <th>price_momentum_m</th>\n",
       "      <th>price_momentum_y</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>is_Halloween</th>\n",
       "      <th>is_ValentinesDay</th>\n",
       "      <th>is_Thanksgiving</th>\n",
       "      <th>is_Christmas</th>\n",
       "      <th>is_NewYear</th>\n",
       "      <th>tm_d</th>\n",
       "      <th>tm_w</th>\n",
       "      <th>tm_m</th>\n",
       "      <th>tm_y</th>\n",
       "      <th>tm_wm</th>\n",
       "      <th>tm_dw</th>\n",
       "      <th>tm_w_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>0.476318</td>\n",
       "      <td>0.919922</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.559570</td>\n",
       "      <td>1.769531</td>\n",
       "      <td>1.559570</td>\n",
       "      <td>0.032745</td>\n",
       "      <td>1.764648</td>\n",
       "      <td>0.881348</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885742</td>\n",
       "      <td>0.896484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.169922</td>\n",
       "      <td>3.169922</td>\n",
       "      <td>2.970703</td>\n",
       "      <td>0.046356</td>\n",
       "      <td>2.980469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.064453</td>\n",
       "      <td>1.043945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_012</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.980469</td>\n",
       "      <td>6.519531</td>\n",
       "      <td>5.980469</td>\n",
       "      <td>0.115967</td>\n",
       "      <td>6.468750</td>\n",
       "      <td>0.916992</td>\n",
       "      <td>3.0</td>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_015</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0.720215</td>\n",
       "      <td>0.680176</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>0.706543</td>\n",
       "      <td>0.972168</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990234</td>\n",
       "      <td>1.001953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_015_CA_1_validation  HOBBIES_1_015  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d  sales  release  sell_price  price_max  price_min  price_std  \\\n",
       "0       CA  1   12.0        0    0.459961   0.500000   0.419922   0.019760   \n",
       "1       CA  1    2.0        0    1.559570   1.769531   1.559570   0.032745   \n",
       "2       CA  1    0.0        0    3.169922   3.169922   2.970703   0.046356   \n",
       "3       CA  1    0.0        0    5.980469   6.519531   5.980469   0.115967   \n",
       "4       CA  1    4.0        0    0.700195   0.720215   0.680176   0.011337   \n",
       "\n",
       "   price_mean  price_norm  price_nunique  item_nunique  price_momentum  \\\n",
       "0    0.476318    0.919922            4.0            16             NaN   \n",
       "1    1.764648    0.881348            2.0             9             NaN   \n",
       "2    2.980469    1.000000            2.0            20             NaN   \n",
       "3    6.468750    0.916992            3.0            71             NaN   \n",
       "4    0.706543    0.972168            3.0            16             NaN   \n",
       "\n",
       "   price_momentum_m  price_momentum_y event_name_1 event_type_1 event_name_2  \\\n",
       "0          0.968750          0.949219          NaN          NaN          NaN   \n",
       "1          0.885742          0.896484          NaN          NaN          NaN   \n",
       "2          1.064453          1.043945          NaN          NaN          NaN   \n",
       "3          0.921875          0.958984          NaN          NaN          NaN   \n",
       "4          0.990234          1.001953          NaN          NaN          NaN   \n",
       "\n",
       "  event_type_2 snap_CA snap_TX snap_WI is_Halloween is_ValentinesDay  \\\n",
       "0          NaN       0       0       0            0                0   \n",
       "1          NaN       0       0       0            0                0   \n",
       "2          NaN       0       0       0            0                0   \n",
       "3          NaN       0       0       0            0                0   \n",
       "4          NaN       0       0       0            0                0   \n",
       "\n",
       "  is_Thanksgiving is_Christmas is_NewYear  tm_d  tm_w  tm_m  tm_y  tm_wm  \\\n",
       "0               0            0          0    29     4     1     0      5   \n",
       "1               0            0          0    29     4     1     0      5   \n",
       "2               0            0          0    29     4     1     0      5   \n",
       "3               0            0          0    29     4     1     0      5   \n",
       "4               0            0          0    29     4     1     0      5   \n",
       "\n",
       "   tm_dw  tm_w_end  \n",
       "0      5         1  \n",
       "1      5         1  \n",
       "2      5         1  \n",
       "3      5         1  \n",
       "4      5         1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['state_id']\n",
      "Encoding ['store_id']\n",
      "Encoding ['cat_id']\n",
      "Encoding ['dept_id']\n",
      "Encoding ['state_id', 'cat_id']\n",
      "Encoding ['state_id', 'dept_id']\n",
      "Encoding ['store_id', 'cat_id']\n",
      "Encoding ['store_id', 'dept_id']\n",
      "Encoding ['item_id']\n",
      "Encoding ['item_id', 'state_id']\n",
      "Encoding ['item_id', 'store_id']\n",
      "Encoding ['tm_dw', 'item_id']\n",
      "Encoding ['tm_dw']\n"
     ]
    }
   ],
   "source": [
    "########################### Apply on grid_df\n",
    "#################################################################################\n",
    "# lets read grid from \n",
    "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "# to be sure that our grids are aligned by index\n",
    "grid_df = pd.concat([pd.read_pickle('../cache/grid_part_1.pkl'),\n",
    "                     pd.read_pickle('../cache/grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('../cache/grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "TARGET = 'sales'\n",
    "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "icols =  [\n",
    "            ['state_id'],\n",
    "            ['store_id'],\n",
    "            ['cat_id'],\n",
    "            ['dept_id'],\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id'],\n",
    "            ['tm_dw','item_id'],\n",
    "            ['tm_dw'],\n",
    "#             ['tm_m'],\n",
    "            ]\n",
    "\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
    "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
    "\n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[['id','d']+keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Mean/Std encoding\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "print('Save Mean/Std encoding')\n",
    "grid_df.to_pickle('../cache/mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46881677 entries, 0 to 46881676\n",
      "Data columns (total 28 columns):\n",
      " #   Column                     Dtype   \n",
      "---  ------                     -----   \n",
      " 0   id                         category\n",
      " 1   d                          int16   \n",
      " 2   enc_state_id_mean          float16 \n",
      " 3   enc_state_id_std           float16 \n",
      " 4   enc_store_id_mean          float16 \n",
      " 5   enc_store_id_std           float16 \n",
      " 6   enc_cat_id_mean            float16 \n",
      " 7   enc_cat_id_std             float16 \n",
      " 8   enc_dept_id_mean           float16 \n",
      " 9   enc_dept_id_std            float16 \n",
      " 10  enc_state_id_cat_id_mean   float16 \n",
      " 11  enc_state_id_cat_id_std    float16 \n",
      " 12  enc_state_id_dept_id_mean  float16 \n",
      " 13  enc_state_id_dept_id_std   float16 \n",
      " 14  enc_store_id_cat_id_mean   float16 \n",
      " 15  enc_store_id_cat_id_std    float16 \n",
      " 16  enc_store_id_dept_id_mean  float16 \n",
      " 17  enc_store_id_dept_id_std   float16 \n",
      " 18  enc_item_id_mean           float16 \n",
      " 19  enc_item_id_std            float16 \n",
      " 20  enc_item_id_state_id_mean  float16 \n",
      " 21  enc_item_id_state_id_std   float16 \n",
      " 22  enc_item_id_store_id_mean  float16 \n",
      " 23  enc_item_id_store_id_std   float16 \n",
      " 24  enc_tm_dw_item_id_mean     float16 \n",
      " 25  enc_tm_dw_item_id_std      float16 \n",
      " 26  enc_tm_dw_mean             float16 \n",
      " 27  enc_tm_dw_std              float16 \n",
      "dtypes: category(1), float16(26), int16(1)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of new features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
