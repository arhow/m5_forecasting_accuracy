{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "   div#notebook-container    { width: 95%; }\n",
       "   div#menubar-container     { width: 65%; }\n",
       "   div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "   div#notebook-container    { width: 95%; }\n",
    "   div#menubar-container     { width: 65%; }\n",
    "   div#maintoolbar-container { width: 99%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "from module.prepare_data import *\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 2000)\n",
    "\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "VER = 4                          # Our model version\n",
    "SEED = 42\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "\n",
    "n_fold = 3\n",
    "part_len = (END_TRAIN-START_TRAIN)//n_fold\n",
    "\n",
    "#PATHS for Features\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "BASE     = '../cache/ori_grid_part_1.pkl'\n",
    "PRICE    = '../cache/ori_grid_part_2.pkl'\n",
    "CALENDAR = '../cache/ori_grid_part_3.pkl'\n",
    "LAGS     = '../cache/ori_lags_df_28.pkl'\n",
    "MEAN_ENC = '../cache/ori_mean_encoding_df.pkl'\n",
    "BASE_PATH  = f'../cache/ver{VER}'\n",
    "BASE_GRID = f'../cache/ver{VER}/base_grid.pkl'\n",
    "\n",
    "#STORES ids\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "TEST_STORE_ID = 'CA_1'\n",
    "REMOVE_FEATURES = ['d', 'id', 'sales', 'state_id', 'store_id', 'wm_yr_wk']\n",
    "BASE_FEATURES = ['id','item_id','dept_id','cat_id','store_id', 'state_id','d','sales','release','sell_price','price_max','price_min','price_std','price_mean','event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI','tm_d','tm_w','tm_m','tm_y','tm_wm','tm_dw','tm_w_end']\n",
    "TS_COLUMNS = ['sell_price','sales','event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI','tm_d','tm_w','tm_m','tm_y','tm_wm','tm_dw','tm_w_end']\n",
    "STATIC_COLUMNS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory ../cache/ver4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if not os.path.exists(BASE_PATH):\n",
    "        os.makedirs(BASE_PATH)\n",
    "except OSError:\n",
    "    print(\"Creation of the directory %s failed\" % BASE_PATH)\n",
    "else:\n",
    "    print(\"Successfully created the directory %s\" % BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helper to load data by store ID\n",
    "#################################################################################\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    if not os.path.exists(BASE_GRID):\n",
    "        grid_df = pd.concat([pd.read_pickle(BASE),\n",
    "                        pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                        pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                        axis=1)\n",
    "        grid_df['item_id'] = grid_df['item_id'].apply(lambda x: int(x.split('_')[-1])).astype('int16')\n",
    "        grid_df['dept_id'] = grid_df['dept_id'].apply(lambda x: int(x.split('_')[-1])).astype('int16')\n",
    "        grid_df['cat_id'] = grid_df['cat_id'].replace({'HOBBIES': 0, 'HOUSEHOLD': 1, 'FOODS': 2}).astype('int16')\n",
    "        grid_df['snap_CA'] = grid_df['snap_CA'].astype('int8')\n",
    "        grid_df['snap_TX'] = grid_df['snap_TX'].astype('int8')\n",
    "        grid_df['snap_WI'] = grid_df['snap_WI'].astype('int8')\n",
    "        for col in ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']:\n",
    "            grid_df[col] = grid_df[col].replace(dict(zip(grid_df[col].unique(), np.arange(grid_df[col].unique().shape[0])))).astype('int32')\n",
    "            \n",
    "        grid_df.to_pickle(BASE_GRID)\n",
    "    else:\n",
    "        grid_df = pd.read_pickle(BASE_GRID)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    grid_df = grid_df[grid_df['store_id']==store]\n",
    "    return grid_df[BASE_FEATURES].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = get_data_by_store(TEST_STORE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4788267 entries, 0 to 4788266\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   sell_price    float16\n",
      " 1   sales         float64\n",
      " 2   event_name_1  int32  \n",
      " 3   event_type_1  int32  \n",
      " 4   event_name_2  int32  \n",
      " 5   event_type_2  int32  \n",
      " 6   snap_CA       int8   \n",
      " 7   snap_TX       int8   \n",
      " 8   snap_WI       int8   \n",
      " 9   tm_d          int8   \n",
      " 10  tm_w          int8   \n",
      " 11  tm_m          int8   \n",
      " 12  tm_y          int8   \n",
      " 13  tm_wm         int8   \n",
      " 14  tm_dw         int8   \n",
      " 15  tm_w_end      int8   \n",
      "dtypes: float16(1), float64(1), int32(4), int8(10)\n",
      "memory usage: 164.4 MB\n"
     ]
    }
   ],
   "source": [
    "# grid_df[TS_COLUMNS].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 56\n",
    "def gen_dataset(series, window_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size))\n",
    "    dataset = dataset.map(lambda window: (window[:-28], window[-28:]))\n",
    "    return dataset\n",
    "\n",
    "def get_train_valid(grid, ts_columns, static_columns, window_size = 56):\n",
    "    \n",
    "    X_train, y_train = [],[]\n",
    "    X_train2 = []\n",
    "    for id_, group in tqdm(grid.groupby('id')):\n",
    "        if group.shape[0] == 0:\n",
    "            continue\n",
    "        train_dataset = gen_dataset(group[ts_columns].values, window_size)\n",
    "        old_n_X_train = len(X_train)\n",
    "        idx = group['sales'].shape[0]-window_size+1\n",
    "        train_dataset_len = 0\n",
    "        for x, y in train_dataset:\n",
    "            X_train.append(x.numpy())# = np.vstack([X_train, x.numpy()])\n",
    "            y_train.append(y.numpy())# = np.vstack([y_train, y.numpy()])\n",
    "            train_dataset_len += 1\n",
    "        \n",
    "        sub_group = group.iloc[window_size//2-1:idx-1+window_size//2]\n",
    "        if (len(X_train)-old_n_X_train) != (sub_group.shape[0]):\n",
    "            print(id_, len(X_train)-old_n_X_train, sub_group.shape[0], group.shape[0])\n",
    "        else:\n",
    "            X_train2.append(sub_group[static_columns])\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)[:,:,1:2]\n",
    "    X_train2 = np.vstack(X_train2)\n",
    "    \n",
    "    return X_train, y_train, X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_part1 = np.load(f'{BASE_PATH}/y_part1_{STORE_CA3}.npy')\n",
    "# y_part1 = y_part1[:,:,1:2]\n",
    "# y_part2 = y_part2[:,:,1:2]\n",
    "# y_part3 = y_part3[:,:,1:2]\n",
    "# y_part4 = y_part4[:,:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{BASE_PATH}/X_part1_{TEST_STORE_ID}.npy'):\n",
    "    X_part1 = np.load(f'{BASE_PATH}/X_part1_{TEST_STORE_ID}.npy')\n",
    "    y_part1 = np.load(f'{BASE_PATH}/y_part1_{TEST_STORE_ID}.npy')\n",
    "    X_part2 = np.load(f'{BASE_PATH}/X_part2_{TEST_STORE_ID}.npy')\n",
    "    y_part2 = np.load(f'{BASE_PATH}/y_part2_{TEST_STORE_ID}.npy')\n",
    "    X_part3 = np.load(f'{BASE_PATH}/X_part3_{TEST_STORE_ID}.npy')\n",
    "    y_part3 = np.load(f'{BASE_PATH}/y_part3_{TEST_STORE_ID}.npy')\n",
    "    X_part4 = np.load(f'{BASE_PATH}/X_part4_{TEST_STORE_ID}.npy')\n",
    "    y_part4 = np.load(f'{BASE_PATH}/y_part4_{TEST_STORE_ID}.npy')\n",
    "    \n",
    "else:\n",
    "\n",
    "    mask_part1 = (grid_df['d']>START_TRAIN)&(grid_df['d']<=END_TRAIN-2*part_len)\n",
    "    mask_part2 = (grid_df['d']>START_TRAIN+part_len)&(grid_df['d']<=END_TRAIN-part_len)\n",
    "    mask_part3 = (grid_df['d']>START_TRAIN+2*part_len)&(grid_df['d']<=END_TRAIN)\n",
    "    mask_part4 = (grid_df['d']>END_TRAIN-28-28)&(grid_df['d']<=END_TRAIN)\n",
    "\n",
    "    X_part1, y_part1, _ = get_train_valid(grid_df[mask_part1], TS_COLUMNS, STATIC_COLUMNS)\n",
    "    X_part2, y_part2, _ = get_train_valid(grid_df[mask_part2], TS_COLUMNS, STATIC_COLUMNS)\n",
    "    X_part3, y_part3, _ = get_train_valid(grid_df[mask_part3], TS_COLUMNS, STATIC_COLUMNS)\n",
    "    X_part4, y_part4, _ = get_train_valid(grid_df[mask_part4],TS_COLUMNS, STATIC_COLUMNS)\n",
    "\n",
    "    np.save(f'{BASE_PATH}/X_part1_{TEST_STORE_ID}.npy', X_part1) # save\n",
    "    np.save(f'{BASE_PATH}/y_part1_{TEST_STORE_ID}.npy', y_part1) # save\n",
    "    np.save(f'{BASE_PATH}/X_part2_{TEST_STORE_ID}.npy', X_part2) # save\n",
    "    np.save(f'{BASE_PATH}/y_part2_{TEST_STORE_ID}.npy', y_part2) # save\n",
    "    np.save(f'{BASE_PATH}/X_part3_{TEST_STORE_ID}.npy', X_part3) # save\n",
    "    np.save(f'{BASE_PATH}/y_part3_{TEST_STORE_ID}.npy', y_part3) # save\n",
    "    np.save(f'{BASE_PATH}/X_part4_{TEST_STORE_ID}.npy', X_part4) # save\n",
    "    np.save(f'{BASE_PATH}/y_part4_{TEST_STORE_ID}.npy', y_part4) # save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack([X_part1, X_part2])\n",
    "y_train = np.vstack([y_part1, y_part2])\n",
    "X_valid = X_part3\n",
    "y_valid = y_part3\n",
    "X_valid2 = X_part4\n",
    "y_valid2 = y_part4\n",
    "train_sample_index = np.random.choice(X_train.shape[0], 500_000)\n",
    "valid_sample_index = np.random.choice(X_valid.shape[0], 200_000)\n",
    "X_train = X_train[train_sample_index].astype('float32')\n",
    "y_train = y_train[train_sample_index].astype('float32')\n",
    "X_valid = X_valid[valid_sample_index].astype('float32')\n",
    "y_valid = y_valid[valid_sample_index].astype('float32')\n",
    "X_valid2 = X_valid2.astype('float32')\n",
    "y_valid2 = y_valid2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, SimpleRNN, Dense, GRU, LSTM, Reshape, Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "class TSBase:\n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def compile(self, optimizer=Adam(1e-4), loss='mse', metrics=['mse']):\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    def save(self,model_name):\n",
    "        self.model.save(model_name+'.h5')\n",
    "        pickle.dump(self.class_info,open(model_name+'.info','wb'))\n",
    "\n",
    "    def data_preprocessing(self,x, y = None):\n",
    "        # overload this method if it doesn't fit your need. \n",
    "        x_new = x\n",
    "        y_new = y\n",
    "\n",
    "        return x_new, y_new\n",
    "\n",
    "    def fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, validation_data=None):\n",
    "        x_train, y_train = self.data_preprocessing(x, y)\n",
    "        if validation_data != None:\n",
    "            x_test, y_test = self.data_preprocessing(validation_data[0], validation_data[1])\n",
    "            validation_data = (x_test, y_test)\n",
    "\n",
    "        self.history=self.model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=verbose,\n",
    "        validation_data=validation_data,\n",
    "        shuffle=False)\n",
    "\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        x_test, _ = self.data_preprocessing(x_test)\n",
    "        y_predict = self.model.predict(x_test)\n",
    "        return y_predict\n",
    "    \n",
    "class Seq2Seq_2(TSBase):\n",
    "    def __init__(self, input_shape, output_shape, cell, cell_units, reload = False ):\n",
    "        \"\"\"\n",
    "            input_shape: shape of input data, (n_memory_steps, n_in_features)\n",
    "            output_shape: shape of output data, (n_forcast_steps, n_out_features)\n",
    "            cell: cell in the RNN part, 'SimpleRNN' / 'LSTM' / 'GRU'\n",
    "            cell_units: number of hidden cell unit in RNN part, integer, e.g. 100\n",
    "            reload: True if feed externally, False if generate from scratch \n",
    "        \"\"\"\n",
    "        if reload:\n",
    "            self.model = None\n",
    "            self.class_info = None\n",
    "        else:\n",
    "            # just for future reload\n",
    "            self.class_info = {'class': 'Seq2Seq', 'input_shape': input_shape, 'output_shape': output_shape,\n",
    "                'cell': cell, 'cell_units': cell_units}\n",
    "\n",
    "            if cell == 'LSTM':\n",
    "                # declare encoder and decoder objects\n",
    "                encoder = LSTM(units = cell_units, return_state = True)\n",
    "                decoder = LSTM(units = cell_units, return_sequences=True, return_state = True)\n",
    "                decoder_dense = Dense(output_shape[-1])\n",
    "\n",
    "                # data flow\n",
    "                encoder_input = Input(input_shape)\n",
    "                encoder_output, state_h, state_c = encoder(encoder_input)\n",
    "                encoder_state = [state_h, state_c]  \n",
    "                \n",
    "                # initial input and state for iteration\n",
    "                iter_input = Reshape((1,output_shape[-1]))(decoder_dense(state_h))\n",
    "                iter_state = encoder_state\n",
    "                all_output = []\n",
    "\n",
    "                for _ in range(output_shape[0]):\n",
    "                    # Run the decoder on one timestep, output == state_h since only one time step\n",
    "                    output, state_h, state_c = decoder(iter_input, initial_state=iter_state)\n",
    "                    output = decoder_dense(output)\n",
    "\n",
    "                    # Store the current prediction (we will concatenate all predictions later)\n",
    "                    all_output.append(output)\n",
    "                    \n",
    "                    # Reinject the outputs and state for the next loop iteration\n",
    "                    iter_input = output\n",
    "                    iter_state = [state_h, state_c]\n",
    "\n",
    "            elif cell == 'SimpleRNN':\n",
    "                # declare encoder and decoder objects\n",
    "                encoder = SimpleRNN(units = cell_units, return_state = True)\n",
    "                decoder = SimpleRNN(units = cell_units, return_sequences=True, return_state = True)\n",
    "                decoder_dense = Dense(output_shape[-1])\n",
    "\n",
    "                # data flow\n",
    "                encoder_input = Input(input_shape)\n",
    "                encoder_output, state_h = encoder(encoder_input)\n",
    "                encoder_state = state_h  \n",
    "                \n",
    "                # initial input and state for iteration\n",
    "                iter_input = Reshape((1,output_shape[-1]))(decoder_dense(state_h))\n",
    "                iter_state = encoder_state\n",
    "                all_output = []\n",
    "\n",
    "                for _ in range(output_shape[0]):\n",
    "                    # Run the decoder on one timestep, output == state_h since only one time step\n",
    "                    output, state_h = decoder(iter_input, initial_state=iter_state)\n",
    "                    output = decoder_dense(output)\n",
    "\n",
    "                    # Store the current prediction (we will concatenate all predictions later)\n",
    "                    all_output.append(output)\n",
    "                    \n",
    "                    # Reinject the outputs and state for the next loop iteration\n",
    "                    iter_input = output\n",
    "                    iter_state = state_h\n",
    "\n",
    "            elif cell == 'GRU':\n",
    "                # declare encoder and decoder objects\n",
    "                encoder = GRU(units = cell_units, return_state = True)\n",
    "                decoder = GRU(units = cell_units, return_sequences=True, return_state = True)\n",
    "                decoder_dense = Dense(output_shape[-1])\n",
    "\n",
    "                # data flow\n",
    "                encoder_input = Input(input_shape)\n",
    "                encoder_output, state_h = encoder(encoder_input)\n",
    "                encoder_state = state_h  \n",
    "                \n",
    "                # initial input and state for iteration\n",
    "                iter_input = Reshape((1,output_shape[-1]))(decoder_dense(state_h))\n",
    "                iter_state = encoder_state\n",
    "                all_output = []\n",
    "\n",
    "                for _ in range(output_shape[0]):\n",
    "                    # Run the decoder on one timestep, output == state_h since only one time step\n",
    "                    output, state_h = decoder(iter_input, initial_state=iter_state)\n",
    "                    output = decoder_dense(output)\n",
    "\n",
    "                    # Store the current prediction (we will concatenate all predictions later)\n",
    "                    all_output.append(output)\n",
    "                    \n",
    "                    # Reinject the outputs and state for the next loop iteration\n",
    "                    iter_input = output\n",
    "                    iter_state = state_h\n",
    "\n",
    "            # Concatenate all predictions\n",
    "            decoder_output = Lambda(lambda x: K.concatenate(x, axis=1))(all_output)\n",
    "            self.model = Model(encoder_input, decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq_2(input_shape=(28,16), output_shape=(28,1), cell='LSTM', cell_units=64)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/10\n",
      "500000/500000 [==============================] - 1965s 4ms/step - loss: 12.1969 - val_loss: 6.6243\n",
      "Epoch 2/10\n",
      "500000/500000 [==============================] - 1913s 4ms/step - loss: 10.7111 - val_loss: 6.4525\n",
      "Epoch 3/10\n",
      "500000/500000 [==============================] - 1913s 4ms/step - loss: 10.2785 - val_loss: 6.3181\n",
      "Epoch 4/10\n",
      "500000/500000 [==============================] - 1919s 4ms/step - loss: 10.0152 - val_loss: 6.2595\n",
      "Epoch 5/10\n",
      "311024/500000 [=================>............] - ETA: 10:02 - loss: 9.7748"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0d74704e229d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-76a2944f01a9>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, validation_data)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         shuffle=False)\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=28, epochs=10, verbose=1, validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0\n",
      "batch loss Tensor(\"truediv_16:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6457da03d533>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-febee52e700a>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, encoder_inputs, decoder_outputs, epochs, verbose)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                 \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\u001b[0m in \u001b[0;36m_ReshapeGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         message=\"Converting sparse IndexedSlices to a dense Tensor.*\")\n\u001b[1;32m--> 540\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   7713\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7714\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 7715\u001b[1;33m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[0;32m   7716\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7717\u001b[0m     result = _dispatch.dispatch(\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3614\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3615\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3616\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3617\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3618\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2025\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   2026\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 2027\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   2028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2029\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1864\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1865\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train, epochs=1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000021873BE5948>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000021873BE5948>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000021873BE5948>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000021873BE5948>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n\n    <ipython-input-10-3e083c6e9192>:52 call\n        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:663 __call__\n        inputs, outputs, args, kwargs)\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1708 _set_connectivity_metadata_\n        input_tensors=inputs, output_tensors=outputs, arguments=kwargs)\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1795 _add_inbound_node\n        input_tensors)\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:515 map_structure\n        structure[0], [func(*x) for x in entries],\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:515 <listcomp>\n        structure[0], [func(*x) for x in entries],\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1794 <lambda>\n        inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\n    AttributeError: 'tuple' object has no attribute 'layer'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9755f8fcc83e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_mlp_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-3e083c6e9192>\u001b[0m in \u001b[0;36mbuild_graph\u001b[1;34m(latent_dim, n_timeseries, n_input_dims, n_mlp_features, weights_path)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBahdanauAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mcontext_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mdecoder_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timeseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_input_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in converted code:\n\n    <ipython-input-10-3e083c6e9192>:52 call\n        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:663 __call__\n        inputs, outputs, args, kwargs)\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1708 _set_connectivity_metadata_\n        input_tensors=inputs, output_tensors=outputs, arguments=kwargs)\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1795 _add_inbound_node\n        input_tensors)\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:515 map_structure\n        structure[0], [func(*x) for x in entries],\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:515 <listcomp>\n        structure[0], [func(*x) for x in entries],\n    C:\\Users\\wangz\\.conda\\envs\\tf_gpu1.14\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1794 <lambda>\n        inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\n    AttributeError: 'tuple' object has no attribute 'layer'\n"
     ]
    }
   ],
   "source": [
    "model = build_graph(64, 28, 16, n_mlp_features=None, weights_path=None)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack([X_part1, X_part2])\n",
    "y_train = np.vstack([y_part1, y_part2])\n",
    "X_valid = X_part3\n",
    "y_valid = y_part3\n",
    "X_valid2 = X_part4\n",
    "y_valid2 = y_part4\n",
    "train_sample_index = np.random.choice(X_train.shape[0], 500_000)\n",
    "valid_sample_index = np.random.choice(X_valid.shape[0], 200_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/1\n",
      "500000/500000 [==============================] - 3484s 7ms/step - loss: 10.2177 - root_mean_squared_error: 3.2851 - val_loss: 5.7234 - val_root_mean_squared_error: 3.0826\n",
      "3049/3049 [==============================] - 3s 1ms/step\n",
      "[('loss', 4.647151498882135), ('root_mean_squared_error', 2.987672805786133)]\n"
     ]
    }
   ],
   "source": [
    "his = model.fit([X_train[train_sample_index],X_train[train_sample_index]],y_train[train_sample_index], validation_data=([X_valid[valid_sample_index],X_valid[valid_sample_index]], y_valid[valid_sample_index]), batch_size=14, epochs=1)\n",
    "score = model.evaluate([X_valid2,X_valid2], y_valid2)\n",
    "print(list(zip(model.metrics_names, score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq\n",
    "Train on 500000 samples, validate on 200000 samples\n",
    "Epoch 1/1\n",
    "500000/500000 [==============================] - 1252s 3ms/step - loss: 10.5394 - root_mean_squared_error: 3.3513 - val_loss: 5.8176 - val_root_mean_squared_error: 3.1287\n",
    "3049/3049 [==============================] - 1s 370us/step\n",
    "[('loss', 4.872688117985962), ('root_mean_squared_error', 3.030351400375366)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi seq2seq\n",
    "Train on 500000 samples, validate on 200000 samples\n",
    "Epoch 1/1\n",
    "500000/500000 [==============================] - 3484s 7ms/step - loss: 10.2177 - root_mean_squared_error: 3.2851 - val_loss: 5.7234 - val_root_mean_squared_error: 3.0826\n",
    "3049/3049 [==============================] - 3s 1ms/step\n",
    "[('loss', 4.647151498882135), ('root_mean_squared_error', 2.987672805786133)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "  else:\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 56\n",
    "def gen_dataset(series, window_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size))\n",
    "    dataset = dataset.map(lambda window: (window[:-28], window[-28:]))\n",
    "    return dataset\n",
    "\n",
    "def get_train_valid(grid, window_size = 56):\n",
    "    \n",
    "    X_train, y_train = [],[]\n",
    "    X_train2 = []\n",
    "    for id_, group in tqdm(grid.groupby('id')):\n",
    "        if group.shape[0] == 0:\n",
    "            continue\n",
    "        train_dataset = gen_dataset(group[['sales']].values, window_size)\n",
    "        old_n_X_train = len(X_train)\n",
    "        idx = group['sales'].shape[0]-window_size+1\n",
    "        train_dataset_len = 0\n",
    "        for x, y in train_dataset:\n",
    "            X_train.append(x.numpy())# = np.vstack([X_train, x.numpy()])\n",
    "            y_train.append(y.numpy())# = np.vstack([y_train, y.numpy()])\n",
    "            train_dataset_len += 1\n",
    "        \n",
    "        if (len(X_train)-old_n_X_train) != (group.iloc[window_size//2:idx+window_size//2].shape[0]):\n",
    "            print(id_, len(X_train)-old_n_X_train, group.iloc[window_size//2:idx+window_size//2].shape[0], group.shape[0])\n",
    "        else:\n",
    "            X_train2.append(group.iloc[window_size//2-1:idx-1+window_size//2][MLP_COLUMNS])\n",
    "#             break\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_train2 = np.vstack(X_train2)\n",
    "    \n",
    "    return X_train, y_train, X_train2\n",
    "\n",
    "\n",
    "def build_graph(latent_dim, n_timeseries, n_mlp_features, weights_path=None):\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(28, 1))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(28, 1))\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(1, activation='relu')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Run training\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    # model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "    #           batch_size=batch_size,\n",
    "    #           epochs=epochs,\n",
    "    #           validation_split=0.2)\n",
    "    \n",
    "#     if type(None) != type(weights_path):\n",
    "#         model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(X_train, X2_train, y_train, X_valid, X2_valid, y_valid, store_id, base_path, n_timeseries, n_mlp_features, batch_size=10, epochs=1):\n",
    "    model = build_graph(n_timeseries, n_mlp_features, None)\n",
    "    his = model.fit([X_train,X2_train], y_train, validation_data=([X_valid,X2_valid], y_valid), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "    model.save_weights(f'{base_path}/bilstm_{store_id}.h5')\n",
    "    return model\n",
    "\n",
    "# def predict_series(model, series, mlp_input, n_timeseries):\n",
    "    \n",
    "#     lst = []\n",
    "#     input1 = series\n",
    "#     input2 = mlp_input\n",
    "#     for i in range(29):\n",
    "#         pred = model.predict([input1.reshape(1,n_timeseries), input2])\n",
    "#         input1 = np.hstack((input1,pred[0]))[-n_timeseries:]\n",
    "#         lst.append(pred[0][0])\n",
    "#     return np.array(lst)\n",
    "\n",
    "def predict_samples(grid_df, model):\n",
    "    pred_df = pd.DataFrame(columns=['id']+[f'F{i}' for i in range(1,29)])\n",
    "    idx = 0\n",
    "    for id_, group in tqdm(grid_df.groupby('id')):\n",
    "        if group.shape[0] == 0:\n",
    "            continue\n",
    "        input1 = group[group['d']<=1913]['sales'].values[-28:].reshape(1,28)\n",
    "        input2 = group[group['d']<=1913].iloc[-1][MLP_COLUMNS].values.reshape(1,39)\n",
    "        predictions = model.predict([input1.astype(float), input2.astype(float)])\n",
    "        pred_df.loc[idx] = dict(zip(pred_df.columns.tolist(),[id_,*predictions[0]]))\n",
    "        idx += 1\n",
    "    return pred_df\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id= 'CA_3'\n",
    "X_part1 = np.load(f'{SAV_BASE_PATH}/X_part1_{store_id}.npy') # load\n",
    "X2_part1 = np.load(f'{SAV_BASE_PATH}/X2_part1_{store_id}.npy') # load\n",
    "y_part1 = np.load(f'{SAV_BASE_PATH}/X_part1_{store_id}.npy') # load\n",
    "X_part2 = np.load(f'{SAV_BASE_PATH}/X_part2_{store_id}.npy') # load\n",
    "X2_part2 = np.load(f'{SAV_BASE_PATH}/X2_part2_{store_id}.npy') # load\n",
    "y_part2 = np.load(f'{SAV_BASE_PATH}/X_part2_{store_id}.npy') # load\n",
    "X_part3 = np.load(f'{SAV_BASE_PATH}/X_part3_{store_id}.npy') # load\n",
    "X2_part3 = np.load(f'{SAV_BASE_PATH}/X2_part3_{store_id}.npy') # load\n",
    "y_part3 = np.load(f'{SAV_BASE_PATH}/X_part3_{store_id}.npy') # load    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part4 = (grid_df['d']>END_TRAIN-28-28)&(grid_df['d']<=END_TRAIN)\n",
    "\n",
    "X_part4, y_part4, X2_part4 = get_train_valid(grid_df[part4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(np.vstack([X_part1, X_part2]),-1)\n",
    "X2_train = np.vstack([X2_part1, X2_part2])\n",
    "y_train = np.expand_dims(np.vstack([y_part1, y_part2]),-1)\n",
    "X_valid = np.expand_dims(X_part3,-1)\n",
    "X2_valid = X2_part3\n",
    "y_valid = np.expand_dims(y_part3,-1)\n",
    "train_sample_index = np.random.choice(X_train.shape[0], 500_000)\n",
    "valid_sample_index = np.random.choice(X_valid.shape[0], 200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 28, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 28, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  [(None, 64), (None,  16896       input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  [(None, 28, 64), (No 16896       input_17[0][0]                   \n",
      "                                                                 lstm_15[0][1]                    \n",
      "                                                                 lstm_15[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 28, 1)        65          lstm_16[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,857\n",
      "Trainable params: 33,857\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_graph(64, 28, n_mlp_features=None, weights_path=None)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/1\n",
      "500000/500000 [==============================] - 1195s 2ms/step - loss: 5.5962 - root_mean_squared_error: 3.0428 - val_loss: 0.2000 - val_root_mean_squared_error: 2.1743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f9f1b03ac8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train[train_sample_index],X_train[train_sample_index]],y_train[train_sample_index], validation_data=([X_valid[valid_sample_index],X_valid[valid_sample_index]], y_valid[valid_sample_index]), batch_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_GRID_DF = pd.read_pickle(f'{SAV_BASE_PATH}/BASE_FEATURES.pkl')\n",
    "grid_df = BASE_GRID_DF[(BASE_GRID_DF['store_id']==store_id)]\n",
    "del BASE_GRID_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_samples(grid_df, model, endpoint=1913):\n",
    "    pred_df = pd.DataFrame(columns=['id']+[f'F{i}' for i in range(1,29)])\n",
    "    idx = 0\n",
    "    for id_, group in tqdm(grid_df.groupby('id')):\n",
    "        if group.shape[0] == 0:\n",
    "            continue\n",
    "        input1 = group[group['d']<=endpoint]['sales'].values[-28:].reshape(1,28,1)\n",
    "#         input2 = group[group['d']<=1913].iloc[-1][MLP_COLUMNS].values.reshape(1,39)\n",
    "        predictions = model.predict([input1.astype(float), input1.astype(float)])\n",
    "        pred_df.loc[idx] = dict(zip(pred_df.columns.tolist(),[id_,*predictions.reshape(-1)]))\n",
    "        idx += 1\n",
    "    return pred_df\n",
    "\n",
    "# pred_df = predict_samples(grid_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid(grid, window_size = 56):\n",
    "    \n",
    "    X_train, y_train = [],[]\n",
    "    X_train2 = []\n",
    "    for id_, group in tqdm(grid.groupby('id')):\n",
    "        if group.shape[0] == 0:\n",
    "            continue\n",
    "        train_dataset = gen_dataset(group[['sales']].values, window_size)\n",
    "        old_n_X_train = len(X_train)\n",
    "        idx = group['sales'].shape[0]-window_size+1\n",
    "        train_dataset_len = 0\n",
    "        for x, y in train_dataset:\n",
    "            X_train.append(x.numpy())# = np.vstack([X_train, x.numpy()])\n",
    "            y_train.append(y.numpy())# = np.vstack([y_train, y.numpy()])\n",
    "            train_dataset_len += 1\n",
    "        \n",
    "        if (len(X_train)-old_n_X_train) != (group.iloc[window_size//2:idx+window_size//2].shape[0]):\n",
    "            print(id_, len(X_train)-old_n_X_train, group.iloc[window_size//2:idx+window_size//2].shape[0], group.shape[0])\n",
    "        else:\n",
    "            X_train2.append(group.iloc[window_size//2-1:idx-1+window_size//2][MLP_COLUMNS])\n",
    "#             break\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_train2 = np.vstack(X_train2)\n",
    "    \n",
    "    return X_train, y_train, X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [01:05<00:00, 462.49it/s]\n"
     ]
    }
   ],
   "source": [
    "part4 = (grid_df['d']>END_TRAIN-28-28)&(grid_df['d']<=END_TRAIN)\n",
    "\n",
    "X_part4, y_part4, X2_part4 = get_train_valid(grid_df[part4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3049, 28, 1), (3049, 28, 1), (3049, 39))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_part4.shape, y_part4.shape, X2_part4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_part4.astype(float), X_part4.astype(float)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.756327027633038"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y_part4.reshape(-1), predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI','tm_d','tm_w','tm_m','tm_y','tm_wm','tm_dw','tm_w_end,enc_store_id_tm_dw_item_id_mean','enc_store_id_tm_dw_item_id_std','enc_store_id_tm_dw_mean','enc_store_id_tm_dw_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>release</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>price_max</th>\n",
       "      <th>price_min</th>\n",
       "      <th>price_std</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>price_norm</th>\n",
       "      <th>price_nunique</th>\n",
       "      <th>item_nunique</th>\n",
       "      <th>price_momentum</th>\n",
       "      <th>price_momentum_m</th>\n",
       "      <th>price_momentum_y</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>tm_d</th>\n",
       "      <th>tm_w</th>\n",
       "      <th>tm_m</th>\n",
       "      <th>tm_y</th>\n",
       "      <th>tm_wm</th>\n",
       "      <th>tm_dw</th>\n",
       "      <th>tm_w_end</th>\n",
       "      <th>enc_store_id_cat_id_mean</th>\n",
       "      <th>enc_store_id_cat_id_std</th>\n",
       "      <th>enc_store_id_dept_id_mean</th>\n",
       "      <th>enc_store_id_dept_id_std</th>\n",
       "      <th>enc_store_id_item_id_mean</th>\n",
       "      <th>enc_store_id_item_id_std</th>\n",
       "      <th>enc_store_id_tm_dw_item_id_mean</th>\n",
       "      <th>enc_store_id_tm_dw_item_id_std</th>\n",
       "      <th>enc_store_id_tm_dw_mean</th>\n",
       "      <th>enc_store_id_tm_dw_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44328227</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44358717</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1859</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>6.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44389207</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.856445</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44419697</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.829102</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>6.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44450187</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.279297</td>\n",
       "      <td>2.527344</td>\n",
       "      <td>2.289062</td>\n",
       "      <td>6.824219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44480677</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>2.447266</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>7.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44511167</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.836914</td>\n",
       "      <td>3.369141</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>7.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44541657</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44572147</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>6.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44602637</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.856445</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44633127</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.829102</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>6.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44663617</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.279297</td>\n",
       "      <td>2.527344</td>\n",
       "      <td>2.289062</td>\n",
       "      <td>6.824219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44694107</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>2.447266</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>7.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44724597</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.836914</td>\n",
       "      <td>3.369141</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>7.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44755087</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1872</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44785577</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>6.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44816067</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.856445</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44846557</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.829102</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>6.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44877047</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.279297</td>\n",
       "      <td>2.527344</td>\n",
       "      <td>2.289062</td>\n",
       "      <td>6.824219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44907537</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1877</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>2.447266</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>7.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44938027</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1878</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.836914</td>\n",
       "      <td>3.369141</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>7.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44968517</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1879</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44999007</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>6.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45029497</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1881</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.856445</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45059987</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.829102</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>6.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45090477</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1883</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.279297</td>\n",
       "      <td>2.527344</td>\n",
       "      <td>2.289062</td>\n",
       "      <td>6.824219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45120967</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1884</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>2.447266</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>7.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45151457</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.836914</td>\n",
       "      <td>3.369141</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>7.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45181947</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45212437</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>6.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45242927</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1888</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.856445</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45273417</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.829102</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>6.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45303907</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.279297</td>\n",
       "      <td>2.527344</td>\n",
       "      <td>2.289062</td>\n",
       "      <td>6.824219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45334397</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>2.447266</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>7.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45364887</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.836914</td>\n",
       "      <td>3.369141</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>7.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45395377</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45425867</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>6.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45456357</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.856445</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45486847</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.829102</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>6.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45517337</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.279297</td>\n",
       "      <td>2.527344</td>\n",
       "      <td>2.289062</td>\n",
       "      <td>6.824219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45547827</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1898</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>2.447266</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>7.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45578317</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1899</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.836914</td>\n",
       "      <td>3.369141</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>7.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45608807</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45639297</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>6.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45669787</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.856445</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45700277</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.829102</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>6.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45730767</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.279297</td>\n",
       "      <td>2.527344</td>\n",
       "      <td>2.289062</td>\n",
       "      <td>6.824219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45761257</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>2.447266</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>7.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45791747</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1906</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.836914</td>\n",
       "      <td>3.369141</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>7.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45822237</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45852727</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>6.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45883217</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.856445</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45913707</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>0.829102</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>6.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45944197</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.279297</td>\n",
       "      <td>2.527344</td>\n",
       "      <td>2.289062</td>\n",
       "      <td>6.824219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45974687</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1912</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>2.447266</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>7.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46005177</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>2.158203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.038086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.836914</td>\n",
       "      <td>3.369141</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>7.929688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  item_id  dept_id  cat_id store_id  \\\n",
       "44328227  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44358717  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44389207  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44419697  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44450187  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44480677  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44511167  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44541657  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44572147  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44602637  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44633127  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44663617  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44694107  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44724597  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44755087  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44785577  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44816067  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44846557  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44877047  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44907537  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44938027  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44968517  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "44999007  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45029497  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45059987  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45090477  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45120967  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45151457  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45181947  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45212437  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45242927  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45273417  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45303907  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45334397  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45364887  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45395377  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45425867  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45456357  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45486847  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45517337  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45547827  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45578317  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45608807  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45639297  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45669787  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45700277  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45730767  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45761257  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45791747  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45822237  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45852727  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45883217  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45913707  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45944197  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "45974687  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "46005177  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "\n",
       "         state_id     d  sales  release  sell_price  price_max  price_min  \\\n",
       "44328227       CA  1858    1.0        0    2.240234   2.240234       1.75   \n",
       "44358717       CA  1859    2.0        0    2.240234   2.240234       1.75   \n",
       "44389207       CA  1860    0.0        0    2.240234   2.240234       1.75   \n",
       "44419697       CA  1861    0.0        0    2.240234   2.240234       1.75   \n",
       "44450187       CA  1862    1.0        0    2.240234   2.240234       1.75   \n",
       "44480677       CA  1863    0.0        0    2.240234   2.240234       1.75   \n",
       "44511167       CA  1864    1.0        0    2.240234   2.240234       1.75   \n",
       "44541657       CA  1865    0.0        0    2.240234   2.240234       1.75   \n",
       "44572147       CA  1866    0.0        0    2.240234   2.240234       1.75   \n",
       "44602637       CA  1867    1.0        0    2.240234   2.240234       1.75   \n",
       "44633127       CA  1868    0.0        0    2.240234   2.240234       1.75   \n",
       "44663617       CA  1869    0.0        0    2.240234   2.240234       1.75   \n",
       "44694107       CA  1870    0.0        0    2.240234   2.240234       1.75   \n",
       "44724597       CA  1871    0.0        0    2.240234   2.240234       1.75   \n",
       "44755087       CA  1872    2.0        0    2.240234   2.240234       1.75   \n",
       "44785577       CA  1873    0.0        0    2.240234   2.240234       1.75   \n",
       "44816067       CA  1874    0.0        0    2.240234   2.240234       1.75   \n",
       "44846557       CA  1875    0.0        0    2.240234   2.240234       1.75   \n",
       "44877047       CA  1876    1.0        0    2.240234   2.240234       1.75   \n",
       "44907537       CA  1877    2.0        0    2.240234   2.240234       1.75   \n",
       "44938027       CA  1878    4.0        0    2.240234   2.240234       1.75   \n",
       "44968517       CA  1879    2.0        0    2.240234   2.240234       1.75   \n",
       "44999007       CA  1880    0.0        0    2.240234   2.240234       1.75   \n",
       "45029497       CA  1881    1.0        0    2.240234   2.240234       1.75   \n",
       "45059987       CA  1882    1.0        0    2.240234   2.240234       1.75   \n",
       "45090477       CA  1883    1.0        0    2.240234   2.240234       1.75   \n",
       "45120967       CA  1884   12.0        0    2.240234   2.240234       1.75   \n",
       "45151457       CA  1885    0.0        0    2.240234   2.240234       1.75   \n",
       "45181947       CA  1886    0.0        0    2.240234   2.240234       1.75   \n",
       "45212437       CA  1887    0.0        0    2.240234   2.240234       1.75   \n",
       "45242927       CA  1888    1.0        0    2.240234   2.240234       1.75   \n",
       "45273417       CA  1889    0.0        0    2.240234   2.240234       1.75   \n",
       "45303907       CA  1890    0.0        0    2.240234   2.240234       1.75   \n",
       "45334397       CA  1891    1.0        0    2.240234   2.240234       1.75   \n",
       "45364887       CA  1892    0.0        0    2.240234   2.240234       1.75   \n",
       "45395377       CA  1893    0.0        0    2.240234   2.240234       1.75   \n",
       "45425867       CA  1894    0.0        0    2.240234   2.240234       1.75   \n",
       "45456357       CA  1895    1.0        0    2.240234   2.240234       1.75   \n",
       "45486847       CA  1896    0.0        0    2.240234   2.240234       1.75   \n",
       "45517337       CA  1897    0.0        0    2.240234   2.240234       1.75   \n",
       "45547827       CA  1898    4.0        0    2.240234   2.240234       1.75   \n",
       "45578317       CA  1899    2.0        0    2.240234   2.240234       1.75   \n",
       "45608807       CA  1900    1.0        0    2.240234   2.240234       1.75   \n",
       "45639297       CA  1901    0.0        0    2.240234   2.240234       1.75   \n",
       "45669787       CA  1902    0.0        0    2.240234   2.240234       1.75   \n",
       "45700277       CA  1903    0.0        0    2.240234   2.240234       1.75   \n",
       "45730767       CA  1904    0.0        0    2.240234   2.240234       1.75   \n",
       "45761257       CA  1905    0.0        0    2.240234   2.240234       1.75   \n",
       "45791747       CA  1906   13.0        0    2.240234   2.240234       1.75   \n",
       "45822237       CA  1907    0.0        0    2.240234   2.240234       1.75   \n",
       "45852727       CA  1908    0.0        0    2.240234   2.240234       1.75   \n",
       "45883217       CA  1909    0.0        0    2.240234   2.240234       1.75   \n",
       "45913707       CA  1910    0.0        0    2.240234   2.240234       1.75   \n",
       "45944197       CA  1911    0.0        0    2.240234   2.240234       1.75   \n",
       "45974687       CA  1912    1.0        0    2.240234   2.240234       1.75   \n",
       "46005177       CA  1913    0.0        0    2.240234   2.240234       1.75   \n",
       "\n",
       "          price_std  price_mean  price_norm  price_nunique  item_nunique  \\\n",
       "44328227   0.115845    2.158203         1.0            3.0            68   \n",
       "44358717   0.115845    2.158203         1.0            3.0            68   \n",
       "44389207   0.115845    2.158203         1.0            3.0            68   \n",
       "44419697   0.115845    2.158203         1.0            3.0            68   \n",
       "44450187   0.115845    2.158203         1.0            3.0            68   \n",
       "44480677   0.115845    2.158203         1.0            3.0            68   \n",
       "44511167   0.115845    2.158203         1.0            3.0            68   \n",
       "44541657   0.115845    2.158203         1.0            3.0            68   \n",
       "44572147   0.115845    2.158203         1.0            3.0            68   \n",
       "44602637   0.115845    2.158203         1.0            3.0            68   \n",
       "44633127   0.115845    2.158203         1.0            3.0            68   \n",
       "44663617   0.115845    2.158203         1.0            3.0            68   \n",
       "44694107   0.115845    2.158203         1.0            3.0            68   \n",
       "44724597   0.115845    2.158203         1.0            3.0            68   \n",
       "44755087   0.115845    2.158203         1.0            3.0            68   \n",
       "44785577   0.115845    2.158203         1.0            3.0            68   \n",
       "44816067   0.115845    2.158203         1.0            3.0            68   \n",
       "44846557   0.115845    2.158203         1.0            3.0            68   \n",
       "44877047   0.115845    2.158203         1.0            3.0            68   \n",
       "44907537   0.115845    2.158203         1.0            3.0            68   \n",
       "44938027   0.115845    2.158203         1.0            3.0            68   \n",
       "44968517   0.115845    2.158203         1.0            3.0            68   \n",
       "44999007   0.115845    2.158203         1.0            3.0            68   \n",
       "45029497   0.115845    2.158203         1.0            3.0            68   \n",
       "45059987   0.115845    2.158203         1.0            3.0            68   \n",
       "45090477   0.115845    2.158203         1.0            3.0            68   \n",
       "45120967   0.115845    2.158203         1.0            3.0            68   \n",
       "45151457   0.115845    2.158203         1.0            3.0            68   \n",
       "45181947   0.115845    2.158203         1.0            3.0            68   \n",
       "45212437   0.115845    2.158203         1.0            3.0            68   \n",
       "45242927   0.115845    2.158203         1.0            3.0            68   \n",
       "45273417   0.115845    2.158203         1.0            3.0            68   \n",
       "45303907   0.115845    2.158203         1.0            3.0            68   \n",
       "45334397   0.115845    2.158203         1.0            3.0            68   \n",
       "45364887   0.115845    2.158203         1.0            3.0            68   \n",
       "45395377   0.115845    2.158203         1.0            3.0            68   \n",
       "45425867   0.115845    2.158203         1.0            3.0            68   \n",
       "45456357   0.115845    2.158203         1.0            3.0            68   \n",
       "45486847   0.115845    2.158203         1.0            3.0            68   \n",
       "45517337   0.115845    2.158203         1.0            3.0            68   \n",
       "45547827   0.115845    2.158203         1.0            3.0            68   \n",
       "45578317   0.115845    2.158203         1.0            3.0            68   \n",
       "45608807   0.115845    2.158203         1.0            3.0            68   \n",
       "45639297   0.115845    2.158203         1.0            3.0            68   \n",
       "45669787   0.115845    2.158203         1.0            3.0            68   \n",
       "45700277   0.115845    2.158203         1.0            3.0            68   \n",
       "45730767   0.115845    2.158203         1.0            3.0            68   \n",
       "45761257   0.115845    2.158203         1.0            3.0            68   \n",
       "45791747   0.115845    2.158203         1.0            3.0            68   \n",
       "45822237   0.115845    2.158203         1.0            3.0            68   \n",
       "45852727   0.115845    2.158203         1.0            3.0            68   \n",
       "45883217   0.115845    2.158203         1.0            3.0            68   \n",
       "45913707   0.115845    2.158203         1.0            3.0            68   \n",
       "45944197   0.115845    2.158203         1.0            3.0            68   \n",
       "45974687   0.115845    2.158203         1.0            3.0            68   \n",
       "46005177   0.115845    2.158203         1.0            3.0            68   \n",
       "\n",
       "          price_momentum  price_momentum_m  price_momentum_y  event_name_1  \\\n",
       "44328227             1.0          1.037109               1.0             0   \n",
       "44358717             1.0          1.037109               1.0             0   \n",
       "44389207             1.0          1.037109               1.0             0   \n",
       "44419697             1.0          1.037109               1.0             0   \n",
       "44450187             1.0          1.037109               1.0             0   \n",
       "44480677             1.0          1.037109               1.0             0   \n",
       "44511167             1.0          1.037109               1.0             0   \n",
       "44541657             1.0          1.037109               1.0             0   \n",
       "44572147             1.0          1.037109               1.0             0   \n",
       "44602637             1.0          1.037109               1.0             0   \n",
       "44633127             1.0          1.037109               1.0             0   \n",
       "44663617             1.0          1.037109               1.0             0   \n",
       "44694107             1.0          1.037109               1.0             0   \n",
       "44724597             1.0          1.037109               1.0             0   \n",
       "44755087             1.0          1.037109               1.0             0   \n",
       "44785577             1.0          1.037109               1.0             0   \n",
       "44816067             1.0          1.037109               1.0             0   \n",
       "44846557             1.0          1.037109               1.0             6   \n",
       "44877047             1.0          1.037109               1.0             0   \n",
       "44907537             1.0          1.037109               1.0             0   \n",
       "44938027             1.0          1.037109               1.0             0   \n",
       "44968517             1.0          1.037109               1.0             0   \n",
       "44999007             1.0          1.037109               1.0             0   \n",
       "45029497             1.0          1.037109               1.0             0   \n",
       "45059987             1.0          1.037109               1.0             7   \n",
       "45090477             1.0          1.037109               1.0             0   \n",
       "45120967             1.0          1.037109               1.0             0   \n",
       "45151457             1.0          1.037109               1.0            30   \n",
       "45181947             1.0          1.037109               1.0             0   \n",
       "45212437             1.0          1.037109               1.0             0   \n",
       "45242927             1.0          1.037109               1.0             0   \n",
       "45273417             1.0          1.037109               1.0             0   \n",
       "45303907             1.0          1.037109               1.0             0   \n",
       "45334397             1.0          1.038086               1.0             0   \n",
       "45364887             1.0          1.038086               1.0             0   \n",
       "45395377             1.0          1.038086               1.0             0   \n",
       "45425867             1.0          1.038086               1.0             0   \n",
       "45456357             1.0          1.038086               1.0             0   \n",
       "45486847             1.0          1.038086               1.0             0   \n",
       "45517337             1.0          1.038086               1.0             0   \n",
       "45547827             1.0          1.038086               1.0             0   \n",
       "45578317             1.0          1.038086               1.0             0   \n",
       "45608807             1.0          1.038086               1.0             0   \n",
       "45639297             1.0          1.038086               1.0             0   \n",
       "45669787             1.0          1.038086               1.0             0   \n",
       "45700277             1.0          1.038086               1.0             0   \n",
       "45730767             1.0          1.038086               1.0             0   \n",
       "45761257             1.0          1.038086               1.0             0   \n",
       "45791747             1.0          1.038086               1.0             0   \n",
       "45822237             1.0          1.038086               1.0             0   \n",
       "45852727             1.0          1.038086               1.0             0   \n",
       "45883217             1.0          1.038086               1.0             0   \n",
       "45913707             1.0          1.038086               1.0             0   \n",
       "45944197             1.0          1.038086               1.0             0   \n",
       "45974687             1.0          1.038086               1.0             0   \n",
       "46005177             1.0          1.038086               1.0             0   \n",
       "\n",
       "          event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  \\\n",
       "44328227             0             0             0        0        0        0   \n",
       "44358717             0             0             0        1        1        0   \n",
       "44389207             0             0             0        1        0        1   \n",
       "44419697             0             0             0        1        1        1   \n",
       "44450187             0             0             0        1        0        0   \n",
       "44480677             0             0             0        1        1        1   \n",
       "44511167             0             0             0        1        1        1   \n",
       "44541657             0             0             0        1        1        0   \n",
       "44572147             0             0             0        1        0        1   \n",
       "44602637             0             0             0        1        1        1   \n",
       "44633127             0             0             0        1        0        0   \n",
       "44663617             0             0             0        0        1        1   \n",
       "44694107             0             0             0        0        1        1   \n",
       "44724597             0             0             0        0        1        0   \n",
       "44755087             0             0             0        0        0        1   \n",
       "44785577             0             0             0        0        1        1   \n",
       "44816067             0             0             0        0        0        0   \n",
       "44846557             2             0             0        0        0        0   \n",
       "44877047             0             0             0        0        0        0   \n",
       "44907537             0             0             0        0        0        0   \n",
       "44938027             0             0             0        0        0        0   \n",
       "44968517             0             0             0        0        0        0   \n",
       "44999007             0             0             0        0        0        0   \n",
       "45029497             0             0             0        0        0        0   \n",
       "45059987             4             0             0        0        0        0   \n",
       "45090477             0             0             0        0        0        0   \n",
       "45120967             0             0             0        0        0        0   \n",
       "45151457             2             0             0        0        0        0   \n",
       "45181947             0             0             0        0        0        0   \n",
       "45212437             0             0             0        0        0        0   \n",
       "45242927             0             0             0        0        0        0   \n",
       "45273417             0             0             0        0        0        0   \n",
       "45303907             0             0             0        1        1        0   \n",
       "45334397             0             0             0        1        0        1   \n",
       "45364887             0             0             0        1        1        1   \n",
       "45395377             0             0             0        1        0        0   \n",
       "45425867             0             0             0        1        1        1   \n",
       "45456357             0             0             0        1        1        1   \n",
       "45486847             0             0             0        1        1        0   \n",
       "45517337             0             0             0        1        0        1   \n",
       "45547827             0             0             0        1        1        1   \n",
       "45578317             0             0             0        1        0        0   \n",
       "45608807             0             0             0        0        1        1   \n",
       "45639297             0             0             0        0        1        1   \n",
       "45669787             0             0             0        0        1        0   \n",
       "45700277             0             0             0        0        0        1   \n",
       "45730767             0             0             0        0        1        1   \n",
       "45761257             0             0             0        0        0        0   \n",
       "45791747             0             0             0        0        0        0   \n",
       "45822237             0             0             0        0        0        0   \n",
       "45852727             0             0             0        0        0        0   \n",
       "45883217             0             0             0        0        0        0   \n",
       "45913707             0             0             0        0        0        0   \n",
       "45944197             0             0             0        0        0        0   \n",
       "45974687             0             0             0        0        0        0   \n",
       "46005177             0             0             0        0        0        0   \n",
       "\n",
       "          tm_d  tm_w  tm_m  tm_y  tm_wm  tm_dw  tm_w_end  \\\n",
       "44328227    29     9     2     5      5      0         0   \n",
       "44358717     1     9     3     5      1      1         0   \n",
       "44389207     2     9     3     5      1      2         0   \n",
       "44419697     3     9     3     5      1      3         0   \n",
       "44450187     4     9     3     5      1      4         0   \n",
       "44480677     5     9     3     5      1      5         1   \n",
       "44511167     6     9     3     5      1      6         1   \n",
       "44541657     7    10     3     5      1      0         0   \n",
       "44572147     8    10     3     5      2      1         0   \n",
       "44602637     9    10     3     5      2      2         0   \n",
       "44633127    10    10     3     5      2      3         0   \n",
       "44663617    11    10     3     5      2      4         0   \n",
       "44694107    12    10     3     5      2      5         1   \n",
       "44724597    13    10     3     5      2      6         1   \n",
       "44755087    14    11     3     5      2      0         0   \n",
       "44785577    15    11     3     5      3      1         0   \n",
       "44816067    16    11     3     5      3      2         0   \n",
       "44846557    17    11     3     5      3      3         0   \n",
       "44877047    18    11     3     5      3      4         0   \n",
       "44907537    19    11     3     5      3      5         1   \n",
       "44938027    20    11     3     5      3      6         1   \n",
       "44968517    21    12     3     5      3      0         0   \n",
       "44999007    22    12     3     5      4      1         0   \n",
       "45029497    23    12     3     5      4      2         0   \n",
       "45059987    24    12     3     5      4      3         0   \n",
       "45090477    25    12     3     5      4      4         0   \n",
       "45120967    26    12     3     5      4      5         1   \n",
       "45151457    27    12     3     5      4      6         1   \n",
       "45181947    28    13     3     5      4      0         0   \n",
       "45212437    29    13     3     5      5      1         0   \n",
       "45242927    30    13     3     5      5      2         0   \n",
       "45273417    31    13     3     5      5      3         0   \n",
       "45303907     1    13     4     5      1      4         0   \n",
       "45334397     2    13     4     5      1      5         1   \n",
       "45364887     3    13     4     5      1      6         1   \n",
       "45395377     4    14     4     5      1      0         0   \n",
       "45425867     5    14     4     5      1      1         0   \n",
       "45456357     6    14     4     5      1      2         0   \n",
       "45486847     7    14     4     5      1      3         0   \n",
       "45517337     8    14     4     5      2      4         0   \n",
       "45547827     9    14     4     5      2      5         1   \n",
       "45578317    10    14     4     5      2      6         1   \n",
       "45608807    11    15     4     5      2      0         0   \n",
       "45639297    12    15     4     5      2      1         0   \n",
       "45669787    13    15     4     5      2      2         0   \n",
       "45700277    14    15     4     5      2      3         0   \n",
       "45730767    15    15     4     5      3      4         0   \n",
       "45761257    16    15     4     5      3      5         1   \n",
       "45791747    17    15     4     5      3      6         1   \n",
       "45822237    18    16     4     5      3      0         0   \n",
       "45852727    19    16     4     5      3      1         0   \n",
       "45883217    20    16     4     5      3      2         0   \n",
       "45913707    21    16     4     5      3      3         0   \n",
       "45944197    22    16     4     5      4      4         0   \n",
       "45974687    23    16     4     5      4      5         1   \n",
       "46005177    24    16     4     5      4      6         1   \n",
       "\n",
       "          enc_store_id_cat_id_mean  enc_store_id_cat_id_std  \\\n",
       "44328227                  3.400391                 9.085938   \n",
       "44358717                  3.400391                 9.085938   \n",
       "44389207                  3.400391                 9.085938   \n",
       "44419697                  3.400391                 9.085938   \n",
       "44450187                  3.400391                 9.085938   \n",
       "44480677                  3.400391                 9.085938   \n",
       "44511167                  3.400391                 9.085938   \n",
       "44541657                  3.400391                 9.085938   \n",
       "44572147                  3.400391                 9.085938   \n",
       "44602637                  3.400391                 9.085938   \n",
       "44633127                  3.400391                 9.085938   \n",
       "44663617                  3.400391                 9.085938   \n",
       "44694107                  3.400391                 9.085938   \n",
       "44724597                  3.400391                 9.085938   \n",
       "44755087                  3.400391                 9.085938   \n",
       "44785577                  3.400391                 9.085938   \n",
       "44816067                  3.400391                 9.085938   \n",
       "44846557                  3.400391                 9.085938   \n",
       "44877047                  3.400391                 9.085938   \n",
       "44907537                  3.400391                 9.085938   \n",
       "44938027                  3.400391                 9.085938   \n",
       "44968517                  3.400391                 9.085938   \n",
       "44999007                  3.400391                 9.085938   \n",
       "45029497                  3.400391                 9.085938   \n",
       "45059987                  3.400391                 9.085938   \n",
       "45090477                  3.400391                 9.085938   \n",
       "45120967                  3.400391                 9.085938   \n",
       "45151457                  3.400391                 9.085938   \n",
       "45181947                  3.400391                 9.085938   \n",
       "45212437                  3.400391                 9.085938   \n",
       "45242927                  3.400391                 9.085938   \n",
       "45273417                  3.400391                 9.085938   \n",
       "45303907                  3.400391                 9.085938   \n",
       "45334397                  3.400391                 9.085938   \n",
       "45364887                  3.400391                 9.085938   \n",
       "45395377                  3.400391                 9.085938   \n",
       "45425867                  3.400391                 9.085938   \n",
       "45456357                  3.400391                 9.085938   \n",
       "45486847                  3.400391                 9.085938   \n",
       "45517337                  3.400391                 9.085938   \n",
       "45547827                  3.400391                 9.085938   \n",
       "45578317                  3.400391                 9.085938   \n",
       "45608807                  3.400391                 9.085938   \n",
       "45639297                  3.400391                 9.085938   \n",
       "45669787                  3.400391                 9.085938   \n",
       "45700277                  3.400391                 9.085938   \n",
       "45730767                  3.400391                 9.085938   \n",
       "45761257                  3.400391                 9.085938   \n",
       "45791747                  3.400391                 9.085938   \n",
       "45822237                  3.400391                 9.085938   \n",
       "45852727                  3.400391                 9.085938   \n",
       "45883217                  3.400391                 9.085938   \n",
       "45913707                  3.400391                 9.085938   \n",
       "45944197                  3.400391                 9.085938   \n",
       "45974687                  3.400391                 9.085938   \n",
       "46005177                  3.400391                 9.085938   \n",
       "\n",
       "          enc_store_id_dept_id_mean  enc_store_id_dept_id_std  \\\n",
       "44328227                   2.103516                  3.941406   \n",
       "44358717                   2.103516                  3.941406   \n",
       "44389207                   2.103516                  3.941406   \n",
       "44419697                   2.103516                  3.941406   \n",
       "44450187                   2.103516                  3.941406   \n",
       "44480677                   2.103516                  3.941406   \n",
       "44511167                   2.103516                  3.941406   \n",
       "44541657                   2.103516                  3.941406   \n",
       "44572147                   2.103516                  3.941406   \n",
       "44602637                   2.103516                  3.941406   \n",
       "44633127                   2.103516                  3.941406   \n",
       "44663617                   2.103516                  3.941406   \n",
       "44694107                   2.103516                  3.941406   \n",
       "44724597                   2.103516                  3.941406   \n",
       "44755087                   2.103516                  3.941406   \n",
       "44785577                   2.103516                  3.941406   \n",
       "44816067                   2.103516                  3.941406   \n",
       "44846557                   2.103516                  3.941406   \n",
       "44877047                   2.103516                  3.941406   \n",
       "44907537                   2.103516                  3.941406   \n",
       "44938027                   2.103516                  3.941406   \n",
       "44968517                   2.103516                  3.941406   \n",
       "44999007                   2.103516                  3.941406   \n",
       "45029497                   2.103516                  3.941406   \n",
       "45059987                   2.103516                  3.941406   \n",
       "45090477                   2.103516                  3.941406   \n",
       "45120967                   2.103516                  3.941406   \n",
       "45151457                   2.103516                  3.941406   \n",
       "45181947                   2.103516                  3.941406   \n",
       "45212437                   2.103516                  3.941406   \n",
       "45242927                   2.103516                  3.941406   \n",
       "45273417                   2.103516                  3.941406   \n",
       "45303907                   2.103516                  3.941406   \n",
       "45334397                   2.103516                  3.941406   \n",
       "45364887                   2.103516                  3.941406   \n",
       "45395377                   2.103516                  3.941406   \n",
       "45425867                   2.103516                  3.941406   \n",
       "45456357                   2.103516                  3.941406   \n",
       "45486847                   2.103516                  3.941406   \n",
       "45517337                   2.103516                  3.941406   \n",
       "45547827                   2.103516                  3.941406   \n",
       "45578317                   2.103516                  3.941406   \n",
       "45608807                   2.103516                  3.941406   \n",
       "45639297                   2.103516                  3.941406   \n",
       "45669787                   2.103516                  3.941406   \n",
       "45700277                   2.103516                  3.941406   \n",
       "45730767                   2.103516                  3.941406   \n",
       "45761257                   2.103516                  3.941406   \n",
       "45791747                   2.103516                  3.941406   \n",
       "45822237                   2.103516                  3.941406   \n",
       "45852727                   2.103516                  3.941406   \n",
       "45883217                   2.103516                  3.941406   \n",
       "45913707                   2.103516                  3.941406   \n",
       "45944197                   2.103516                  3.941406   \n",
       "45974687                   2.103516                  3.941406   \n",
       "46005177                   2.103516                  3.941406   \n",
       "\n",
       "          enc_store_id_item_id_mean  enc_store_id_item_id_std  \\\n",
       "44328227                   1.208008                   2.34375   \n",
       "44358717                   1.208008                   2.34375   \n",
       "44389207                   1.208008                   2.34375   \n",
       "44419697                   1.208008                   2.34375   \n",
       "44450187                   1.208008                   2.34375   \n",
       "44480677                   1.208008                   2.34375   \n",
       "44511167                   1.208008                   2.34375   \n",
       "44541657                   1.208008                   2.34375   \n",
       "44572147                   1.208008                   2.34375   \n",
       "44602637                   1.208008                   2.34375   \n",
       "44633127                   1.208008                   2.34375   \n",
       "44663617                   1.208008                   2.34375   \n",
       "44694107                   1.208008                   2.34375   \n",
       "44724597                   1.208008                   2.34375   \n",
       "44755087                   1.208008                   2.34375   \n",
       "44785577                   1.208008                   2.34375   \n",
       "44816067                   1.208008                   2.34375   \n",
       "44846557                   1.208008                   2.34375   \n",
       "44877047                   1.208008                   2.34375   \n",
       "44907537                   1.208008                   2.34375   \n",
       "44938027                   1.208008                   2.34375   \n",
       "44968517                   1.208008                   2.34375   \n",
       "44999007                   1.208008                   2.34375   \n",
       "45029497                   1.208008                   2.34375   \n",
       "45059987                   1.208008                   2.34375   \n",
       "45090477                   1.208008                   2.34375   \n",
       "45120967                   1.208008                   2.34375   \n",
       "45151457                   1.208008                   2.34375   \n",
       "45181947                   1.208008                   2.34375   \n",
       "45212437                   1.208008                   2.34375   \n",
       "45242927                   1.208008                   2.34375   \n",
       "45273417                   1.208008                   2.34375   \n",
       "45303907                   1.208008                   2.34375   \n",
       "45334397                   1.208008                   2.34375   \n",
       "45364887                   1.208008                   2.34375   \n",
       "45395377                   1.208008                   2.34375   \n",
       "45425867                   1.208008                   2.34375   \n",
       "45456357                   1.208008                   2.34375   \n",
       "45486847                   1.208008                   2.34375   \n",
       "45517337                   1.208008                   2.34375   \n",
       "45547827                   1.208008                   2.34375   \n",
       "45578317                   1.208008                   2.34375   \n",
       "45608807                   1.208008                   2.34375   \n",
       "45639297                   1.208008                   2.34375   \n",
       "45669787                   1.208008                   2.34375   \n",
       "45700277                   1.208008                   2.34375   \n",
       "45730767                   1.208008                   2.34375   \n",
       "45761257                   1.208008                   2.34375   \n",
       "45791747                   1.208008                   2.34375   \n",
       "45822237                   1.208008                   2.34375   \n",
       "45852727                   1.208008                   2.34375   \n",
       "45883217                   1.208008                   2.34375   \n",
       "45913707                   1.208008                   2.34375   \n",
       "45944197                   1.208008                   2.34375   \n",
       "45974687                   1.208008                   2.34375   \n",
       "46005177                   1.208008                   2.34375   \n",
       "\n",
       "          enc_store_id_tm_dw_item_id_mean  enc_store_id_tm_dw_item_id_std  \\\n",
       "44328227                         1.222656                        2.265625   \n",
       "44358717                         1.022461                        1.820312   \n",
       "44389207                         0.843750                        1.856445   \n",
       "44419697                         0.829102                        1.476562   \n",
       "44450187                         1.279297                        2.527344   \n",
       "44480677                         1.421875                        2.447266   \n",
       "44511167                         1.836914                        3.369141   \n",
       "44541657                         1.222656                        2.265625   \n",
       "44572147                         1.022461                        1.820312   \n",
       "44602637                         0.843750                        1.856445   \n",
       "44633127                         0.829102                        1.476562   \n",
       "44663617                         1.279297                        2.527344   \n",
       "44694107                         1.421875                        2.447266   \n",
       "44724597                         1.836914                        3.369141   \n",
       "44755087                         1.222656                        2.265625   \n",
       "44785577                         1.022461                        1.820312   \n",
       "44816067                         0.843750                        1.856445   \n",
       "44846557                         0.829102                        1.476562   \n",
       "44877047                         1.279297                        2.527344   \n",
       "44907537                         1.421875                        2.447266   \n",
       "44938027                         1.836914                        3.369141   \n",
       "44968517                         1.222656                        2.265625   \n",
       "44999007                         1.022461                        1.820312   \n",
       "45029497                         0.843750                        1.856445   \n",
       "45059987                         0.829102                        1.476562   \n",
       "45090477                         1.279297                        2.527344   \n",
       "45120967                         1.421875                        2.447266   \n",
       "45151457                         1.836914                        3.369141   \n",
       "45181947                         1.222656                        2.265625   \n",
       "45212437                         1.022461                        1.820312   \n",
       "45242927                         0.843750                        1.856445   \n",
       "45273417                         0.829102                        1.476562   \n",
       "45303907                         1.279297                        2.527344   \n",
       "45334397                         1.421875                        2.447266   \n",
       "45364887                         1.836914                        3.369141   \n",
       "45395377                         1.222656                        2.265625   \n",
       "45425867                         1.022461                        1.820312   \n",
       "45456357                         0.843750                        1.856445   \n",
       "45486847                         0.829102                        1.476562   \n",
       "45517337                         1.279297                        2.527344   \n",
       "45547827                         1.421875                        2.447266   \n",
       "45578317                         1.836914                        3.369141   \n",
       "45608807                         1.222656                        2.265625   \n",
       "45639297                         1.022461                        1.820312   \n",
       "45669787                         0.843750                        1.856445   \n",
       "45700277                         0.829102                        1.476562   \n",
       "45730767                         1.279297                        2.527344   \n",
       "45761257                         1.421875                        2.447266   \n",
       "45791747                         1.836914                        3.369141   \n",
       "45822237                         1.222656                        2.265625   \n",
       "45852727                         1.022461                        1.820312   \n",
       "45883217                         0.843750                        1.856445   \n",
       "45913707                         0.829102                        1.476562   \n",
       "45944197                         1.279297                        2.527344   \n",
       "45974687                         1.421875                        2.447266   \n",
       "46005177                         1.836914                        3.369141   \n",
       "\n",
       "          enc_store_id_tm_dw_mean  enc_store_id_tm_dw_std  \n",
       "44328227                 2.416016                6.773438  \n",
       "44358717                 2.208984                6.210938  \n",
       "44389207                 2.134766                6.031250  \n",
       "44419697                 2.111328                6.085938  \n",
       "44450187                 2.289062                6.824219  \n",
       "44480677                 2.757812                7.976562  \n",
       "44511167                 2.882812                7.929688  \n",
       "44541657                 2.416016                6.773438  \n",
       "44572147                 2.208984                6.210938  \n",
       "44602637                 2.134766                6.031250  \n",
       "44633127                 2.111328                6.085938  \n",
       "44663617                 2.289062                6.824219  \n",
       "44694107                 2.757812                7.976562  \n",
       "44724597                 2.882812                7.929688  \n",
       "44755087                 2.416016                6.773438  \n",
       "44785577                 2.208984                6.210938  \n",
       "44816067                 2.134766                6.031250  \n",
       "44846557                 2.111328                6.085938  \n",
       "44877047                 2.289062                6.824219  \n",
       "44907537                 2.757812                7.976562  \n",
       "44938027                 2.882812                7.929688  \n",
       "44968517                 2.416016                6.773438  \n",
       "44999007                 2.208984                6.210938  \n",
       "45029497                 2.134766                6.031250  \n",
       "45059987                 2.111328                6.085938  \n",
       "45090477                 2.289062                6.824219  \n",
       "45120967                 2.757812                7.976562  \n",
       "45151457                 2.882812                7.929688  \n",
       "45181947                 2.416016                6.773438  \n",
       "45212437                 2.208984                6.210938  \n",
       "45242927                 2.134766                6.031250  \n",
       "45273417                 2.111328                6.085938  \n",
       "45303907                 2.289062                6.824219  \n",
       "45334397                 2.757812                7.976562  \n",
       "45364887                 2.882812                7.929688  \n",
       "45395377                 2.416016                6.773438  \n",
       "45425867                 2.208984                6.210938  \n",
       "45456357                 2.134766                6.031250  \n",
       "45486847                 2.111328                6.085938  \n",
       "45517337                 2.289062                6.824219  \n",
       "45547827                 2.757812                7.976562  \n",
       "45578317                 2.882812                7.929688  \n",
       "45608807                 2.416016                6.773438  \n",
       "45639297                 2.208984                6.210938  \n",
       "45669787                 2.134766                6.031250  \n",
       "45700277                 2.111328                6.085938  \n",
       "45730767                 2.289062                6.824219  \n",
       "45761257                 2.757812                7.976562  \n",
       "45791747                 2.882812                7.929688  \n",
       "45822237                 2.416016                6.773438  \n",
       "45852727                 2.208984                6.210938  \n",
       "45883217                 2.134766                6.031250  \n",
       "45913707                 2.111328                6.085938  \n",
       "45944197                 2.289062                6.824219  \n",
       "45974687                 2.757812                7.976562  \n",
       "46005177                 2.882812                7.929688  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df[part4][grid_df[part4]['id']=='FOODS_1_001_CA_3_validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>release</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_store_id_cat_id_mean</th>\n",
       "      <th>enc_store_id_cat_id_std</th>\n",
       "      <th>enc_store_id_dept_id_mean</th>\n",
       "      <th>enc_store_id_dept_id_std</th>\n",
       "      <th>enc_store_id_item_id_mean</th>\n",
       "      <th>enc_store_id_item_id_std</th>\n",
       "      <th>enc_store_id_tm_dw_item_id_mean</th>\n",
       "      <th>enc_store_id_tm_dw_item_id_std</th>\n",
       "      <th>enc_store_id_tm_dw_mean</th>\n",
       "      <th>enc_store_id_tm_dw_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45181947</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>1886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.240234</td>\n",
       "      <td>...</td>\n",
       "      <td>3.400391</td>\n",
       "      <td>9.085938</td>\n",
       "      <td>2.103516</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>1.208008</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.416016</td>\n",
       "      <td>6.773438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  item_id  dept_id  cat_id store_id  \\\n",
       "45181947  FOODS_1_001_CA_3_validation        1        1       2     CA_3   \n",
       "\n",
       "         state_id     d  sales  release  sell_price  ...  \\\n",
       "45181947       CA  1886    0.0        0    2.240234  ...   \n",
       "\n",
       "          enc_store_id_cat_id_mean  enc_store_id_cat_id_std  \\\n",
       "45181947                  3.400391                 9.085938   \n",
       "\n",
       "          enc_store_id_dept_id_mean  enc_store_id_dept_id_std  \\\n",
       "45181947                   2.103516                  3.941406   \n",
       "\n",
       "          enc_store_id_item_id_mean  enc_store_id_item_id_std  \\\n",
       "45181947                   1.208008                   2.34375   \n",
       "\n",
       "          enc_store_id_tm_dw_item_id_mean  enc_store_id_tm_dw_item_id_std  \\\n",
       "45181947                         1.222656                        2.265625   \n",
       "\n",
       "          enc_store_id_tm_dw_mean  enc_store_id_tm_dw_std  \n",
       "45181947                 2.416016                6.773438  \n",
       "\n",
       "[1 rows x 44 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df[part4][grid_df[part4]['id']=='FOODS_1_001_CA_3_validation'].iloc[28:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_part4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1064443, 28, 1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[-1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_valid[-1:].astype(float), X_valid[-1:].astype(float)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (85372,) (28,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-3bfe9fb7381f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-d31952cf01de>\u001b[0m in \u001b[0;36mrmse\u001b[1;34m(y, y_pred)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (85372,) (28,) "
     ]
    }
   ],
   "source": [
    "rmse(y_true, predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [00:41<00:00, 729.43it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_df = predict_samples(grid_df, model,1913-28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>0.945388</td>\n",
       "      <td>2.003579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.988437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991595</td>\n",
       "      <td>2.004172</td>\n",
       "      <td>3.996796</td>\n",
       "      <td>1.987121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.994274</td>\n",
       "      <td>1.000470</td>\n",
       "      <td>1.003092</td>\n",
       "      <td>12.005028</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_002_CA_3_validation</td>\n",
       "      <td>0.947493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990681</td>\n",
       "      <td>0.997816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991029</td>\n",
       "      <td>2.999773</td>\n",
       "      <td>0.988705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_003_CA_3_validation</td>\n",
       "      <td>1.980366</td>\n",
       "      <td>1.996166</td>\n",
       "      <td>1.997639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991572</td>\n",
       "      <td>3.000392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.988590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.998410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991140</td>\n",
       "      <td>0.997473</td>\n",
       "      <td>1.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_004_CA_3_validation</td>\n",
       "      <td>2.979722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.994847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_005_CA_3_validation</td>\n",
       "      <td>0.968645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.999153</td>\n",
       "      <td>0.989044</td>\n",
       "      <td>2.001684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992203</td>\n",
       "      <td>2.005431</td>\n",
       "      <td>1.997760</td>\n",
       "      <td>...</td>\n",
       "      <td>1.990899</td>\n",
       "      <td>0.991345</td>\n",
       "      <td>3.003465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990334</td>\n",
       "      <td>5.002184</td>\n",
       "      <td>1.988181</td>\n",
       "      <td>11.984709</td>\n",
       "      <td>5.998434</td>\n",
       "      <td>5.018535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>HOUSEHOLD_2_512_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.983884</td>\n",
       "      <td>0.993348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.002082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000955</td>\n",
       "      <td>0.991547</td>\n",
       "      <td>...</td>\n",
       "      <td>3.004490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.994336</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.994228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>HOUSEHOLD_2_513_CA_3_validation</td>\n",
       "      <td>1.981005</td>\n",
       "      <td>2.992184</td>\n",
       "      <td>0.989580</td>\n",
       "      <td>4.000650</td>\n",
       "      <td>2.983788</td>\n",
       "      <td>5.984475</td>\n",
       "      <td>0.997073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.003559</td>\n",
       "      <td>0.990593</td>\n",
       "      <td>1.001264</td>\n",
       "      <td>1.003189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.004761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>HOUSEHOLD_2_514_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>HOUSEHOLD_2_515_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992053</td>\n",
       "      <td>2.004418</td>\n",
       "      <td>0.995675</td>\n",
       "      <td>1.002987</td>\n",
       "      <td>1.003892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>HOUSEHOLD_2_516_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3049 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id        F1        F2        F3        F4  \\\n",
       "0         FOODS_1_001_CA_3_validation  0.945388  2.003579  0.000000  0.000000   \n",
       "1         FOODS_1_002_CA_3_validation  0.947493  0.000000  0.000000  0.987979   \n",
       "2         FOODS_1_003_CA_3_validation  1.980366  1.996166  1.997639  0.000000   \n",
       "3         FOODS_1_004_CA_3_validation  2.979722  0.000000  1.994847  0.000000   \n",
       "4         FOODS_1_005_CA_3_validation  0.968645  0.000000  1.999153  0.989044   \n",
       "...                               ...       ...       ...       ...       ...   \n",
       "3044  HOUSEHOLD_2_512_CA_3_validation  0.000000  0.000000  0.983884  0.993348   \n",
       "3045  HOUSEHOLD_2_513_CA_3_validation  1.981005  2.992184  0.989580  4.000650   \n",
       "3046  HOUSEHOLD_2_514_CA_3_validation  0.000000  0.000000  0.000000  0.000000   \n",
       "3047  HOUSEHOLD_2_515_CA_3_validation  0.000000  0.000000  0.000000  0.000000   \n",
       "3048  HOUSEHOLD_2_516_CA_3_validation  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            F5        F6        F7        F8        F9  ...       F19  \\\n",
       "0     0.988437  0.000000  0.992105  0.000000  0.000000  ...  0.991595   \n",
       "1     0.000000  0.990681  0.997816  0.000000  0.000000  ...  0.993240   \n",
       "2     0.000000  0.991572  3.000392  0.000000  0.988590  ...  0.000000   \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "4     2.001684  0.000000  0.992203  2.005431  1.997760  ...  1.990899   \n",
       "...        ...       ...       ...       ...       ...  ...       ...   \n",
       "3044  0.000000  2.002082  0.000000  2.000955  0.991547  ...  3.004490   \n",
       "3045  2.983788  5.984475  0.997073  0.000000  0.000000  ...  0.991001   \n",
       "3046  0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "3047  0.000000  0.000000  0.000000  0.990822  0.000000  ...  0.992053   \n",
       "3048  0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "\n",
       "           F20       F21       F22       F23       F24       F25        F26  \\\n",
       "0     2.004172  3.996796  1.987121  0.000000  0.994274  1.000470   1.003092   \n",
       "1     0.000000  0.992987  0.000000  0.000000  0.000000  0.000000   0.991029   \n",
       "2     0.992334  0.998410  0.000000  0.000000  0.000000  0.000000   0.991140   \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "4     0.991345  3.003465  0.000000  0.990334  5.002184  1.988181  11.984709   \n",
       "...        ...       ...       ...       ...       ...       ...        ...   \n",
       "3044  0.000000  0.990324  0.000000  0.994336  0.999221  0.000000   0.994228   \n",
       "3045  0.000000  0.000000  2.003559  0.990593  1.001264  1.003189   0.000000   \n",
       "3046  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3047  2.004418  0.995675  1.002987  1.003892  0.000000  0.000000   0.000000   \n",
       "3048  0.000000  0.000000  0.000000  0.000000  0.000000  0.990592   0.000000   \n",
       "\n",
       "            F27       F28  \n",
       "0     12.005028  0.000000  \n",
       "1      2.999773  0.988705  \n",
       "2      0.997473  1.001314  \n",
       "3      0.000000  0.000000  \n",
       "4      5.998434  5.018535  \n",
       "...         ...       ...  \n",
       "3044   0.000000  0.000000  \n",
       "3045   0.000000  2.004761  \n",
       "3046   0.000000  0.000000  \n",
       "3047   0.000000  0.000000  \n",
       "3048   0.000000  0.991381  \n",
       "\n",
       "[3049 rows x 29 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df2 = pd.melt(pred_df,\n",
    "                  id_vars=['id'],\n",
    "                  var_name='d',\n",
    "                  value_name='sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df2['d'] = grid_df2['d'].apply(lambda x: int(x[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_df2.sort_values(by=['id','d'])['sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = grid_df[(grid_df['d']>=1913-28)&(grid_df['d']<1913)][['id','d','sales']].sort_values(by=['id','d'])['sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.974663110840904"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y_true,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_baseline = pd.read_csv('../cache/submission/lgbm_baseline_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
    "lgb_baseline = pd.concat([lgb_baseline[~lgb_baseline['id'].isin(pred_df.id.unique().tolist())], pred_df], axis=0)\n",
    "# lgb_baseline = lgb_baseline.merge(pred_df, on=['id'], how='left').fillna(0)\n",
    "lgb_baseline.to_csv('../cache/submission/replace_ca3_lgbm_baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.808594</td>\n",
       "      <td>0.732422</td>\n",
       "      <td>0.711914</td>\n",
       "      <td>0.752930</td>\n",
       "      <td>0.948242</td>\n",
       "      <td>1.083984</td>\n",
       "      <td>1.169922</td>\n",
       "      <td>0.881836</td>\n",
       "      <td>0.892578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.874023</td>\n",
       "      <td>1.097656</td>\n",
       "      <td>1.051758</td>\n",
       "      <td>0.833984</td>\n",
       "      <td>0.799316</td>\n",
       "      <td>0.762695</td>\n",
       "      <td>0.802246</td>\n",
       "      <td>0.916016</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>1.027344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.201782</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.196289</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.270020</td>\n",
       "      <td>0.316406</td>\n",
       "      <td>0.243774</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215088</td>\n",
       "      <td>0.270996</td>\n",
       "      <td>0.280273</td>\n",
       "      <td>0.204346</td>\n",
       "      <td>0.193115</td>\n",
       "      <td>0.185791</td>\n",
       "      <td>0.198975</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.293213</td>\n",
       "      <td>0.301270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.426758</td>\n",
       "      <td>0.391602</td>\n",
       "      <td>0.408691</td>\n",
       "      <td>0.403809</td>\n",
       "      <td>0.549316</td>\n",
       "      <td>0.682617</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>0.456543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557617</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>0.687012</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>0.444824</td>\n",
       "      <td>0.475830</td>\n",
       "      <td>0.473633</td>\n",
       "      <td>0.601562</td>\n",
       "      <td>0.718262</td>\n",
       "      <td>0.665527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.681641</td>\n",
       "      <td>1.412109</td>\n",
       "      <td>1.414062</td>\n",
       "      <td>1.511719</td>\n",
       "      <td>1.986328</td>\n",
       "      <td>2.773438</td>\n",
       "      <td>3.113281</td>\n",
       "      <td>1.733398</td>\n",
       "      <td>1.427734</td>\n",
       "      <td>...</td>\n",
       "      <td>1.881836</td>\n",
       "      <td>2.589844</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>1.763672</td>\n",
       "      <td>1.533203</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.927734</td>\n",
       "      <td>2.863281</td>\n",
       "      <td>3.267578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.958008</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.896484</td>\n",
       "      <td>0.942383</td>\n",
       "      <td>1.068359</td>\n",
       "      <td>1.379883</td>\n",
       "      <td>1.486328</td>\n",
       "      <td>1.011719</td>\n",
       "      <td>1.024414</td>\n",
       "      <td>...</td>\n",
       "      <td>1.059570</td>\n",
       "      <td>1.470703</td>\n",
       "      <td>1.487305</td>\n",
       "      <td>1.005859</td>\n",
       "      <td>0.887695</td>\n",
       "      <td>0.928711</td>\n",
       "      <td>0.895996</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.462891</td>\n",
       "      <td>1.457031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>HOUSEHOLD_2_512_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.988287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989662</td>\n",
       "      <td>2.003428</td>\n",
       "      <td>1.996493</td>\n",
       "      <td>0.998099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996859</td>\n",
       "      <td>2.006725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.974878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.964254</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>2.995314</td>\n",
       "      <td>1.989989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>HOUSEHOLD_2_513_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.991708</td>\n",
       "      <td>0.978572</td>\n",
       "      <td>0.994755</td>\n",
       "      <td>0.998916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.004572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.995169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993438</td>\n",
       "      <td>2.005111</td>\n",
       "      <td>1.997271</td>\n",
       "      <td>3.997645</td>\n",
       "      <td>1.988478</td>\n",
       "      <td>4.997004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>HOUSEHOLD_2_514_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>HOUSEHOLD_2_515_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990573</td>\n",
       "      <td>0.997179</td>\n",
       "      <td>1.001137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>HOUSEHOLD_2_516_CA_3_validation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60980 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id        F1        F2        F3        F4  \\\n",
       "0       HOBBIES_1_001_CA_1_validation  0.808594  0.732422  0.711914  0.752930   \n",
       "1       HOBBIES_1_002_CA_1_validation  0.201782  0.193237  0.171875  0.196289   \n",
       "2       HOBBIES_1_003_CA_1_validation  0.426758  0.391602  0.408691  0.403809   \n",
       "3       HOBBIES_1_004_CA_1_validation  1.681641  1.412109  1.414062  1.511719   \n",
       "4       HOBBIES_1_005_CA_1_validation  0.958008  0.875000  0.896484  0.942383   \n",
       "...                               ...       ...       ...       ...       ...   \n",
       "3044  HOUSEHOLD_2_512_CA_3_validation  0.000000  3.988287  0.000000  0.000000   \n",
       "3045  HOUSEHOLD_2_513_CA_3_validation  0.000000  2.991708  0.978572  0.994755   \n",
       "3046  HOUSEHOLD_2_514_CA_3_validation  0.000000  0.000000  0.000000  0.000000   \n",
       "3047  HOUSEHOLD_2_515_CA_3_validation  0.000000  0.000000  0.000000  0.000000   \n",
       "3048  HOUSEHOLD_2_516_CA_3_validation  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            F5        F6        F7        F8        F9  ...       F19  \\\n",
       "0     0.948242  1.083984  1.169922  0.881836  0.892578  ...  0.874023   \n",
       "1     0.219727  0.270020  0.316406  0.243774  0.223022  ...  0.215088   \n",
       "2     0.549316  0.682617  0.619629  0.472656  0.456543  ...  0.557617   \n",
       "3     1.986328  2.773438  3.113281  1.733398  1.427734  ...  1.881836   \n",
       "4     1.068359  1.379883  1.486328  1.011719  1.024414  ...  1.059570   \n",
       "...        ...       ...       ...       ...       ...  ...       ...   \n",
       "3044  0.000000  0.989662  2.003428  1.996493  0.998099  ...  0.996859   \n",
       "3045  0.998916  0.000000  0.993803  0.000000  2.004572  ...  0.996056   \n",
       "3046  0.000000  0.000000  0.000000  0.000000  0.991025  ...  0.990647   \n",
       "3047  0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.990573   \n",
       "3048  0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.990572   \n",
       "\n",
       "           F20       F21       F22       F23       F24       F25       F26  \\\n",
       "0     1.097656  1.051758  0.833984  0.799316  0.762695  0.802246  0.916016   \n",
       "1     0.270996  0.280273  0.204346  0.193115  0.185791  0.198975  0.205078   \n",
       "2     0.660156  0.687012  0.510254  0.444824  0.475830  0.473633  0.601562   \n",
       "3     2.589844  3.101562  1.763672  1.533203  1.421875  1.422852  1.927734   \n",
       "4     1.470703  1.487305  1.005859  0.887695  0.928711  0.895996  1.125000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3044  2.006725  0.000000  0.000000  5.974878  0.000000  4.964254  0.982502   \n",
       "3045  0.000000  0.995169  0.000000  0.993438  2.005111  1.997271  3.997645   \n",
       "3046  0.000000  0.000000  0.000000  0.991108  0.000000  0.000000  0.000000   \n",
       "3047  0.997179  1.001137  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3048  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "           F27       F28  \n",
       "0     1.062500  1.027344  \n",
       "1     0.293213  0.301270  \n",
       "2     0.718262  0.665527  \n",
       "3     2.863281  3.267578  \n",
       "4     1.462891  1.457031  \n",
       "...        ...       ...  \n",
       "3044  2.995314  1.989989  \n",
       "3045  1.988478  4.997004  \n",
       "3046  0.000000  0.000000  \n",
       "3047  0.000000  0.000000  \n",
       "3048  0.000000  0.000000  \n",
       "\n",
       "[60980 rows x 29 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1_x</th>\n",
       "      <th>F2_x</th>\n",
       "      <th>F3_x</th>\n",
       "      <th>F4_x</th>\n",
       "      <th>F5_x</th>\n",
       "      <th>F6_x</th>\n",
       "      <th>F7_x</th>\n",
       "      <th>F8_x</th>\n",
       "      <th>F9_x</th>\n",
       "      <th>...</th>\n",
       "      <th>F19_y</th>\n",
       "      <th>F20_y</th>\n",
       "      <th>F21_y</th>\n",
       "      <th>F22_y</th>\n",
       "      <th>F23_y</th>\n",
       "      <th>F24_y</th>\n",
       "      <th>F25_y</th>\n",
       "      <th>F26_y</th>\n",
       "      <th>F27_y</th>\n",
       "      <th>F28_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.808594</td>\n",
       "      <td>0.732422</td>\n",
       "      <td>0.711914</td>\n",
       "      <td>0.752930</td>\n",
       "      <td>0.948242</td>\n",
       "      <td>1.083984</td>\n",
       "      <td>1.169922</td>\n",
       "      <td>0.881836</td>\n",
       "      <td>0.892578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.201782</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.196289</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.270020</td>\n",
       "      <td>0.316406</td>\n",
       "      <td>0.243774</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.426758</td>\n",
       "      <td>0.391602</td>\n",
       "      <td>0.408691</td>\n",
       "      <td>0.403809</td>\n",
       "      <td>0.549316</td>\n",
       "      <td>0.682617</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>0.456543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.681641</td>\n",
       "      <td>1.412109</td>\n",
       "      <td>1.414062</td>\n",
       "      <td>1.511719</td>\n",
       "      <td>1.986328</td>\n",
       "      <td>2.773438</td>\n",
       "      <td>3.113281</td>\n",
       "      <td>1.733398</td>\n",
       "      <td>1.427734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.958008</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.896484</td>\n",
       "      <td>0.942383</td>\n",
       "      <td>1.068359</td>\n",
       "      <td>1.379883</td>\n",
       "      <td>1.486328</td>\n",
       "      <td>1.011719</td>\n",
       "      <td>1.024414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60975</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60976</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60977</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60978</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60979</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60980 rows  57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id      F1_x      F2_x      F3_x      F4_x  \\\n",
       "0      HOBBIES_1_001_CA_1_validation  0.808594  0.732422  0.711914  0.752930   \n",
       "1      HOBBIES_1_002_CA_1_validation  0.201782  0.193237  0.171875  0.196289   \n",
       "2      HOBBIES_1_003_CA_1_validation  0.426758  0.391602  0.408691  0.403809   \n",
       "3      HOBBIES_1_004_CA_1_validation  1.681641  1.412109  1.414062  1.511719   \n",
       "4      HOBBIES_1_005_CA_1_validation  0.958008  0.875000  0.896484  0.942383   \n",
       "...                              ...       ...       ...       ...       ...   \n",
       "60975    FOODS_3_823_WI_3_evaluation  0.000000  0.000000  0.000000  0.000000   \n",
       "60976    FOODS_3_824_WI_3_evaluation  0.000000  0.000000  0.000000  0.000000   \n",
       "60977    FOODS_3_825_WI_3_evaluation  0.000000  0.000000  0.000000  0.000000   \n",
       "60978    FOODS_3_826_WI_3_evaluation  0.000000  0.000000  0.000000  0.000000   \n",
       "60979    FOODS_3_827_WI_3_evaluation  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "           F5_x      F6_x      F7_x      F8_x      F9_x  ...  F19_y  F20_y  \\\n",
       "0      0.948242  1.083984  1.169922  0.881836  0.892578  ...    0.0    0.0   \n",
       "1      0.219727  0.270020  0.316406  0.243774  0.223022  ...    0.0    0.0   \n",
       "2      0.549316  0.682617  0.619629  0.472656  0.456543  ...    0.0    0.0   \n",
       "3      1.986328  2.773438  3.113281  1.733398  1.427734  ...    0.0    0.0   \n",
       "4      1.068359  1.379883  1.486328  1.011719  1.024414  ...    0.0    0.0   \n",
       "...         ...       ...       ...       ...       ...  ...    ...    ...   \n",
       "60975  0.000000  0.000000  0.000000  0.000000  0.000000  ...    0.0    0.0   \n",
       "60976  0.000000  0.000000  0.000000  0.000000  0.000000  ...    0.0    0.0   \n",
       "60977  0.000000  0.000000  0.000000  0.000000  0.000000  ...    0.0    0.0   \n",
       "60978  0.000000  0.000000  0.000000  0.000000  0.000000  ...    0.0    0.0   \n",
       "60979  0.000000  0.000000  0.000000  0.000000  0.000000  ...    0.0    0.0   \n",
       "\n",
       "       F21_y  F22_y  F23_y  F24_y  F25_y  F26_y  F27_y  F28_y  \n",
       "0        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "60975    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "60976    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "60977    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "60978    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "60979    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[60980 rows x 57 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca3_ids = []\n",
    "for id_ in lgb_baseline.id.unique():\n",
    "    if 'CA_3' in id_:\n",
    "        ca3_ids.append(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = lgb_baseline[lgb_baseline['id'].isin(ca3_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5918.938812255859\n",
      "5453.120452880859\n",
      "5234.604309082031\n",
      "5206.524627685547\n",
      "5691.846588134766\n",
      "7010.581939697266\n",
      "7984.301086425781\n",
      "6776.377227783203\n",
      "6540.396392822266\n",
      "6022.037933349609\n",
      "5602.921966552734\n",
      "6255.215881347656\n",
      "7514.764434814453\n",
      "7107.095291137695\n",
      "6899.395782470703\n",
      "6368.650009155273\n",
      "5776.670211791992\n",
      "5643.580017089844\n",
      "5962.638031005859\n",
      "7238.612091064453\n",
      "7819.089630126953\n",
      "6425.430725097656\n",
      "5904.887588500977\n",
      "5658.6055908203125\n",
      "5572.8787841796875\n",
      "5960.247650146484\n",
      "7349.634429931641\n",
      "7934.983856201172\n"
     ]
    }
   ],
   "source": [
    "for col in [f'F{i}' for i in range(1,29)]:\n",
    "    print(df_temp[col].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5462.296257972717\n",
      "5474.489412546158\n",
      "5189.987372756004\n",
      "5463.501700758934\n",
      "6553.12259221077\n",
      "7198.065579891205\n",
      "8387.158918738365\n",
      "6610.26856815815\n",
      "6210.309138774872\n",
      "5675.529461622238\n",
      "5629.015412688255\n",
      "6183.9447066783905\n",
      "7768.314437866211\n",
      "8590.295038461685\n",
      "6567.783604025841\n",
      "5630.338264346123\n",
      "5366.513480186462\n",
      "5491.342837333679\n",
      "5335.472414016724\n",
      "6922.629359126091\n",
      "8211.952028155327\n",
      "6056.76638007164\n",
      "5673.512939810753\n",
      "5226.354175209999\n",
      "5011.96452319622\n",
      "5619.815213918686\n",
      "7417.341598749161\n",
      "7714.297100782394\n"
     ]
    }
   ],
   "source": [
    "for col in [f'F{i}' for i in range(1,29)]:\n",
    "    print(pred_df[col].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_id = 'CA_1'\n",
    "n_fold = 3\n",
    "part_len = (END_TRAIN-START_TRAIN)//n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_IDS = pd.read_pickle(f'{SAV_BASE_PATH}/BASE_FEATURES.pkl').store_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_2_007_CA_1_validation 0 27 55\n",
      "FOODS_2_031_CA_1_validation 0 13 41\n",
      "FOODS_2_032_CA_1_validation 0 13 41\n",
      "FOODS_2_035_CA_1_validation 0 6 34\n",
      "FOODS_3_044_CA_1_validation 0 6 34\n",
      "FOODS_3_105_CA_1_validation 0 6 34\n",
      "FOODS_3_205_CA_1_validation 0 6 34\n",
      "FOODS_3_229_CA_1_validation 0 20 48\n",
      "FOODS_3_270_CA_1_validation 0 13 41\n",
      "FOODS_3_275_CA_1_validation 0 6 34\n",
      "FOODS_3_359_CA_1_validation 0 27 55\n",
      "FOODS_3_429_CA_1_validation 0 13 41\n",
      "FOODS_3_482_CA_1_validation 0 27 55\n",
      "FOODS_3_521_CA_1_validation 0 6 34\n",
      "FOODS_3_557_CA_1_validation 0 13 41\n",
      "FOODS_3_693_CA_1_validation 0 20 48\n",
      "HOBBIES_1_219_CA_1_validation 0 27 55\n",
      "HOBBIES_1_354_CA_1_validation 0 6 34\n",
      "HOUSEHOLD_1_283_CA_1_validation 0 13 41\n",
      "HOUSEHOLD_1_332_CA_1_validation 0 6 34\n",
      "HOUSEHOLD_1_384_CA_1_validation 0 6 34\n",
      "HOUSEHOLD_2_114_CA_1_validation 0 20 48\n",
      "HOUSEHOLD_2_132_CA_1_validation 0 6 34\n",
      "HOUSEHOLD_2_307_CA_1_validation 0 6 34\n",
      "HOUSEHOLD_2_360_CA_1_validation 0 6 34\n",
      "HOUSEHOLD_2_487_CA_1_validation 0 27 55\n",
      "HOUSEHOLD_2_515_CA_1_validation 0 6 34\n",
      "FOODS_2_084_CA_1_validation 0 25 53\n",
      "FOODS_2_185_CA_1_validation 0 25 53\n",
      "FOODS_3_038_CA_1_validation 0 25 53\n",
      "FOODS_3_047_CA_1_validation 0 25 53\n",
      "FOODS_3_119_CA_1_validation 0 25 53\n",
      "FOODS_3_134_CA_1_validation 0 11 39\n",
      "FOODS_3_500_CA_1_validation 0 25 53\n",
      "FOODS_3_796_CA_1_validation 0 11 39\n",
      "HOUSEHOLD_2_248_CA_1_validation 0 25 53\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 327s 654us/sample - loss: 14.5747 - root_mean_squared_error: 3.8177 - val_loss: 9.0699 - val_root_mean_squared_error: 3.0116\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 12.6028 - root_mean_squared_error: 3.5500 - val_loss: 8.8189 - val_root_mean_squared_error: 2.9697\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 325s 650us/sample - loss: 12.3943 - root_mean_squared_error: 3.5206 - val_loss: 8.5539 - val_root_mean_squared_error: 2.9247\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 322s 643us/sample - loss: 12.3036 - root_mean_squared_error: 3.5077 - val_loss: 8.5381 - val_root_mean_squared_error: 2.9220\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 320s 641us/sample - loss: 12.2478 - root_mean_squared_error: 3.4997 - val_loss: 8.5000 - val_root_mean_squared_error: 2.9155\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 12.2195 - root_mean_squared_error: 3.4956 - val_loss: 8.4924 - val_root_mean_squared_error: 2.9142\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 322s 645us/sample - loss: 12.1958 - root_mean_squared_error: 3.4923 - val_loss: 8.4652 - val_root_mean_squared_error: 2.9095\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 325s 649us/sample - loss: 12.1614 - root_mean_squared_error: 3.4873 - val_loss: 8.4563 - val_root_mean_squared_error: 2.9080\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 12.1580 - root_mean_squared_error: 3.4868 - val_loss: 8.5216 - val_root_mean_squared_error: 2.9192\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 325s 650us/sample - loss: 12.1315 - root_mean_squared_error: 3.4830 - val_loss: 8.4582 - val_root_mean_squared_error: 2.9083\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 324s 647us/sample - loss: 11.9594 - root_mean_squared_error: 3.4582 - val_loss: 8.1338 - val_root_mean_squared_error: 2.8520\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 325s 651us/sample - loss: 11.6349 - root_mean_squared_error: 3.4110 - val_loss: 8.0755 - val_root_mean_squared_error: 2.8417\n",
      "Epoch 13/20\n",
      "500000/500000 [==============================] - 325s 649us/sample - loss: 11.6087 - root_mean_squared_error: 3.4072 - val_loss: 8.0702 - val_root_mean_squared_error: 2.8408\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 11.5856 - root_mean_squared_error: 3.4038 - val_loss: 8.0362 - val_root_mean_squared_error: 2.8348\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 11.5676 - root_mean_squared_error: 3.4011 - val_loss: 8.1467 - val_root_mean_squared_error: 2.8542\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 11.5641 - root_mean_squared_error: 3.4006 - val_loss: 8.0738 - val_root_mean_squared_error: 2.8414\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 326s 652us/sample - loss: 11.5388 - root_mean_squared_error: 3.3969 - val_loss: 8.2361 - val_root_mean_squared_error: 2.8699\n",
      "Epoch 18/20\n",
      "500000/500000 [==============================] - 322s 645us/sample - loss: 11.5393 - root_mean_squared_error: 3.3969 - val_loss: 8.0183 - val_root_mean_squared_error: 2.8317\n",
      "Epoch 19/20\n",
      "500000/500000 [==============================] - 322s 645us/sample - loss: 11.5263 - root_mean_squared_error: 3.3950 - val_loss: 8.0289 - val_root_mean_squared_error: 2.8335\n",
      "Epoch 20/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 11.5120 - root_mean_squared_error: 3.3929 - val_loss: 8.0731 - val_root_mean_squared_error: 2.8413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30361/30490 [02:02<00:00, 251.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_3_418_CA_2_validation 0 6 34\n",
      "FOODS_3_482_CA_2_validation 0 6 34\n",
      "FOODS_3_557_CA_2_validation 0 6 34\n",
      "FOODS_3_693_CA_2_validation 0 20 48\n",
      "FOODS_3_761_CA_2_validation 0 13 41\n",
      "HOBBIES_1_219_CA_2_validation 0 27 55\n",
      "HOBBIES_1_334_CA_2_validation 0 6 34\n",
      "HOBBIES_1_339_CA_2_validation 0 6 34\n",
      "HOBBIES_1_354_CA_2_validation 0 6 34\n",
      "HOUSEHOLD_1_018_CA_2_validation 0 20 48\n",
      "HOUSEHOLD_1_022_CA_2_validation 0 13 41\n",
      "HOUSEHOLD_1_267_CA_2_validation 0 20 48\n",
      "HOUSEHOLD_1_283_CA_2_validation 0 6 34\n",
      "HOUSEHOLD_1_332_CA_2_validation 0 20 48\n",
      "HOUSEHOLD_1_384_CA_2_validation 0 13 41\n",
      "HOUSEHOLD_2_132_CA_2_validation 0 6 34\n",
      "HOUSEHOLD_2_175_CA_2_validation 0 6 34\n",
      "HOUSEHOLD_2_177_CA_2_validation 0 13 41\n",
      "HOUSEHOLD_2_307_CA_2_validation 0 13 41\n",
      "FOODS_2_305_CA_2_validation 0 18 46\n",
      "FOODS_3_003_CA_2_validation 0 25 53\n",
      "FOODS_3_047_CA_2_validation 0 25 53\n",
      "FOODS_3_119_CA_2_validation 0 25 53\n",
      "FOODS_3_134_CA_2_validation 0 4 32\n",
      "FOODS_3_147_CA_2_validation 0 18 46\n",
      "FOODS_3_166_CA_2_validation 0 25 53\n",
      "FOODS_3_169_CA_2_validation 0 18 46\n",
      "FOODS_3_500_CA_2_validation 0 25 53\n",
      "FOODS_3_517_CA_2_validation 0 11 39\n",
      "FOODS_3_647_CA_2_validation 0 18 46\n",
      "FOODS_3_796_CA_2_validation 0 11 39\n",
      "HOBBIES_1_125_CA_2_validation 0 4 32\n",
      "HOUSEHOLD_1_020_CA_2_validation 0 4 32\n",
      "HOUSEHOLD_1_066_CA_2_validation 0 11 39\n",
      "HOUSEHOLD_1_120_CA_2_validation 0 4 32\n",
      "HOUSEHOLD_1_245_CA_2_validation 0 4 32\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 7.4314 - root_mean_squared_error: 2.7261 - val_loss: 6.7698 - val_root_mean_squared_error: 2.6019\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 324s 647us/sample - loss: 5.9209 - root_mean_squared_error: 2.4333 - val_loss: 6.5441 - val_root_mean_squared_error: 2.5581\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 325s 649us/sample - loss: 5.8461 - root_mean_squared_error: 2.4179 - val_loss: 6.4734 - val_root_mean_squared_error: 2.5443\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 323s 645us/sample - loss: 5.8181 - root_mean_squared_error: 2.4121 - val_loss: 6.4426 - val_root_mean_squared_error: 2.5382\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 5.8008 - root_mean_squared_error: 2.4085 - val_loss: 6.4119 - val_root_mean_squared_error: 2.5322\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 320s 640us/sample - loss: 5.7893 - root_mean_squared_error: 2.4061 - val_loss: 6.4046 - val_root_mean_squared_error: 2.5307\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 322s 645us/sample - loss: 5.7792 - root_mean_squared_error: 2.4040 - val_loss: 6.4198 - val_root_mean_squared_error: 2.5337\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 323s 645us/sample - loss: 5.7715 - root_mean_squared_error: 2.4024 - val_loss: 6.3808 - val_root_mean_squared_error: 2.5260\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 5.7646 - root_mean_squared_error: 2.4010 - val_loss: 6.3531 - val_root_mean_squared_error: 2.5205\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 321s 643us/sample - loss: 5.7574 - root_mean_squared_error: 2.3995 - val_loss: 6.3768 - val_root_mean_squared_error: 2.5252\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 5.7520 - root_mean_squared_error: 2.3983 - val_loss: 6.3655 - val_root_mean_squared_error: 2.5230\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 321s 642us/sample - loss: 5.7470 - root_mean_squared_error: 2.3973 - val_loss: 6.3439 - val_root_mean_squared_error: 2.5187\n",
      "Epoch 13/20\n",
      "500000/500000 [==============================] - 322s 643us/sample - loss: 5.5874 - root_mean_squared_error: 2.3638 - val_loss: 6.1481 - val_root_mean_squared_error: 2.4795\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 322s 643us/sample - loss: 5.5499 - root_mean_squared_error: 2.3558 - val_loss: 6.0821 - val_root_mean_squared_error: 2.4662\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 325s 650us/sample - loss: 5.5464 - root_mean_squared_error: 2.3551 - val_loss: 6.1316 - val_root_mean_squared_error: 2.4762\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 323s 645us/sample - loss: 5.5421 - root_mean_squared_error: 2.3542 - val_loss: 6.1407 - val_root_mean_squared_error: 2.4780\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 324s 649us/sample - loss: 5.5320 - root_mean_squared_error: 2.3520 - val_loss: 6.0705 - val_root_mean_squared_error: 2.4638\n",
      "Epoch 20/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 5.5292 - root_mean_squared_error: 2.3514 - val_loss: 6.0706 - val_root_mean_squared_error: 2.4639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [02:10<00:00, 234.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_2_096_CA_3_validation 0 20 48\n",
      "FOODS_3_044_CA_3_validation 0 6 34\n",
      "FOODS_3_084_CA_3_validation 0 27 55\n",
      "FOODS_3_105_CA_3_validation 0 6 34\n",
      "FOODS_3_218_CA_3_validation 0 6 34\n",
      "FOODS_3_229_CA_3_validation 0 6 34\n",
      "FOODS_3_270_CA_3_validation 0 20 48\n",
      "FOODS_3_275_CA_3_validation 0 27 55\n",
      "FOODS_3_359_CA_3_validation 0 27 55\n",
      "FOODS_3_429_CA_3_validation 0 13 41\n",
      "FOODS_3_482_CA_3_validation 0 27 55\n",
      "FOODS_3_525_CA_3_validation 0 13 41\n",
      "HOBBIES_1_219_CA_3_validation 0 13 41\n",
      "HOBBIES_1_354_CA_3_validation 0 6 34\n",
      "HOBBIES_2_041_CA_3_validation 0 27 55\n",
      "HOUSEHOLD_1_018_CA_3_validation 0 20 48\n",
      "HOUSEHOLD_1_332_CA_3_validation 0 20 48\n",
      "HOUSEHOLD_1_384_CA_3_validation 0 6 34\n",
      "HOUSEHOLD_1_435_CA_3_validation 0 13 41\n",
      "HOUSEHOLD_1_450_CA_3_validation 0 6 34\n",
      "HOUSEHOLD_1_495_CA_3_validation 0 6 34\n",
      "HOUSEHOLD_2_005_CA_3_validation 0 6 34\n",
      "HOUSEHOLD_2_060_CA_3_validation 0 13 41\n",
      "HOUSEHOLD_2_114_CA_3_validation 0 20 48\n",
      "HOUSEHOLD_2_132_CA_3_validation 0 6 34\n",
      "HOUSEHOLD_2_175_CA_3_validation 0 20 48\n",
      "HOUSEHOLD_2_177_CA_3_validation 0 20 48\n",
      "HOUSEHOLD_2_225_CA_3_validation 0 20 48\n",
      "HOUSEHOLD_2_263_CA_3_validation 0 6 34\n",
      "HOUSEHOLD_2_280_CA_3_validation 0 20 48\n",
      "HOUSEHOLD_2_307_CA_3_validation 0 13 41\n",
      "HOUSEHOLD_2_360_CA_3_validation 0 20 48\n",
      "HOUSEHOLD_2_515_CA_3_validation 0 13 41\n",
      "FOODS_2_185_CA_3_validation 0 18 46\n",
      "FOODS_3_047_CA_3_validation 0 25 53\n",
      "FOODS_3_134_CA_3_validation 0 11 39\n",
      "FOODS_3_166_CA_3_validation 0 25 53\n",
      "FOODS_3_278_CA_3_validation 0 25 53\n",
      "FOODS_3_647_CA_3_validation 0 18 46\n",
      "FOODS_3_796_CA_3_validation 0 18 46\n",
      "HOBBIES_1_125_CA_3_validation 0 18 46\n",
      "HOBBIES_1_269_CA_3_validation 0 4 32\n",
      "HOBBIES_1_279_CA_3_validation 0 11 39\n",
      "HOUSEHOLD_1_098_CA_3_validation 0 4 32\n",
      "HOUSEHOLD_1_120_CA_3_validation 0 11 39\n",
      "HOUSEHOLD_1_245_CA_3_validation 0 4 32\n",
      "HOUSEHOLD_1_320_CA_3_validation 0 4 32\n",
      "HOUSEHOLD_1_421_CA_3_validation 0 4 32\n",
      "HOUSEHOLD_1_496_CA_3_validation 0 4 32\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 328s 656us/sample - loss: 30.4230 - root_mean_squared_error: 5.5157 - val_loss: 19.1291 - val_root_mean_squared_error: 4.3737\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 323s 645us/sample - loss: 28.1200 - root_mean_squared_error: 5.3028 - val_loss: 19.1551 - val_root_mean_squared_error: 4.3767\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 28.0088 - root_mean_squared_error: 5.2923 - val_loss: 19.1315 - val_root_mean_squared_error: 4.3740\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 27.9766 - root_mean_squared_error: 5.2893 - val_loss: 19.2287 - val_root_mean_squared_error: 4.3851\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 27.9705 - root_mean_squared_error: 5.2887 - val_loss: 19.2506 - val_root_mean_squared_error: 4.3875\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 325s 650us/sample - loss: 27.9471 - root_mean_squared_error: 5.2865 - val_loss: 19.7612 - val_root_mean_squared_error: 4.4454\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 325s 649us/sample - loss: 27.8814 - root_mean_squared_error: 5.2803 - val_loss: 19.5681 - val_root_mean_squared_error: 4.4236\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 27.8948 - root_mean_squared_error: 5.2816 - val_loss: 19.2432 - val_root_mean_squared_error: 4.3867\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 323s 645us/sample - loss: 27.8758 - root_mean_squared_error: 5.2798 - val_loss: 19.1314 - val_root_mean_squared_error: 4.3739\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 27.8497 - root_mean_squared_error: 5.2773 - val_loss: 19.1389 - val_root_mean_squared_error: 4.3748\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 324s 649us/sample - loss: 27.8639 - root_mean_squared_error: 5.2786 - val_loss: 19.4452 - val_root_mean_squared_error: 4.4097\n",
      "Epoch 13/20\n",
      "500000/500000 [==============================] - 324s 647us/sample - loss: 27.8401 - root_mean_squared_error: 5.2764 - val_loss: 19.1587 - val_root_mean_squared_error: 4.3771\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 27.8243 - root_mean_squared_error: 5.2749 - val_loss: 19.0189 - val_root_mean_squared_error: 4.3611\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 27.8418 - root_mean_squared_error: 5.2765 - val_loss: 19.1493 - val_root_mean_squared_error: 4.3760\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 27.8686 - root_mean_squared_error: 5.2791 - val_loss: 19.0972 - val_root_mean_squared_error: 4.3700\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 27.7585 - root_mean_squared_error: 5.2686 - val_loss: 19.1012 - val_root_mean_squared_error: 4.3705\n",
      "Epoch 18/20\n",
      "500000/500000 [==============================] - 322s 645us/sample - loss: 27.7993 - root_mean_squared_error: 5.2725 - val_loss: 19.0914 - val_root_mean_squared_error: 4.3694\n",
      "Epoch 19/20\n",
      "500000/500000 [==============================] - 325s 649us/sample - loss: 26.8698 - root_mean_squared_error: 5.1836 - val_loss: 18.4444 - val_root_mean_squared_error: 4.2947\n",
      "Epoch 20/20\n",
      "495860/500000 [============================>.] - ETA: 2s - loss: 26.7292 - root_mean_squared_error: 5.1700"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [02:13<00:00, 228.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_1_056_CA_4_validation 0 20 48\n",
      "FOODS_2_031_CA_4_validation 0 13 41\n",
      "FOODS_2_032_CA_4_validation 0 6 34\n",
      "FOODS_2_035_CA_4_validation 0 13 41\n",
      "FOODS_2_301_CA_4_validation 0 27 55\n",
      "FOODS_3_044_CA_4_validation 0 6 34\n",
      "FOODS_3_084_CA_4_validation 0 6 34\n",
      "FOODS_3_105_CA_4_validation 0 6 34\n",
      "FOODS_3_169_CA_4_validation 0 6 34\n",
      "FOODS_3_218_CA_4_validation 0 6 34\n",
      "FOODS_3_229_CA_4_validation 0 6 34\n",
      "FOODS_3_359_CA_4_validation 0 20 48\n",
      "FOODS_3_521_CA_4_validation 0 6 34\n",
      "FOODS_3_525_CA_4_validation 0 13 41\n",
      "FOODS_3_557_CA_4_validation 0 6 34\n",
      "FOODS_3_583_CA_4_validation 0 6 34\n",
      "FOODS_3_693_CA_4_validation 0 20 48\n",
      "FOODS_3_761_CA_4_validation 0 20 48\n",
      "HOBBIES_1_149_CA_4_validation 0 13 41\n",
      "HOBBIES_1_219_CA_4_validation 0 20 48\n",
      "HOBBIES_1_265_CA_4_validation 0 13 41\n",
      "HOBBIES_2_084_CA_4_validation 0 27 55\n",
      "HOUSEHOLD_1_283_CA_4_validation 0 6 34\n",
      "HOUSEHOLD_1_384_CA_4_validation 0 6 34\n",
      "HOUSEHOLD_1_435_CA_4_validation 0 6 34\n",
      "HOUSEHOLD_1_450_CA_4_validation 0 6 34\n",
      "HOUSEHOLD_2_058_CA_4_validation 0 13 41\n",
      "HOUSEHOLD_2_060_CA_4_validation 0 6 34\n",
      "HOUSEHOLD_2_114_CA_4_validation 0 13 41\n",
      "HOUSEHOLD_2_132_CA_4_validation 0 6 34\n",
      "HOUSEHOLD_2_177_CA_4_validation 0 13 41\n",
      "HOUSEHOLD_2_225_CA_4_validation 0 6 34\n",
      "HOUSEHOLD_2_307_CA_4_validation 0 13 41\n",
      "FOODS_2_084_CA_4_validation 0 25 53\n",
      "FOODS_2_185_CA_4_validation 0 11 39\n",
      "FOODS_3_038_CA_4_validation 0 25 53\n",
      "FOODS_3_134_CA_4_validation 0 18 46\n",
      "FOODS_3_429_CA_4_validation 0 25 53\n",
      "FOODS_3_466_CA_4_validation 0 11 39\n",
      "FOODS_3_500_CA_4_validation 0 25 53\n",
      "FOODS_3_563_CA_4_validation 0 11 39\n",
      "FOODS_3_647_CA_4_validation 0 18 46\n",
      "HOBBIES_1_125_CA_4_validation 0 4 32\n",
      "HOBBIES_1_186_CA_4_validation 0 25 53\n",
      "HOBBIES_1_269_CA_4_validation 0 4 32\n",
      "HOBBIES_1_279_CA_4_validation 0 4 32\n",
      "HOUSEHOLD_1_242_CA_4_validation 0 18 46\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 326s 653us/sample - loss: 5.0299 - root_mean_squared_error: 2.2428 - val_loss: 3.4840 - val_root_mean_squared_error: 1.8666\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 324s 649us/sample - loss: 3.8197 - root_mean_squared_error: 1.9544 - val_loss: 2.9106 - val_root_mean_squared_error: 1.7061\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 321s 642us/sample - loss: 3.6744 - root_mean_squared_error: 1.9169 - val_loss: 2.8300 - val_root_mean_squared_error: 1.6823\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 3.6235 - root_mean_squared_error: 1.9035 - val_loss: 2.8090 - val_root_mean_squared_error: 1.6760\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 319s 638us/sample - loss: 3.5974 - root_mean_squared_error: 1.8967 - val_loss: 2.7828 - val_root_mean_squared_error: 1.6682\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 3.5691 - root_mean_squared_error: 1.8892 - val_loss: 2.7658 - val_root_mean_squared_error: 1.6631\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 322s 645us/sample - loss: 3.5412 - root_mean_squared_error: 1.8818 - val_loss: 2.6913 - val_root_mean_squared_error: 1.6405\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 320s 640us/sample - loss: 3.4441 - root_mean_squared_error: 1.8558 - val_loss: 2.6848 - val_root_mean_squared_error: 1.6385\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 320s 641us/sample - loss: 3.4354 - root_mean_squared_error: 1.8535 - val_loss: 2.6750 - val_root_mean_squared_error: 1.6355\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 321s 641us/sample - loss: 3.4228 - root_mean_squared_error: 1.8501 - val_loss: 2.6606 - val_root_mean_squared_error: 1.6311\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 3.4186 - root_mean_squared_error: 1.8489 - val_loss: 2.6684 - val_root_mean_squared_error: 1.6335\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 320s 639us/sample - loss: 3.4144 - root_mean_squared_error: 1.8478 - val_loss: 2.6641 - val_root_mean_squared_error: 1.6322\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 322s 643us/sample - loss: 3.4110 - root_mean_squared_error: 1.8469 - val_loss: 2.6642 - val_root_mean_squared_error: 1.6322\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 322s 645us/sample - loss: 3.4049 - root_mean_squared_error: 1.8452 - val_loss: 2.6571 - val_root_mean_squared_error: 1.6300\n",
      "Epoch 19/20\n",
      "500000/500000 [==============================] - 320s 639us/sample - loss: 3.4029 - root_mean_squared_error: 1.8447 - val_loss: 2.6629 - val_root_mean_squared_error: 1.6318\n",
      "Epoch 20/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 3.3996 - root_mean_squared_error: 1.8438 - val_loss: 2.6605 - val_root_mean_squared_error: 1.6311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 29674/30490 [02:10<00:03, 225.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_3_044_TX_1_validation 0 6 34\n",
      "FOODS_3_072_TX_1_validation 0 27 55\n",
      "FOODS_3_084_TX_1_validation 0 6 34\n",
      "FOODS_3_105_TX_1_validation 0 6 34\n",
      "FOODS_3_229_TX_1_validation 0 27 55\n",
      "FOODS_3_270_TX_1_validation 0 27 55\n",
      "FOODS_3_359_TX_1_validation 0 27 55\n",
      "FOODS_3_429_TX_1_validation 0 6 34\n",
      "FOODS_3_482_TX_1_validation 0 27 55\n",
      "FOODS_3_583_TX_1_validation 0 27 55\n",
      "FOODS_3_662_TX_1_validation 0 27 55\n",
      "FOODS_3_693_TX_1_validation 0 13 41\n",
      "HOBBIES_1_219_TX_1_validation 0 13 41\n",
      "HOBBIES_1_241_TX_1_validation 0 13 41\n",
      "HOBBIES_2_084_TX_1_validation 0 6 34\n",
      "HOUSEHOLD_1_190_TX_1_validation 0 6 34\n",
      "HOUSEHOLD_1_384_TX_1_validation 0 13 41\n",
      "HOUSEHOLD_1_435_TX_1_validation 0 13 41\n",
      "HOUSEHOLD_1_450_TX_1_validation 0 6 34\n",
      "HOUSEHOLD_2_060_TX_1_validation 0 13 41\n",
      "HOUSEHOLD_2_114_TX_1_validation 0 13 41\n",
      "HOUSEHOLD_2_132_TX_1_validation 0 13 41\n",
      "HOUSEHOLD_2_360_TX_1_validation 0 13 41\n",
      "HOUSEHOLD_2_515_TX_1_validation 0 13 41\n",
      "FOODS_2_185_TX_1_validation 0 25 53\n",
      "FOODS_2_390_TX_1_validation 0 18 46\n",
      "FOODS_3_003_TX_1_validation 0 11 39\n",
      "FOODS_3_038_TX_1_validation 0 25 53\n",
      "FOODS_3_047_TX_1_validation 0 18 46\n",
      "FOODS_3_119_TX_1_validation 0 18 46\n",
      "FOODS_3_134_TX_1_validation 0 25 53\n",
      "FOODS_3_166_TX_1_validation 0 25 53\n",
      "FOODS_3_500_TX_1_validation 0 25 53\n",
      "FOODS_3_647_TX_1_validation 0 18 46\n",
      "HOBBIES_1_158_TX_1_validation 0 11 39\n",
      "HOUSEHOLD_2_067_TX_1_validation 0 4 32\n",
      "HOUSEHOLD_2_196_TX_1_validation 0 4 32\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 325s 651us/sample - loss: 9.3666 - root_mean_squared_error: 3.0605 - val_loss: 5.6217 - val_root_mean_squared_error: 2.3710\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 320s 641us/sample - loss: 7.9864 - root_mean_squared_error: 2.8260 - val_loss: 5.2826 - val_root_mean_squared_error: 2.2984\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 7.8737 - root_mean_squared_error: 2.8060 - val_loss: 5.4429 - val_root_mean_squared_error: 2.3330\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 321s 642us/sample - loss: 7.8510 - root_mean_squared_error: 2.8020 - val_loss: 5.1679 - val_root_mean_squared_error: 2.2733\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 320s 641us/sample - loss: 7.8148 - root_mean_squared_error: 2.7955 - val_loss: 5.1865 - val_root_mean_squared_error: 2.2774\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 325s 650us/sample - loss: 7.7951 - root_mean_squared_error: 2.7920 - val_loss: 5.1280 - val_root_mean_squared_error: 2.2645\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 320s 640us/sample - loss: 7.7845 - root_mean_squared_error: 2.7901 - val_loss: 5.4903 - val_root_mean_squared_error: 2.3431\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 7.7855 - root_mean_squared_error: 2.7902 - val_loss: 5.1548 - val_root_mean_squared_error: 2.2704\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 7.7621 - root_mean_squared_error: 2.7861 - val_loss: 5.1113 - val_root_mean_squared_error: 2.2608\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 7.7616 - root_mean_squared_error: 2.7860 - val_loss: 5.1340 - val_root_mean_squared_error: 2.2658\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 321s 642us/sample - loss: 7.7520 - root_mean_squared_error: 2.7842 - val_loss: 5.1837 - val_root_mean_squared_error: 2.2768\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 7.7393 - root_mean_squared_error: 2.7820 - val_loss: 5.2445 - val_root_mean_squared_error: 2.2901\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 7.4360 - root_mean_squared_error: 2.7269 - val_loss: 4.9383 - val_root_mean_squared_error: 2.2222\n",
      "Epoch 20/20\n",
      "500000/500000 [==============================] - 325s 650us/sample - loss: 7.4420 - root_mean_squared_error: 2.7280 - val_loss: 4.8873 - val_root_mean_squared_error: 2.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [02:14<00:00, 226.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_2_007_TX_2_validation 0 27 55\n",
      "FOODS_2_035_TX_2_validation 0 13 41\n",
      "FOODS_2_096_TX_2_validation 0 27 55\n",
      "FOODS_2_301_TX_2_validation 0 20 48\n",
      "FOODS_3_044_TX_2_validation 0 6 34\n",
      "FOODS_3_084_TX_2_validation 0 20 48\n",
      "FOODS_3_105_TX_2_validation 0 6 34\n",
      "FOODS_3_229_TX_2_validation 0 27 55\n",
      "FOODS_3_275_TX_2_validation 0 27 55\n",
      "FOODS_3_359_TX_2_validation 0 20 48\n",
      "FOODS_3_521_TX_2_validation 0 6 34\n",
      "FOODS_3_525_TX_2_validation 0 6 34\n",
      "FOODS_3_693_TX_2_validation 0 20 48\n",
      "HOBBIES_1_219_TX_2_validation 0 20 48\n",
      "HOBBIES_1_241_TX_2_validation 0 13 41\n",
      "HOBBIES_1_334_TX_2_validation 0 13 41\n",
      "HOBBIES_1_339_TX_2_validation 0 6 34\n",
      "HOBBIES_1_354_TX_2_validation 0 6 34\n",
      "HOBBIES_2_015_TX_2_validation 0 6 34\n",
      "HOBBIES_2_041_TX_2_validation 0 13 41\n",
      "HOUSEHOLD_1_190_TX_2_validation 0 6 34\n",
      "HOUSEHOLD_1_384_TX_2_validation 0 6 34\n",
      "HOUSEHOLD_1_435_TX_2_validation 0 13 41\n",
      "HOUSEHOLD_2_060_TX_2_validation 0 13 41\n",
      "HOUSEHOLD_2_114_TX_2_validation 0 13 41\n",
      "HOUSEHOLD_2_175_TX_2_validation 0 27 55\n",
      "HOUSEHOLD_2_177_TX_2_validation 0 13 41\n",
      "HOUSEHOLD_2_225_TX_2_validation 0 20 48\n",
      "HOUSEHOLD_2_280_TX_2_validation 0 13 41\n",
      "HOUSEHOLD_2_307_TX_2_validation 0 6 34\n",
      "HOUSEHOLD_2_360_TX_2_validation 0 6 34\n",
      "HOUSEHOLD_2_381_TX_2_validation 0 27 55\n",
      "HOUSEHOLD_2_515_TX_2_validation 0 6 34\n",
      "FOODS_3_003_TX_2_validation 0 11 39\n",
      "FOODS_3_038_TX_2_validation 0 4 32\n",
      "FOODS_3_047_TX_2_validation 0 11 39\n",
      "FOODS_3_134_TX_2_validation 0 25 53\n",
      "FOODS_3_166_TX_2_validation 0 11 39\n",
      "FOODS_3_278_TX_2_validation 0 25 53\n",
      "FOODS_3_366_TX_2_validation 0 4 32\n",
      "FOODS_3_500_TX_2_validation 0 18 46\n",
      "FOODS_3_647_TX_2_validation 0 18 46\n",
      "FOODS_3_760_TX_2_validation 0 25 53\n",
      "HOBBIES_1_125_TX_2_validation 0 18 46\n",
      "HOBBIES_1_269_TX_2_validation 0 11 39\n",
      "HOBBIES_1_279_TX_2_validation 0 4 32\n",
      "HOUSEHOLD_1_245_TX_2_validation 0 4 32\n",
      "HOUSEHOLD_2_196_TX_2_validation 0 4 32\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 327s 654us/sample - loss: 14.7853 - root_mean_squared_error: 3.8452 - val_loss: 8.1872 - val_root_mean_squared_error: 2.8613\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 12.9504 - root_mean_squared_error: 3.5987 - val_loss: 7.9781 - val_root_mean_squared_error: 2.8246\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 12.7842 - root_mean_squared_error: 3.5755 - val_loss: 7.8711 - val_root_mean_squared_error: 2.8055\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 323s 645us/sample - loss: 12.3089 - root_mean_squared_error: 3.5084 - val_loss: 7.4304 - val_root_mean_squared_error: 2.7259\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 11.8114 - root_mean_squared_error: 3.4368 - val_loss: 6.9302 - val_root_mean_squared_error: 2.6325\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 326s 651us/sample - loss: 11.2868 - root_mean_squared_error: 3.3596 - val_loss: 6.9039 - val_root_mean_squared_error: 2.6275\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 324s 647us/sample - loss: 11.2231 - root_mean_squared_error: 3.3501 - val_loss: 7.0747 - val_root_mean_squared_error: 2.6598\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 11.1896 - root_mean_squared_error: 3.3451 - val_loss: 6.9269 - val_root_mean_squared_error: 2.6319\n",
      "Epoch 13/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 11.1964 - root_mean_squared_error: 3.3461 - val_loss: 6.9405 - val_root_mean_squared_error: 2.6345\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 326s 652us/sample - loss: 11.1920 - root_mean_squared_error: 3.3454 - val_loss: 6.8790 - val_root_mean_squared_error: 2.6228\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 11.1718 - root_mean_squared_error: 3.3424 - val_loss: 6.8828 - val_root_mean_squared_error: 2.6235\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 329s 659us/sample - loss: 11.1617 - root_mean_squared_error: 3.3409 - val_loss: 6.9897 - val_root_mean_squared_error: 2.6438\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 326s 652us/sample - loss: 11.1512 - root_mean_squared_error: 3.3393 - val_loss: 6.8436 - val_root_mean_squared_error: 2.6160\n",
      "Epoch 18/20\n",
      "500000/500000 [==============================] - 326s 652us/sample - loss: 11.1146 - root_mean_squared_error: 3.3339 - val_loss: 6.9670 - val_root_mean_squared_error: 2.6395\n",
      "Epoch 19/20\n",
      "500000/500000 [==============================] - 326s 652us/sample - loss: 11.1292 - root_mean_squared_error: 3.3360 - val_loss: 6.8248 - val_root_mean_squared_error: 2.6124\n",
      "Epoch 20/20\n",
      "498980/500000 [============================>.] - ETA: 0s - loss: 11.0775 - root_mean_squared_error: 3.3283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [02:19<00:00, 218.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_2_007_TX_3_validation 0 6 34\n",
      "FOODS_2_035_TX_3_validation 0 13 41\n",
      "FOODS_2_115_TX_3_validation 0 13 41\n",
      "FOODS_2_132_TX_3_validation 0 27 55\n",
      "FOODS_2_265_TX_3_validation 0 27 55\n",
      "FOODS_2_269_TX_3_validation 0 27 55\n",
      "FOODS_2_273_TX_3_validation 0 27 55\n",
      "FOODS_3_044_TX_3_validation 0 13 41\n",
      "FOODS_3_072_TX_3_validation 0 27 55\n",
      "FOODS_3_077_TX_3_validation 0 6 34\n",
      "FOODS_3_084_TX_3_validation 0 20 48\n",
      "FOODS_3_105_TX_3_validation 0 6 34\n",
      "FOODS_3_229_TX_3_validation 0 27 55\n",
      "FOODS_3_270_TX_3_validation 0 27 55\n",
      "FOODS_3_275_TX_3_validation 0 27 55\n",
      "FOODS_3_359_TX_3_validation 0 20 48\n",
      "FOODS_3_429_TX_3_validation 0 6 34\n",
      "FOODS_3_525_TX_3_validation 0 6 34\n",
      "FOODS_3_557_TX_3_validation 0 27 55\n",
      "FOODS_3_583_TX_3_validation 0 27 55\n",
      "HOBBIES_1_219_TX_3_validation 0 20 48\n",
      "HOBBIES_1_241_TX_3_validation 0 13 41\n",
      "HOUSEHOLD_1_267_TX_3_validation 0 6 34\n",
      "HOUSEHOLD_1_332_TX_3_validation 0 20 48\n",
      "HOUSEHOLD_2_060_TX_3_validation 0 13 41\n",
      "HOUSEHOLD_2_114_TX_3_validation 0 20 48\n",
      "HOUSEHOLD_2_132_TX_3_validation 0 6 34\n",
      "HOUSEHOLD_2_175_TX_3_validation 0 6 34\n",
      "HOUSEHOLD_2_263_TX_3_validation 0 13 41\n",
      "HOUSEHOLD_2_280_TX_3_validation 0 13 41\n",
      "HOUSEHOLD_2_307_TX_3_validation 0 6 34\n",
      "HOUSEHOLD_2_360_TX_3_validation 0 6 34\n",
      "FOODS_2_185_TX_3_validation 0 25 53\n",
      "FOODS_3_038_TX_3_validation 0 25 53\n",
      "FOODS_3_119_TX_3_validation 0 18 46\n",
      "FOODS_3_134_TX_3_validation 0 25 53\n",
      "FOODS_3_166_TX_3_validation 0 18 46\n",
      "FOODS_3_254_TX_3_validation 0 18 46\n",
      "FOODS_3_260_TX_3_validation 0 25 53\n",
      "FOODS_3_278_TX_3_validation 0 11 39\n",
      "FOODS_3_647_TX_3_validation 0 18 46\n",
      "HOBBIES_1_344_TX_3_validation 0 25 53\n",
      "HOUSEHOLD_2_196_TX_3_validation 0 4 32\n",
      "HOUSEHOLD_2_248_TX_3_validation 0 25 53\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 9.8870 - root_mean_squared_error: 3.1444 - val_loss: 7.8777 - val_root_mean_squared_error: 2.8067\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 9.3503 - root_mean_squared_error: 3.0578 - val_loss: 7.4584 - val_root_mean_squared_error: 2.7310\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 9.1839 - root_mean_squared_error: 3.0305 - val_loss: 7.3388 - val_root_mean_squared_error: 2.7090\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 9.1355 - root_mean_squared_error: 3.0225 - val_loss: 7.3019 - val_root_mean_squared_error: 2.7022\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 331s 663us/sample - loss: 9.0773 - root_mean_squared_error: 3.0129 - val_loss: 7.2929 - val_root_mean_squared_error: 2.7005\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 329s 657us/sample - loss: 9.0615 - root_mean_squared_error: 3.0102 - val_loss: 7.3084 - val_root_mean_squared_error: 2.7034\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 323s 645us/sample - loss: 9.0382 - root_mean_squared_error: 3.0064 - val_loss: 7.3274 - val_root_mean_squared_error: 2.7069\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 9.0259 - root_mean_squared_error: 3.0043 - val_loss: 7.2556 - val_root_mean_squared_error: 2.6936\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 322s 645us/sample - loss: 9.0139 - root_mean_squared_error: 3.0023 - val_loss: 7.2655 - val_root_mean_squared_error: 2.6955\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 8.9992 - root_mean_squared_error: 2.9999 - val_loss: 7.2685 - val_root_mean_squared_error: 2.6960\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 324s 649us/sample - loss: 8.9923 - root_mean_squared_error: 2.9987 - val_loss: 7.2572 - val_root_mean_squared_error: 2.6939\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 324s 647us/sample - loss: 8.9838 - root_mean_squared_error: 2.9973 - val_loss: 7.3336 - val_root_mean_squared_error: 2.7081\n",
      "Epoch 18/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 8.9782 - root_mean_squared_error: 2.9964 - val_loss: 7.2763 - val_root_mean_squared_error: 2.6975\n",
      "Epoch 19/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 8.9791 - root_mean_squared_error: 2.9965 - val_loss: 7.2723 - val_root_mean_squared_error: 2.6967\n",
      "Epoch 20/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 8.9715 - root_mean_squared_error: 2.9953 - val_loss: 7.4149 - val_root_mean_squared_error: 2.7230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [02:16<00:00, 222.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_2_035_WI_1_validation 0 13 41\n",
      "FOODS_2_115_WI_1_validation 0 13 41\n",
      "FOODS_2_310_WI_1_validation 0 27 55\n",
      "FOODS_3_044_WI_1_validation 0 6 34\n",
      "FOODS_3_084_WI_1_validation 0 6 34\n",
      "FOODS_3_105_WI_1_validation 0 6 34\n",
      "FOODS_3_205_WI_1_validation 0 6 34\n",
      "FOODS_3_229_WI_1_validation 0 27 55\n",
      "FOODS_3_270_WI_1_validation 0 20 48\n",
      "FOODS_3_275_WI_1_validation 0 20 48\n",
      "FOODS_3_359_WI_1_validation 0 20 48\n",
      "FOODS_3_429_WI_1_validation 0 6 34\n",
      "FOODS_3_482_WI_1_validation 0 20 48\n",
      "FOODS_3_521_WI_1_validation 0 6 34\n",
      "FOODS_3_662_WI_1_validation 0 13 41\n",
      "FOODS_3_693_WI_1_validation 0 27 55\n",
      "FOODS_3_707_WI_1_validation 0 6 34\n",
      "HOBBIES_1_219_WI_1_validation 0 27 55\n",
      "HOBBIES_1_241_WI_1_validation 0 6 34\n",
      "HOUSEHOLD_1_190_WI_1_validation 0 6 34\n",
      "HOUSEHOLD_1_267_WI_1_validation 0 20 48\n",
      "HOUSEHOLD_1_283_WI_1_validation 0 13 41\n",
      "HOUSEHOLD_2_060_WI_1_validation 0 13 41\n",
      "HOUSEHOLD_2_114_WI_1_validation 0 13 41\n",
      "HOUSEHOLD_2_164_WI_1_validation 0 20 48\n",
      "HOUSEHOLD_2_177_WI_1_validation 0 27 55\n",
      "HOUSEHOLD_2_225_WI_1_validation 0 20 48\n",
      "HOUSEHOLD_2_487_WI_1_validation 0 27 55\n",
      "FOODS_3_047_WI_1_validation 0 25 53\n",
      "FOODS_3_278_WI_1_validation 0 18 46\n",
      "FOODS_3_500_WI_1_validation 0 18 46\n",
      "FOODS_3_759_WI_1_validation 0 11 39\n",
      "HOUSEHOLD_1_060_WI_1_validation 0 25 53\n",
      "HOUSEHOLD_1_328_WI_1_validation 0 4 32\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 327s 654us/sample - loss: 6.6869 - root_mean_squared_error: 2.5859 - val_loss: 5.5274 - val_root_mean_squared_error: 2.3510\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 5.0712 - root_mean_squared_error: 2.2519 - val_loss: 4.7299 - val_root_mean_squared_error: 2.1748\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 321s 643us/sample - loss: 4.9544 - root_mean_squared_error: 2.2258 - val_loss: 4.6518 - val_root_mean_squared_error: 2.1568\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 4.9300 - root_mean_squared_error: 2.2204 - val_loss: 4.6464 - val_root_mean_squared_error: 2.1556\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 324s 647us/sample - loss: 4.9125 - root_mean_squared_error: 2.2164 - val_loss: 4.6223 - val_root_mean_squared_error: 2.1500\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 324s 649us/sample - loss: 4.8986 - root_mean_squared_error: 2.2133 - val_loss: 4.6122 - val_root_mean_squared_error: 2.1476\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 322s 644us/sample - loss: 4.8854 - root_mean_squared_error: 2.2103 - val_loss: 4.6380 - val_root_mean_squared_error: 2.1536\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 4.8758 - root_mean_squared_error: 2.2081 - val_loss: 4.5890 - val_root_mean_squared_error: 2.1422\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 323s 647us/sample - loss: 4.8674 - root_mean_squared_error: 2.2062 - val_loss: 4.5966 - val_root_mean_squared_error: 2.1440\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 323s 646us/sample - loss: 4.8603 - root_mean_squared_error: 2.2046 - val_loss: 4.6094 - val_root_mean_squared_error: 2.1470\n",
      "Epoch 13/20\n",
      "500000/500000 [==============================] - 324s 649us/sample - loss: 4.8540 - root_mean_squared_error: 2.2032 - val_loss: 4.5867 - val_root_mean_squared_error: 2.1416\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 326s 651us/sample - loss: 4.8474 - root_mean_squared_error: 2.2017 - val_loss: 4.5679 - val_root_mean_squared_error: 2.1373\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 327s 655us/sample - loss: 4.8372 - root_mean_squared_error: 2.1994 - val_loss: 4.5505 - val_root_mean_squared_error: 2.1332\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 328s 657us/sample - loss: 4.5316 - root_mean_squared_error: 2.1288 - val_loss: 4.3810 - val_root_mean_squared_error: 2.0931\n",
      "Epoch 18/20\n",
      "500000/500000 [==============================] - 327s 655us/sample - loss: 4.4658 - root_mean_squared_error: 2.1132 - val_loss: 4.2099 - val_root_mean_squared_error: 2.0518\n",
      "Epoch 19/20\n",
      "500000/500000 [==============================] - 327s 654us/sample - loss: 4.4545 - root_mean_squared_error: 2.1106 - val_loss: 4.2409 - val_root_mean_squared_error: 2.0593\n",
      "Epoch 20/20\n",
      "500000/500000 [==============================] - 323s 645us/sample - loss: 4.4468 - root_mean_squared_error: 2.1087 - val_loss: 4.2775 - val_root_mean_squared_error: 2.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [02:18<00:00, 220.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_2_007_WI_2_validation 0 27 55\n",
      "FOODS_2_035_WI_2_validation 0 13 41\n",
      "FOODS_2_132_WI_2_validation 0 27 55\n",
      "FOODS_2_259_WI_2_validation 0 27 55\n",
      "FOODS_2_273_WI_2_validation 0 27 55\n",
      "FOODS_3_044_WI_2_validation 0 20 48\n",
      "FOODS_3_084_WI_2_validation 0 27 55\n",
      "FOODS_3_105_WI_2_validation 0 20 48\n",
      "FOODS_3_205_WI_2_validation 0 6 34\n",
      "FOODS_3_270_WI_2_validation 0 27 55\n",
      "FOODS_3_275_WI_2_validation 0 20 48\n",
      "FOODS_3_429_WI_2_validation 0 6 34\n",
      "FOODS_3_482_WI_2_validation 0 13 41\n",
      "FOODS_3_662_WI_2_validation 0 27 55\n",
      "FOODS_3_707_WI_2_validation 0 6 34\n",
      "FOODS_3_733_WI_2_validation 0 6 34\n",
      "HOBBIES_1_019_WI_2_validation 0 6 34\n",
      "HOBBIES_1_190_WI_2_validation 0 6 34\n",
      "HOBBIES_1_219_WI_2_validation 0 27 55\n",
      "HOBBIES_1_241_WI_2_validation 0 6 34\n",
      "HOUSEHOLD_1_190_WI_2_validation 0 6 34\n",
      "HOUSEHOLD_1_283_WI_2_validation 0 13 41\n",
      "HOUSEHOLD_1_435_WI_2_validation 0 6 34\n",
      "HOUSEHOLD_2_060_WI_2_validation 0 6 34\n",
      "HOUSEHOLD_2_132_WI_2_validation 0 6 34\n",
      "HOUSEHOLD_2_177_WI_2_validation 0 6 34\n",
      "HOUSEHOLD_2_515_WI_2_validation 0 6 34\n",
      "FOODS_2_185_WI_2_validation 0 25 53\n",
      "FOODS_3_003_WI_2_validation 0 25 53\n",
      "FOODS_3_038_WI_2_validation 0 18 46\n",
      "FOODS_3_047_WI_2_validation 0 25 53\n",
      "FOODS_3_119_WI_2_validation 0 18 46\n",
      "FOODS_3_166_WI_2_validation 0 25 53\n",
      "FOODS_3_255_WI_2_validation 0 11 39\n",
      "FOODS_3_278_WI_2_validation 0 25 53\n",
      "FOODS_3_472_WI_2_validation 0 25 53\n",
      "FOODS_3_647_WI_2_validation 0 25 53\n",
      "FOODS_3_796_WI_2_validation 0 18 46\n",
      "HOBBIES_1_046_WI_2_validation 0 18 46\n",
      "HOBBIES_1_344_WI_2_validation 0 25 53\n",
      "HOUSEHOLD_1_245_WI_2_validation 0 4 32\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 327s 654us/sample - loss: 14.3302 - root_mean_squared_error: 3.7855 - val_loss: 10.6905 - val_root_mean_squared_error: 3.2696\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 327s 654us/sample - loss: 12.6337 - root_mean_squared_error: 3.5544 - val_loss: 10.4092 - val_root_mean_squared_error: 3.2263\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 325s 649us/sample - loss: 12.4571 - root_mean_squared_error: 3.5295 - val_loss: 10.3250 - val_root_mean_squared_error: 3.2133\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 325s 649us/sample - loss: 12.3231 - root_mean_squared_error: 3.5104 - val_loss: 10.2234 - val_root_mean_squared_error: 3.1974\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 325s 651us/sample - loss: 12.2816 - root_mean_squared_error: 3.5045 - val_loss: 10.2908 - val_root_mean_squared_error: 3.2079\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 329s 659us/sample - loss: 12.2073 - root_mean_squared_error: 3.4939 - val_loss: 10.1848 - val_root_mean_squared_error: 3.1914\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 325s 651us/sample - loss: 12.1887 - root_mean_squared_error: 3.4912 - val_loss: 10.1752 - val_root_mean_squared_error: 3.1899\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 325s 651us/sample - loss: 12.1762 - root_mean_squared_error: 3.4894 - val_loss: 10.1815 - val_root_mean_squared_error: 3.1909\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 326s 652us/sample - loss: 12.1490 - root_mean_squared_error: 3.4855 - val_loss: 10.2060 - val_root_mean_squared_error: 3.1947\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 12.1426 - root_mean_squared_error: 3.4846 - val_loss: 10.1102 - val_root_mean_squared_error: 3.1797\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 326s 652us/sample - loss: 12.1319 - root_mean_squared_error: 3.4831 - val_loss: 10.1433 - val_root_mean_squared_error: 3.1849\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 327s 654us/sample - loss: 12.1217 - root_mean_squared_error: 3.4816 - val_loss: 10.2388 - val_root_mean_squared_error: 3.1998\n",
      "Epoch 17/20\n",
      "500000/500000 [==============================] - 328s 655us/sample - loss: 12.1129 - root_mean_squared_error: 3.4804 - val_loss: 10.1242 - val_root_mean_squared_error: 3.1819\n",
      "Epoch 18/20\n",
      "500000/500000 [==============================] - 327s 653us/sample - loss: 12.1018 - root_mean_squared_error: 3.4788 - val_loss: 10.1429 - val_root_mean_squared_error: 3.1848\n",
      "Epoch 20/20\n",
      "500000/500000 [==============================] - 326s 651us/sample - loss: 12.0924 - root_mean_squared_error: 3.4774 - val_loss: 10.1265 - val_root_mean_squared_error: 3.1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [02:16<00:00, 223.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_2_007_WI_3_validation 0 6 34\n",
      "FOODS_2_035_WI_3_validation 0 13 41\n",
      "FOODS_2_115_WI_3_validation 0 13 41\n",
      "FOODS_3_275_WI_3_validation 0 20 48\n",
      "FOODS_3_359_WI_3_validation 0 27 55\n",
      "FOODS_3_429_WI_3_validation 0 6 34\n",
      "FOODS_3_521_WI_3_validation 0 6 34\n",
      "FOODS_3_583_WI_3_validation 0 27 55\n",
      "FOODS_3_662_WI_3_validation 0 6 34\n",
      "FOODS_3_747_WI_3_validation 0 20 48\n",
      "HOBBIES_1_219_WI_3_validation 0 27 55\n",
      "HOBBIES_1_241_WI_3_validation 0 6 34\n",
      "HOUSEHOLD_1_190_WI_3_validation 0 6 34\n",
      "HOUSEHOLD_1_283_WI_3_validation 0 20 48\n",
      "HOUSEHOLD_1_332_WI_3_validation 0 20 48\n",
      "HOUSEHOLD_2_020_WI_3_validation 0 27 55\n",
      "HOUSEHOLD_2_060_WI_3_validation 0 13 41\n",
      "HOUSEHOLD_2_114_WI_3_validation 0 13 41\n",
      "HOUSEHOLD_2_132_WI_3_validation 0 6 34\n",
      "HOUSEHOLD_2_177_WI_3_validation 0 20 48\n",
      "HOUSEHOLD_2_360_WI_3_validation 0 13 41\n",
      "FOODS_2_390_WI_3_validation 0 11 39\n",
      "FOODS_3_003_WI_3_validation 0 18 46\n",
      "FOODS_3_038_WI_3_validation 0 25 53\n",
      "FOODS_3_047_WI_3_validation 0 25 53\n",
      "FOODS_3_119_WI_3_validation 0 18 46\n",
      "FOODS_3_166_WI_3_validation 0 18 46\n",
      "FOODS_3_278_WI_3_validation 0 25 53\n",
      "FOODS_3_366_WI_3_validation 0 25 53\n",
      "FOODS_3_466_WI_3_validation 0 11 39\n",
      "FOODS_3_500_WI_3_validation 0 18 46\n",
      "FOODS_3_517_WI_3_validation 0 25 53\n",
      "FOODS_3_563_WI_3_validation 0 11 39\n",
      "FOODS_3_735_WI_3_validation 0 18 46\n",
      "HOBBIES_1_125_WI_3_validation 0 4 32\n",
      "HOBBIES_1_269_WI_3_validation 0 4 32\n",
      "HOUSEHOLD_1_116_WI_3_validation 0 11 39\n",
      "HOUSEHOLD_2_396_WI_3_validation 0 11 39\n",
      "Train on 500000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 324s 648us/sample - loss: 10.4438 - root_mean_squared_error: 3.2317 - val_loss: 7.6071 - val_root_mean_squared_error: 2.7581\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 326s 652us/sample - loss: 8.6796 - root_mean_squared_error: 2.9461 - val_loss: 7.2513 - val_root_mean_squared_error: 2.6928\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 329s 658us/sample - loss: 8.1296 - root_mean_squared_error: 2.8512 - val_loss: 6.6950 - val_root_mean_squared_error: 2.5875\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 331s 662us/sample - loss: 7.7627 - root_mean_squared_error: 2.7862 - val_loss: 6.3988 - val_root_mean_squared_error: 2.5296\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 328s 656us/sample - loss: 7.7453 - root_mean_squared_error: 2.7830 - val_loss: 6.3702 - val_root_mean_squared_error: 2.5239\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 331s 661us/sample - loss: 7.7143 - root_mean_squared_error: 2.7775 - val_loss: 6.3529 - val_root_mean_squared_error: 2.5205\n",
      "Epoch 14/20\n",
      "500000/500000 [==============================] - 330s 660us/sample - loss: 7.7041 - root_mean_squared_error: 2.7756 - val_loss: 6.3155 - val_root_mean_squared_error: 2.5131\n",
      "Epoch 15/20\n",
      "500000/500000 [==============================] - 332s 663us/sample - loss: 7.6978 - root_mean_squared_error: 2.7745 - val_loss: 6.3427 - val_root_mean_squared_error: 2.5185\n",
      "Epoch 16/20\n",
      "500000/500000 [==============================] - 326s 653us/sample - loss: 7.7045 - root_mean_squared_error: 2.7757 - val_loss: 6.3189 - val_root_mean_squared_error: 2.5137\n",
      "Epoch 17/20\n",
      " 59140/500000 [==>...........................] - ETA: 3:55 - loss: 7.3491 - root_mean_squared_error: 2.7109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000/500000 [==============================] - 325s 650us/sample - loss: 7.6808 - root_mean_squared_error: 2.7714 - val_loss: 6.3364 - val_root_mean_squared_error: 2.5172\n",
      "Epoch 18/20\n",
      "107350/500000 [=====>........................] - ETA: 3:27 - loss: 7.9251 - root_mean_squared_error: 2.8152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000/500000 [==============================] - 325s 650us/sample - loss: 7.6717 - root_mean_squared_error: 2.7698 - val_loss: 6.3947 - val_root_mean_squared_error: 2.5288\n",
      "Epoch 19/20\n",
      "169070/500000 [=========>....................] - ETA: 2:55 - loss: 7.2058 - root_mean_squared_error: 2.6844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000/500000 [==============================] - 327s 655us/sample - loss: 7.6675 - root_mean_squared_error: 2.7690 - val_loss: 6.3966 - val_root_mean_squared_error: 2.5291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30490/30490 [02:17<00:00, 221.05it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "for store_id in STORE_IDS:\n",
    "    BASE_GRID_DF = pd.read_pickle(f'{SAV_BASE_PATH}/BASE_FEATURES.pkl')\n",
    "    grid_df = BASE_GRID_DF[(BASE_GRID_DF['store_id']==store_id)]\n",
    "    del BASE_GRID_DF\n",
    "    part1 = (grid_df['d']>START_TRAIN)&(grid_df['d']<=END_TRAIN-2*part_len)\n",
    "    part2 = (grid_df['d']>START_TRAIN+part_len)&(grid_df['d']<=END_TRAIN-part_len)\n",
    "    part3 = (grid_df['d']>START_TRAIN+2*part_len)&(grid_df['d']<=END_TRAIN)\n",
    "\n",
    "    X_part1, y_part1, X2_part1 = get_train_valid(grid_df[part1])\n",
    "    X_part2, y_part2, X2_part2 = get_train_valid(grid_df[part2])\n",
    "    X_part3, y_part3, X2_part3 = get_train_valid(grid_df[part3])\n",
    "    \n",
    "    np.save(f'{SAV_BASE_PATH}/X_part1_{store_id}.npy', X_part1) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/X2_part1_{store_id}.npy', X2_part1) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/y_part1_{store_id}.npy', y_part1) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/X_part2_{store_id}.npy', X_part2) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/X2_part2_{store_id}.npy', X2_part2) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/y_part2_{store_id}.npy', y_part2) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/X_part3_{store_id}.npy', X_part3) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/X2_part3_{store_id}.npy', X2_part3) # save\n",
    "    np.save(f'{SAV_BASE_PATH}/y_part3_{store_id}.npy', y_part3) # save\n",
    "    \n",
    "    # X_part1 = np.load(f'X_part1.npy') # load\n",
    "    # y_part1 = np.load(f'y_part1.npy') # load\n",
    "    # X_part2 = np.load(f'X_part2.npy') # load\n",
    "    # y_part2 = np.load(f'y_part2.npy') # load\n",
    "    # X_part3 = np.load(f'X_part3.npy') # load\n",
    "    # y_part3 = np.load(f'y_part3.npy') # load\n",
    "    \n",
    "    X_train = np.vstack([X_part1, X_part2])\n",
    "    X2_train = np.vstack([X2_part1, X2_part2])\n",
    "    y_train = np.vstack([y_part1, y_part2])\n",
    "    X_valid = X_part3\n",
    "    X2_valid = X2_part3\n",
    "    y_valid = y_part3\n",
    "    train_sample_index = np.random.choice(X_train.shape[0], 500_000)\n",
    "    valid_sample_index = np.random.choice(X_valid.shape[0], 200_000)\n",
    "    \n",
    "    model = train_model(X_train[train_sample_index][:,:], X2_train[train_sample_index], y_train[train_sample_index], \n",
    "                    X_valid[valid_sample_index][:,:], X2_valid[valid_sample_index], y_valid[valid_sample_index], \n",
    "                    store_id, SAV_BASE_PATH, 28, 39, epochs=20)\n",
    "    \n",
    "    pred_df_i = predict_samples(grid_df, model)\n",
    "    pred_df = pd.concat([pred_df, pred_df_i], axis=0)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 40586.18937710277\n",
      "F2 29395.143401602167\n",
      "F3 38120.38545977829\n",
      "F4 36982.47288805284\n",
      "F5 33591.79892578081\n",
      "F6 39587.439021511425\n",
      "F7 21188.550194994838\n",
      "F8 30484.770024415222\n",
      "F9 35838.44382213248\n",
      "F10 42286.74087745728\n",
      "F11 27302.81191282044\n",
      "F12 31864.939928364183\n",
      "F13 36296.47553243057\n",
      "F14 31806.939007062843\n",
      "F15 36411.397060816176\n",
      "F16 40450.16693109856\n",
      "F17 17131.708616930875\n",
      "F18 33480.58410119888\n",
      "F19 23496.48269185616\n",
      "F20 34811.43517611466\n",
      "F21 38752.0814780636\n",
      "F22 39937.21767022612\n",
      "F23 27844.90706532309\n",
      "F24 40845.706697984686\n",
      "F25 25612.66726968481\n",
      "F26 39288.13686922821\n",
      "F27 39248.542891609424\n",
      "F28 34933.17951326945\n"
     ]
    }
   ],
   "source": [
    "for col in [f'F{i}' for i in range(1,29)]:\n",
    "    print(col, pred_df[col].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928475</td>\n",
       "      <td>1.088036</td>\n",
       "      <td>0.856354</td>\n",
       "      <td>1.026586</td>\n",
       "      <td>0.547378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.222985</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036828</td>\n",
       "      <td>0.810852</td>\n",
       "      <td>0.573717</td>\n",
       "      <td>0.759222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.016626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.601889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.689259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_002_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455638</td>\n",
       "      <td>0.636054</td>\n",
       "      <td>0.360710</td>\n",
       "      <td>0.294070</td>\n",
       "      <td>0.092625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.830625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361501</td>\n",
       "      <td>0.190376</td>\n",
       "      <td>0.128430</td>\n",
       "      <td>0.463296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.762140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_003_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553006</td>\n",
       "      <td>0.494741</td>\n",
       "      <td>0.323444</td>\n",
       "      <td>0.643956</td>\n",
       "      <td>0.142067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.695731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676567</td>\n",
       "      <td>0.619101</td>\n",
       "      <td>0.481467</td>\n",
       "      <td>0.353034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.724214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.561560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_004_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.015525</td>\n",
       "      <td>2.188738</td>\n",
       "      <td>1.849086</td>\n",
       "      <td>2.785250</td>\n",
       "      <td>2.958735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.067468</td>\n",
       "      <td>...</td>\n",
       "      <td>4.352099</td>\n",
       "      <td>4.261330</td>\n",
       "      <td>5.083395</td>\n",
       "      <td>5.753962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.373944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.045892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.907110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_005_CA_1_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.465038</td>\n",
       "      <td>1.265729</td>\n",
       "      <td>1.287864</td>\n",
       "      <td>1.192603</td>\n",
       "      <td>1.236912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.745727</td>\n",
       "      <td>...</td>\n",
       "      <td>1.239331</td>\n",
       "      <td>1.134817</td>\n",
       "      <td>1.429517</td>\n",
       "      <td>1.915966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.489210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.213023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.360599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60975</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60976</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60977</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60978</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60979</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60980 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id   F1        F2        F3        F4  \\\n",
       "0      FOODS_1_001_CA_1_validation  0.0  0.928475  1.088036  0.856354   \n",
       "1      FOODS_1_002_CA_1_validation  0.0  0.455638  0.636054  0.360710   \n",
       "2      FOODS_1_003_CA_1_validation  0.0  0.553006  0.494741  0.323444   \n",
       "3      FOODS_1_004_CA_1_validation  0.0  3.015525  2.188738  1.849086   \n",
       "4      FOODS_1_005_CA_1_validation  0.0  1.465038  1.265729  1.287864   \n",
       "...                            ...  ...       ...       ...       ...   \n",
       "60975  FOODS_3_823_WI_3_evaluation  0.0  0.000000  0.000000  0.000000   \n",
       "60976  FOODS_3_824_WI_3_evaluation  0.0  0.000000  0.000000  0.000000   \n",
       "60977  FOODS_3_825_WI_3_evaluation  0.0  0.000000  0.000000  0.000000   \n",
       "60978  FOODS_3_826_WI_3_evaluation  0.0  0.000000  0.000000  0.000000   \n",
       "60979  FOODS_3_827_WI_3_evaluation  0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "             F5        F6   F7   F8        F9  ...       F19       F20  \\\n",
       "0      1.026586  0.547378  0.0  0.0  1.222985  ...  1.036828  0.810852   \n",
       "1      0.294070  0.092625  0.0  0.0  0.830625  ...  0.361501  0.190376   \n",
       "2      0.643956  0.142067  0.0  0.0  0.695731  ...  0.676567  0.619101   \n",
       "3      2.785250  2.958735  0.0  0.0  5.067468  ...  4.352099  4.261330   \n",
       "4      1.192603  1.236912  0.0  0.0  1.745727  ...  1.239331  1.134817   \n",
       "...         ...       ...  ...  ...       ...  ...       ...       ...   \n",
       "60975  0.000000  0.000000  0.0  0.0  0.000000  ...  0.000000  0.000000   \n",
       "60976  0.000000  0.000000  0.0  0.0  0.000000  ...  0.000000  0.000000   \n",
       "60977  0.000000  0.000000  0.0  0.0  0.000000  ...  0.000000  0.000000   \n",
       "60978  0.000000  0.000000  0.0  0.0  0.000000  ...  0.000000  0.000000   \n",
       "60979  0.000000  0.000000  0.0  0.0  0.000000  ...  0.000000  0.000000   \n",
       "\n",
       "            F21       F22  F23       F24  F25       F26  F27       F28  \n",
       "0      0.573717  0.759222  0.0  1.016626  0.0  0.601889  0.0  0.689259  \n",
       "1      0.128430  0.463296  0.0  0.762140  0.0  0.177825  0.0  0.411012  \n",
       "2      0.481467  0.353034  0.0  0.724214  0.0  0.561560  0.0  0.398130  \n",
       "3      5.083395  5.753962  0.0  4.373944  0.0  3.045892  0.0  5.907110  \n",
       "4      1.429517  1.915966  0.0  1.489210  0.0  1.213023  0.0  1.360599  \n",
       "...         ...       ...  ...       ...  ...       ...  ...       ...  \n",
       "60975  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
       "60976  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
       "60977  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
       "60978  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
       "60979  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
       "\n",
       "[60980 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(f'{ORIGINAL}/sample_submission.csv')\n",
    "pred_df = pd.concat([pred_df, sample_submission[~sample_submission['id'].isin(pred_df.id.unique().tolist())]], axis=0)\n",
    "pred_df.to_csv('../cache/submission/lstm_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
