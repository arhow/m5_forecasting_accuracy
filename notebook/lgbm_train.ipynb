{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATHS for Features\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "BASE = 'grid_df.pkl'\n",
    "FEATURE1     = 'feature1.pkl'\n",
    "FEATURE2     = 'feature2.pkl'\n",
    "\n",
    "########################### Model params\n",
    "#################################################################################\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "#                     'verbose': -1,\n",
    "                } \n",
    "\n",
    "########################### Vars\n",
    "#################################################################################\n",
    "VER = 1                          # Our model version\n",
    "SEED = 42                        # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d','item_id', 'dept_id', 'cat_id',TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helper to load data by store ID\n",
    "#################################################################################\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    df = pd.read_pickle(BASE)\n",
    "    df = df[df['store_id']==store]\n",
    "    for p in [FEATURE1, FEATURE2]:\n",
    "        df_feat = pd.read_pickle(p)\n",
    "        df = pd.merge(df, df_feat.drop(columns=['sales']), on=['id', 'd'], how='left')\n",
    "        \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "#     df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    train_mask = df['d']<=END_TRAIN-P_HORIZON\n",
    "    valid_mask = (df['d']<=END_TRAIN)&(df['d']>(END_TRAIN-P_HORIZON))\n",
    "    test_mask = df['d']>END_TRAIN\n",
    "        \n",
    "    tran_df = df[train_mask].reset_index(drop=True)\n",
    "    valid_df = df[valid_mask].reset_index(drop=True)\n",
    "    test_df = df[test_mask].reset_index(drop=True)\n",
    "    \n",
    "    return tran_df, valid_df, test_df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1 4617523 85372 85372\n",
      "[100]\tvalid_0's rmse: 2.03753\n",
      "[200]\tvalid_0's rmse: 2.03082\n",
      "[300]\tvalid_0's rmse: 2.02636\n",
      "[400]\tvalid_0's rmse: 2.02506\n",
      "[500]\tvalid_0's rmse: 2.02244\n",
      "[600]\tvalid_0's rmse: 2.02163\n",
      "[700]\tvalid_0's rmse: 2.02151\n",
      "[800]\tvalid_0's rmse: 2.02101\n",
      "[900]\tvalid_0's rmse: 2.02064\n",
      "[1000]\tvalid_0's rmse: 2.02077\n",
      "[1100]\tvalid_0's rmse: 2.01978\n",
      "[1200]\tvalid_0's rmse: 2.02009\n",
      "[1300]\tvalid_0's rmse: 2.01967\n",
      "[1400]\tvalid_0's rmse: 2.01918\n",
      "11.32 min: encode\n",
      "Train CA_2 4190404 85372 85372\n",
      "[100]\tvalid_0's rmse: 1.91443\n",
      "[200]\tvalid_0's rmse: 1.87778\n",
      "[300]\tvalid_0's rmse: 1.86928\n",
      "[400]\tvalid_0's rmse: 1.8652\n",
      "[500]\tvalid_0's rmse: 1.86345\n",
      "[600]\tvalid_0's rmse: 1.86277\n",
      "[700]\tvalid_0's rmse: 1.86181\n",
      "[800]\tvalid_0's rmse: 1.86058\n",
      "[900]\tvalid_0's rmse: 1.85999\n",
      "[1000]\tvalid_0's rmse: 1.8592\n",
      "[1100]\tvalid_0's rmse: 1.85882\n",
      "[1200]\tvalid_0's rmse: 1.85815\n",
      "[1300]\tvalid_0's rmse: 1.85789\n",
      "[1400]\tvalid_0's rmse: 1.85813\n",
      "10.37 min: encode\n",
      "Train CA_3 4586569 85372 85372\n",
      "[100]\tvalid_0's rmse: 2.50579\n",
      "[200]\tvalid_0's rmse: 2.48481\n",
      "[300]\tvalid_0's rmse: 2.47492\n",
      "[400]\tvalid_0's rmse: 2.46988\n",
      "[500]\tvalid_0's rmse: 2.46717\n",
      "[600]\tvalid_0's rmse: 2.46637\n",
      "[700]\tvalid_0's rmse: 2.46474\n",
      "[800]\tvalid_0's rmse: 2.46464\n",
      "[900]\tvalid_0's rmse: 2.46426\n",
      "[1000]\tvalid_0's rmse: 2.46387\n",
      "[1100]\tvalid_0's rmse: 2.46487\n",
      "[1200]\tvalid_0's rmse: 2.46472\n",
      "[1300]\tvalid_0's rmse: 2.46384\n",
      "[1400]\tvalid_0's rmse: 2.46297\n",
      "12.04 min: encode\n",
      "Train CA_4 4481814 85372 85372\n",
      "[100]\tvalid_0's rmse: 1.33717\n",
      "[200]\tvalid_0's rmse: 1.33409\n",
      "[300]\tvalid_0's rmse: 1.33343\n",
      "[400]\tvalid_0's rmse: 1.33314\n",
      "[500]\tvalid_0's rmse: 1.33236\n",
      "[600]\tvalid_0's rmse: 1.33209\n",
      "[700]\tvalid_0's rmse: 1.33198\n"
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame()\n",
    "\n",
    "########################### Train Models\n",
    "#################################################################################\n",
    "for store_id in STORES_IDS:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get grid for current store\n",
    "    tran_df, valid_df, test_df, features_columns = get_data_by_store(store_id)\n",
    "    \n",
    "    print('Train', store_id, tran_df.shape[0], valid_df.shape[0], test_df.shape[0])\n",
    "    train_data = lgb.Dataset(tran_df[features_columns], label=tran_df[TARGET])\n",
    "    valid_data = lgb.Dataset(valid_df[features_columns], label=valid_df[TARGET])\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,train_data,valid_sets = [valid_data],verbose_eval = 100)\n",
    "    \n",
    "    test_df[TARGET] = estimator.predict(test_df[features_columns])\n",
    "    predictions = pd.concat([predictions, test_df], axis=0)\n",
    "    \n",
    "    print('%0.2f min: train' % ((time.time() - start_time) / 60))\n",
    "    \n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')\n",
    "predictions = predictions[['id', 'date', 'sales']]\n",
    "predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'sales').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "validation = submission[['id']].merge(predictions, on = 'id')\n",
    "final = pd.concat([validation, evaluation])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
