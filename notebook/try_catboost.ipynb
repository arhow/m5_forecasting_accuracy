{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "   div#notebook-container    { width: 95%; }\n",
       "   div#menubar-container     { width: 65%; }\n",
       "   div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "   div#notebook-container    { width: 95%; }\n",
    "   div#menubar-container     { width: 65%; }\n",
    "   div#maintoolbar-container { width: 99%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import Pool as catPool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "from module.prepare_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_FEATURE_PKL = '../cache/ver10/grid_features.pkl'\n",
    "END_TRAIN = 1913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = pd.read_pickle(TEMP_FEATURE_PKL)\n",
    "# for col in ['item_id','dept_id','cat_id','event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']:\n",
    "#     n_nan = grid_df[col].isna().sum()\n",
    "#     if n_nan > 0:\n",
    "#         grid_df[col] = grid_df[col].astype(str).fillna('NONE').astype('category')\n",
    "#         print(col, grid_df[col].dtype, grid_df[col].isna().sum())\n",
    "# grid_df.to_pickle(TEMP_FEATURE_PKL)\n",
    "# del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_df = pd.read_pickle(TEMP_FEATURE_PKL)\n",
    "# grid_df['groups'] = grid_df['tm_y'].astype(str) + '_' + (grid_df['tm_m']//3).astype(str)\n",
    "# grid_df.to_pickle(TEMP_FEATURE_PKL)\n",
    "# del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = pd.read_pickle(TEMP_FEATURE_PKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 46881677 entries, 0 to 46881676\n",
      "Data columns (total 79 columns):\n",
      " #   Column                  Dtype   \n",
      "---  ------                  -----   \n",
      " 0   id                      object  \n",
      " 1   item_id                 category\n",
      " 2   dept_id                 category\n",
      " 3   cat_id                  category\n",
      " 4   store_id                object  \n",
      " 5   state_id                object  \n",
      " 6   d                       int16   \n",
      " 7   sales                   float16 \n",
      " 8   release                 int16   \n",
      " 9   wm_yr_wk                int16   \n",
      " 10  sell_price              float16 \n",
      " 11  price_max               float16 \n",
      " 12  price_min               float16 \n",
      " 13  price_std               float16 \n",
      " 14  price_mean              float16 \n",
      " 15  price_norm              float16 \n",
      " 16  price_nunique           float16 \n",
      " 17  item_nunique            int16   \n",
      " 18  price_momentum          float16 \n",
      " 19  price_momentum_m        float16 \n",
      " 20  price_momentum_y        float16 \n",
      " 21  event_name_1            category\n",
      " 22  event_type_1            category\n",
      " 23  event_name_2            category\n",
      " 24  event_type_2            category\n",
      " 25  snap_CA                 category\n",
      " 26  snap_TX                 category\n",
      " 27  snap_WI                 category\n",
      " 28  tm_d                    int8    \n",
      " 29  tm_w                    int8    \n",
      " 30  tm_m                    int8    \n",
      " 31  tm_y                    int8    \n",
      " 32  tm_wm                   int8    \n",
      " 33  tm_dw                   int8    \n",
      " 34  tm_w_end                int8    \n",
      " 35  sales_lag_28            float16 \n",
      " 36  sales_lag_29            float16 \n",
      " 37  sales_lag_30            float16 \n",
      " 38  sales_lag_31            float16 \n",
      " 39  sales_lag_32            float16 \n",
      " 40  sales_lag_33            float16 \n",
      " 41  sales_lag_34            float16 \n",
      " 42  sales_lag_35            float16 \n",
      " 43  sales_lag_36            float16 \n",
      " 44  sales_lag_37            float16 \n",
      " 45  sales_lag_38            float16 \n",
      " 46  sales_lag_39            float16 \n",
      " 47  sales_lag_40            float16 \n",
      " 48  sales_lag_41            float16 \n",
      " 49  sales_lag_42            float16 \n",
      " 50  rolling_mean_7          float16 \n",
      " 51  rolling_std_7           float16 \n",
      " 52  rolling_mean_14         float16 \n",
      " 53  rolling_std_14          float16 \n",
      " 54  rolling_mean_30         float16 \n",
      " 55  rolling_std_30          float16 \n",
      " 56  rolling_mean_60         float16 \n",
      " 57  rolling_std_60          float16 \n",
      " 58  rolling_mean_180        float16 \n",
      " 59  rolling_std_180         float16 \n",
      " 60  enc_cat_id_mean         float16 \n",
      " 61  enc_cat_id_std          float16 \n",
      " 62  enc_dept_id_mean        float16 \n",
      " 63  enc_dept_id_std         float16 \n",
      " 64  enc_item_id_mean        float16 \n",
      " 65  enc_item_id_std         float16 \n",
      " 66  rolling_mean_tmp_1_7    float16 \n",
      " 67  rolling_mean_tmp_1_14   float16 \n",
      " 68  rolling_mean_tmp_1_30   float16 \n",
      " 69  rolling_mean_tmp_1_60   float16 \n",
      " 70  rolling_mean_tmp_7_7    float16 \n",
      " 71  rolling_mean_tmp_7_14   float16 \n",
      " 72  rolling_mean_tmp_7_30   float16 \n",
      " 73  rolling_mean_tmp_7_60   float16 \n",
      " 74  rolling_mean_tmp_14_7   float16 \n",
      " 75  rolling_mean_tmp_14_14  float16 \n",
      " 76  rolling_mean_tmp_14_30  float16 \n",
      " 77  rolling_mean_tmp_14_60  float16 \n",
      " 78  groups                  object  \n",
      "dtypes: category(10), float16(54), int16(4), int8(7), object(4)\n",
      "memory usage: 7.6+ GB\n"
     ]
    }
   ],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.to_pickle(TEMP_FEATURE_PKL)\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cat_params = {\n",
    "    'n_estimators':1400,\n",
    "    'loss_function':'Tweedie',\n",
    "    # 'tweedie_variance_power': 1.1,\n",
    "    'eval_metric':'RMSE',\n",
    "    'subsample': 0.5,\n",
    "    'sampling_frequency':1,\n",
    "    'learning_rate':0.03,\n",
    "    'max_leaves': 2 ** 11 - 1,\n",
    "    'min_data_in_leaf': 2 ** 12 - 1,\n",
    "#     'feature_fraction': 0.5,\n",
    "    'max_bin': 100,\n",
    "    'verbose': 1,\n",
    "    'random_seed': SEED,\n",
    "}\n",
    "\n",
    "\n",
    "def train_evaluate_model(feature_columns, target, base_path, stores_ids=STORES_IDS, permutation=False):\n",
    "\n",
    "    his = []\n",
    "    for store_id in stores_ids:\n",
    "        print('Train', store_id)\n",
    "\n",
    "        grid_df = get_data_by_store(store_id)\n",
    "        train_mask = grid_df['d'] <= END_TRAIN\n",
    "        # valid_mask = (grid_df['d'] > END_TRAIN-28 -100) & (grid_df['d'] <= END_TRAIN)\n",
    "        preds_mask = grid_df['d'] > (END_TRAIN - 100)\n",
    "\n",
    "        ## Initiating our GroupKFold\n",
    "        folds = GroupKFold(n_splits=3)\n",
    "        # grid_df['groups'] = grid_df['tm_y'].astype(str) + '_' + grid_df['tm_m'].astype(str)\n",
    "        split_groups = grid_df[train_mask]['groups']\n",
    "\n",
    "        # Saving part of the dataset for later predictions\n",
    "        # Removing features that we need to calculate recursively\n",
    "        keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "        grid_df[preds_mask].reset_index(drop=True)[keep_cols].to_pickle(f'{base_path}/test_{store_id}_ver{VER}.pkl')\n",
    "        # grid_df[valid_mask].reset_index(drop=True)[keep_cols].to_pickle(f'{base_path}/valid_{store_id}_ver{VER}.pkl')\n",
    "\n",
    "        feature_columns_i = feature_columns[store_id]\n",
    "        # Main Data\n",
    "        X, y = grid_df[train_mask][feature_columns_i], grid_df[train_mask][target]\n",
    "        del grid_df\n",
    "        \n",
    "        categorical_features_indices = np.where(X.dtypes == 'category')[0]\n",
    "\n",
    "\n",
    "        # Launch seeder again to make lgb training 100% deterministic\n",
    "        # with each \"code line\" np.random \"evolves\"\n",
    "        # so we need (may want) to \"reset\" it\n",
    "\n",
    "        for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n",
    "\n",
    "            print('Fold:', fold_)\n",
    "            trn_X, trn_y = X.iloc[trn_idx, :], y[trn_idx]\n",
    "            val_X, val_y = X.iloc[val_idx, :], y[val_idx]\n",
    "            \n",
    "            train_pool = catPool(trn_X, trn_y, cat_features=categorical_features_indices)\n",
    "            validate_pool = catPool(val_X, val_y, cat_features=categorical_features_indices)\n",
    "            estimator = CatBoostRegressor(**cat_params)\n",
    "            estimator = estimator.fit(train_pool, eval_set=validate_pool, cat_features = categorical_features_indices, silent=True)\n",
    "\n",
    "            if permutation:\n",
    "                importance_df = permutation_importance(estimator, pd.concat([val_X,val_y], axis=1), feature_columns_i, target, metric=root_mean_sqared_error,verbose=0)\n",
    "            else:\n",
    "                importance_df = None\n",
    "\n",
    "            prediction_val = estimator.predict(val_X.values)\n",
    "            rmse_val = rmse(val_y.values, prediction_val)\n",
    "            prediction_trn = estimator.predict(trn_X.values)\n",
    "            rmse_trn = rmse(trn_y.values, prediction_trn)\n",
    "\n",
    "            # Save model - it's not real '.bin' but a pickle file\n",
    "            # estimator = lgb.Booster(model_file='model.txt')\n",
    "            # can only predict with the best iteration (or the saving iteration)\n",
    "            # pickle.dump gives us more flexibility\n",
    "            # like estimator.predict(TEST, num_iteration=100)\n",
    "            # num_iteration - number of iteration want to predict with,\n",
    "            # NULL or <= 0 means use best iteration\n",
    "            model_name = f'{base_path}/cat_model_{store_id}_fold{fold_}_ver{VER}.bin'\n",
    "            pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "            # Remove temporary files and objects\n",
    "            # to free some hdd space and ram memory\n",
    "            del estimator, trn_X, val_X, trn_y, val_y\n",
    "            gc.collect()\n",
    "\n",
    "            his.append({'rmse_val': rmse_val, 'rmse_trn':rmse_trn, 'rmse_diff':rmse_val-rmse_trn, 'fold_': fold_, 'store_id': store_id, 'prediction_val':prediction_val, 'permutation_importance':importance_df})\n",
    "\n",
    "    return pd.DataFrame(his)\n",
    "\n",
    "def get_data_by_store(store_id, grid_df_path = TEMP_FEATURE_PKL):\n",
    "    grid_df = pd.read_pickle(grid_df_path)\n",
    "    return grid_df[grid_df['store_id']==store_id].reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_cols = dict(zip(STORES_IDS, [M5_FEATURES]*len(STORES_IDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "Fold: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No matching signature found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-538a93d52ce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0museful_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTARGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBASE_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#stores_ids=['CA_1']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse_trn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse_diff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-f36a810f8109>\u001b[0m in \u001b[0;36mtrain_evaluate_model\u001b[1;34m(feature_columns, target, base_path, stores_ids, permutation)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mtrain_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrn_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_features_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mvalidate_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_features_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcat_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, cat_features, text_features, column_description, pairs, delimiter, has_header, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\u001b[0m\n\u001b[0;32m    432\u001b[0m                     )\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_init\u001b[1;34m(self, data, label, cat_features, text_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0mbaseline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msamples_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_baseline_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._set_label_features_order\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.__pyx_fused_cpdef\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No matching signature found"
     ]
    }
   ],
   "source": [
    "history_df = train_evaluate_model(useful_cols, TARGET, BASE_PATH)#stores_ids=['CA_1']\n",
    "print(history_df.rmse_trn.mean(), history_df.rmse_val.mean(), history_df.rmse_diff.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
